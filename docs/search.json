[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "DATIKA",
    "section": "",
    "text": "Data Visualization\n\n\n\n\n\n\nData Science\n\n\nData Visualization\n\n\n\nData visualization simplifies complex information, making it easier to understand and leading to more informed decision-making\n\n\n\n\n\nMay 5, 2024\n\n\nMasumbuko Semba\n\n\n\n\n\n\n\n\n\n\n\n\nData cleaning, merging, and appending\n\n\n\n\n\n\nvisualization\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 1, 2024\n\n\nMasumbuko Semba\n\n\n\n\n\n\n\n\n\n\n\n\nTidying Data frame\n\n\n\n\n\n\nvisualization\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 29, 2024\n\n\nMasumbuko Semba\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Descriptive Statistics\n\n\n\n\n\n\nvisualization\n\n\ncode\n\n\nStatistics\n\n\n\nExamine measures of center and dispersion of the data so that we can gain valuable insights into the characteristics and distribution of various metrics that are in a dataset\n\n\n\n\n\nApr 25, 2024\n\n\nMasumbuko Semba\n\n\n\n\n\n\n\n\n\n\n\n\nBasic plots with ggplot2\n\n\n\n\n\n\nvisualization\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 1, 2024\n\n\nMasumbuko Semba\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing data with grammar of graphics\n\n\n\n\n\n\nvisualization\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 25, 2024\n\n\nMasumbuko Semba\n\n\n\n\n\n\n\n\n\n\n\n\nImporting table files into R\n\n\n\n\n\n\nvisualization\n\n\ncode\n\n\n\nLearning to import tabular files from local directory intot R session is an important skills in R programming\n\n\n\n\n\nFeb 26, 2024\n\n\nMasumbuko Semba\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding vector and dataframe\n\n\n\n\n\n\nvisualization\n\n\ncode\n\n\n\nUnderstanding vectoor and dataframe as core data storage in R is an important part, which allows for data analysis and visualization\n\n\n\n\n\nFeb 12, 2024\n\n\nMasumbuko Semba\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Data Structures in R\n\n\n\n\n\n\nvisualization\n\n\ncode\n\n\n\nLearn the primary data structures (vector and data frame) in R, which are the foundation of data manipulation and analysis in R \n\n\n\n\n\nFeb 3, 2024\n\n\nMasumbuko Semba\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding and using Data types in R\n\n\n\n\n\n\nvisualization\n\n\ncode\n\n\n\nLearn the primary data types in R, which are the foundation of data programming in R \n\n\n\n\n\nJan 26, 2024\n\n\nMasumbuko Semba\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Started with R and RStudio\n\n\n\n\n\n\nvisualization\n\n\ncode\n\n\n\nLearn the basic of programming with R using RStudio. We’ll install R, and RStudio RStudio, an extremely popular development environment for R \n\n\n\n\n\nJan 24, 2024\n\n\nMasumbuko Semba\n\n\n\n\n\n\n\n\n\n\n\n\nThe basics of R programming\n\n\n\n\n\n\nData Science\n\n\nR Basic\n\n\n\nThe R programming language serves as a powerful tool for statistical computing and graphics, offering extensive capabilities for data analysis, visualization, and modeling\n\n\n\n\n\nJan 24, 2024\n\n\nMasumbuko Semba\n\n\n\n\n\n\n\n\n\n\n\n\nThe basics of R and Rstudio\n\n\n\n\n\n\nvisualization\n\n\ncode\n\n\nanalysis\n\n\n\nUnderstanding the building blocks of R and its working environment Rstudio for smooth operations in data science\n\n\n\n\n\nJan 24, 2024\n\n\nMasumbuko Semba\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a Timeline graphic using R and ggplot2\n\n\n\n\n\n\nvisualization\n\n\ncode\n\n\nanalysis\n\n\n\nCreating a timeline graphic using ggplot2, which is a powerful data visualization library in R\n\n\n\n\n\nNov 24, 2023\n\n\nMasumbuko Semba\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/vectorDataframe/index.html",
    "href": "posts/vectorDataframe/index.html",
    "title": "Understanding vector and dataframe",
    "section": "",
    "text": "R language is a flexible language that allows to work with different kind of data format (R Core Team, 2023). This include integer, numeric, character, complex, dates and logical. The default data type or class in R is double precision– numeric. In a nutshell, R treats all kind of data into five categories but we deal with only four in this book. Datasets in R are often a combination of seven different data types are highlighted in Figure 1;\n\n\n\n\n\n\nFigure 1: Common data types often collected and stored for anaysis and modelling"
  },
  {
    "objectID": "posts/vectorDataframe/index.html#numeric",
    "href": "posts/vectorDataframe/index.html#numeric",
    "title": "Understanding vector and dataframe",
    "section": "Numeric",
    "text": "Numeric\nThe most common data type in R is numeric. The numeric class holds the set of real numbers — decimal place numbers. We create a numeric vector using a c() function but you can use any function that creates a sequence of numbers. For example, we can create a numeric vector of SST as follows;\n\nsst = c(25.4, 26, 28, 27.8, 29, 24.8, 22.3)\n\nWe can check whether the variable sst is numeric with is.numeric function\n\nis.numeric(sst)\n\n[1] TRUE"
  },
  {
    "objectID": "posts/vectorDataframe/index.html#integer",
    "href": "posts/vectorDataframe/index.html#integer",
    "title": "Understanding vector and dataframe",
    "section": "Integer",
    "text": "Integer\nInteger vector data type is actually a special case of numeric data. Unlike numeric, integer values do not have decimal places.They are commonly used for counting or indexing. Creating an integer vector is similar to numeric vector except that we need to instruct R to treat the data as integer and not numeric or double. To command R creating integer, we specify a suffix L to an element\n\ndepth = c(5L, 10L, 15L, 20L, 25L,30L)\nis.vector(depth);class(depth)\n\n[1] TRUE\n\n\n[1] \"integer\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nif your variable does not have decimals, R will automatically set the type as integers instead of numeric.\n\n\n\naa = c(20,68,78,50)\n\nYou can check if the data is integer with is.integer() and can convert numeric value to an integer with as.integer()\n\nis.integer(aa)\n\n[1] FALSE\n\n\nYou can query the class of the object with the class() to know the class of the object\n\nclass(aa)\n\n[1] \"numeric\"\n\n\nAlthough the object bb is integer as confirmed with as.integer() function, the class() ouput the answer as numeric. This is because the defaul type of number in r is numeric. However, you can use the function as.integer() to convert numeric value to integer\n\nclass(as.integer(aa))\n\n[1] \"integer\""
  },
  {
    "objectID": "posts/vectorDataframe/index.html#character",
    "href": "posts/vectorDataframe/index.html#character",
    "title": "Understanding vector and dataframe",
    "section": "Character",
    "text": "Character\nIn programming terms, we usually call text as string. This often are text data like names. A character vector may contain a single character , a word or a group of words. The elements must be enclosed with a single or double quotations mark.\n\nsites = c(\"Pemba Channel\", \"Zanzibar Channnel\", \"Pemba Channel\")\nis.vector(sites); class(sites)\n\n[1] TRUE\n\n\n[1] \"character\"\n\n\nWe can be sure whether the object is a string with is.character() or check the class of the object with class().\n\ncountries = c(\"Kenya\", \"Uganda\", \"Rwanda\", \"Tanzania\")\nclass(countries)\n\n[1] \"character\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nEverything inside \"\" will be considered as character, no matter if it looks like character or not"
  },
  {
    "objectID": "posts/vectorDataframe/index.html#factor",
    "href": "posts/vectorDataframe/index.html#factor",
    "title": "Understanding vector and dataframe",
    "section": "Factor",
    "text": "Factor\nFactor variables are a special case of character variables in the sense that it also contains text. However, factor variables are used when there are a limited number of unique character strings. It often represents a categorical variable. For instance, the gender will usually take on only two values, \"female\" or \"male\" (and will be considered as a factor variable) whereas the name will generally have lots of possibilities (and thus will be considered as a character variable). To create a factor variable use the factor() function:\n\n    maturity.stage &lt;- factor(c(\"I\", \"II\", \"III\", \"IV\", \"V\"))\n    maturity.stage\n\n[1] I   II  III IV  V  \nLevels: I II III IV V\n\n\nTo know the different levels of a factor variable, use levels():\n\n levels(maturity.stage)\n\n[1] \"I\"   \"II\"  \"III\" \"IV\"  \"V\"  \n\n\nBy default, the levels are sorted alphabetically. You can reorder the levels with the argument levels in the factor() function:\n\nmature &lt;- factor(maturity.stage, levels = c(\"V\", \"III\"))\n    levels(mature)\n\n[1] \"V\"   \"III\"\n\n\nCharacter strings can be converted to factors with as.factor():\n\n text &lt;- c(\"test1\", \"test2\", \"test1\", \"test1\") # create a character vector\n    class(text) # to know the class\n\n[1] \"character\"\n\n\n\n text_factor &lt;- as.factor(text) # transform to factor\n    class(text_factor) # recheck the class\n\n[1] \"factor\"\n\n\nThe character strings have been transformed to factors, as shown by its class of the type factor.\nOften we wish to take a continuous numerical vector and transform it into a factor. The function cut() takes a vector of numerical data and creates a factor based on your give cut-points. Let us make a fictional total length of 508 bigeye tuna with rnorm() function.\n\ntl.cm = rnorm(n = 508, mean = 40, sd = 18)\n\n# mosaic::plotDist(dist = \"norm\", mean = 40, sd = 18, under = F, kind = \"cdf\", add = TRUE)\n\ntl.cm |&gt;\n  tibble::as.tibble() |&gt;\n  ggstatsplot::gghistostats(x = value, binwidth = 10, test.value = 40.2, type = \"n\", normal.curve = T, centrality.type = \"p\", xlab = \"Total length (cm)\")\n\n\n\n\n\n\n\nFigure 2: Normal distribution of bigeye tuna’s tota length\n\n\n\n\n\nWe can now breaks the distribution into groups and make a simple plot as shown in ?@fig-lfq, where frequency of bigeye tuna color coded with the group size\n\ngroup = cut(tl.cm, breaks = c(0,30,60,110),\n            labels = c(\"Below 20\", \"30-60\", \"Above 60\"))\nis.factor(group)\n\n[1] TRUE\n\nlevels(group)\n\n[1] \"Below 20\" \"30-60\"    \"Above 60\"\n\n\n\nbarplot(table(group), las = 1, horiz = FALSE, col = c(\"blue\", \"green\", \"red\"), ylab = \"Frequency\", xlab = \"\")\n\n\n\n\n\n\n\nFigure 3: Length frequency of bigeye tuna"
  },
  {
    "objectID": "posts/vectorDataframe/index.html#logical",
    "href": "posts/vectorDataframe/index.html#logical",
    "title": "Understanding vector and dataframe",
    "section": "Logical",
    "text": "Logical\nLogical data (or simply logical ) represent the logical TRUE state and the logical FALSE state. Logical variables are the variables in which logical data are stored. Logical variables can assume only two states:\n\nFALSE, always represent by 0;\nTRUE, always represented by a nonzero object. Usually, the digit 1 is used for TRUE.\n\nWe can create logical variables indirectly, through logical operations, such as the result of a comparison between two numbers. These operations return logical values. For example, type the following statement at the R console:\n\n5 &gt; 3;\n\n[1] TRUE\n\n5 &lt; 3\n\n[1] FALSE\n\n\nSince 5 is indeed greater than 3, the result of the comparison is true, however, 5 is not less than 3, and hence the comparison is false. The sign &gt; and &lt; are relational operators, returning logical data types as a result.\n\n value1 &lt;- 7\n    value2 &lt;- 9\n\n\n    greater &lt;- value1 &gt; value2\n    greater\n\n[1] FALSE\n\n    class(greater)\n\n[1] \"logical\"\n\n\n\n    # is value1 less than or equal to value2?\n    less &lt;- value1 &lt;= value2\n    less\n\n[1] TRUE\n\n    class(less)\n\n[1] \"logical\"\n\n\nIt is also possible to transform logical data into numeric data. After the transformation from logical to numeric with the as.numeric() command, FALSE values equal to 0 and TRUE values equal to 1:\n\n greater_num &lt;- as.numeric(greater)\n    sum(greater)\n\n[1] 0\n\n\n\n   less_num &lt;- as.numeric(less)\n    sum(less)\n\n[1] 1\n\n\nConversely, numeric data can be converted to logical data, with FALSE for all values equal to 0 and TRUE for all other values.\n\n  x &lt;- 0\n  as.logical(x)\n\n[1] FALSE\n\n\n\n y &lt;- 5\nas.logical(y)\n\n[1] TRUE"
  },
  {
    "objectID": "posts/vectorDataframe/index.html#date-and-time",
    "href": "posts/vectorDataframe/index.html#date-and-time",
    "title": "Understanding vector and dataframe",
    "section": "Date and Time",
    "text": "Date and Time\nDate and time are also treated as vector in R\n\ndate.time = seq(lubridate::dmy(010121), \n                lubridate::dmy(250121), \n                length.out = 5)\ndate.time\n\n[1] \"2021-01-01\" \"2021-01-07\" \"2021-01-13\" \"2021-01-19\" \"2021-01-25\""
  },
  {
    "objectID": "posts/vectorDataframe/index.html#sequence-numbers",
    "href": "posts/vectorDataframe/index.html#sequence-numbers",
    "title": "Understanding vector and dataframe",
    "section": "Sequence Numbers",
    "text": "Sequence Numbers\nThere are few R operators that are designed for creating vecor of non-random numbers. These functions provide multiple ways for generating sequences of numbers\nThe colon : operator, explicitly generate regular sequence of numbers between the lower and upper boundary numbers specified. For example, generating number beween 0 and 10, we simply write;\n\nvector.seq = 0:10\nvector.seq\n\n [1]  0  1  2  3  4  5  6  7  8  9 10\n\n\nHowever, if you want to generate a vector of sequence number with specified interval, let say we want to generate number between 0 and 10 with interval of 2, then the seq() function is used\n\nregular.vector = seq(from = 0,to = 10, by = 2)\nregular.vector\n\n[1]  0  2  4  6  8 10\n\n\nunlike the seq() function and : operator that works with numbers, the rep() function generate sequence of repeated numbers or strings to create a vector\n\nid = rep(x = 3, each = 4)\nstation = rep(x = \"Station1\", each = 4)\nid;station\n\n[1] 3 3 3 3\n\n\n[1] \"Station1\" \"Station1\" \"Station1\" \"Station1\""
  },
  {
    "objectID": "posts/vectorDataframe/index.html#sequence-characters",
    "href": "posts/vectorDataframe/index.html#sequence-characters",
    "title": "Understanding vector and dataframe",
    "section": "Sequence characters",
    "text": "Sequence characters\nThe rep() function allows to parse each and times arguments. The each argument allows creation of vector that that repeat each element in a vector according to specified number.\n\nsampled.months = c(\"January\", \"March\", \"May\")\nrep(x = sampled.months, each = 3)\n\n[1] \"January\" \"January\" \"January\" \"March\"   \"March\"   \"March\"   \"May\"    \n[8] \"May\"     \"May\"    \n\n\nBut the times argument repeat the whole vector to specfied times\n\nrep(x = sampled.months, times = 3)\n\n[1] \"January\" \"March\"   \"May\"     \"January\" \"March\"   \"May\"     \"January\"\n[8] \"March\"   \"May\""
  },
  {
    "objectID": "posts/vectorDataframe/index.html#generating-normal-distribution",
    "href": "posts/vectorDataframe/index.html#generating-normal-distribution",
    "title": "Understanding vector and dataframe",
    "section": "Generating normal distribution",
    "text": "Generating normal distribution\nThe central limit theorem that ensure the data is normal distributed is well known to statistician. R has a rnorm() function which makes vector of normal distributed values. For example to generate a vector of 40 sea surface temperature values from a normal distribution with a mean of 25, and standard deviation of 1.58, we simply type this expression in console;\n\nsst = rnorm(n = 40, mean = 25,sd = 1.58)\nsst\n\n [1] 23.04693 24.99349 25.68869 23.84683 25.69666 24.93500 23.44773 26.62016\n [9] 27.67181 26.30010 22.03781 25.77229 23.92286 23.35629 27.66600 28.08170\n[17] 22.16890 24.93247 24.46477 25.94592 24.50469 28.61894 21.42219 26.88232\n[25] 26.96524 22.87907 26.34715 22.76567 24.19697 25.49118 29.21119 22.55112\n[33] 23.87877 25.75880 24.54350 23.59964 22.44975 25.43948 25.33276 23.46390"
  },
  {
    "objectID": "posts/vectorDataframe/index.html#rounding-off-numbers",
    "href": "posts/vectorDataframe/index.html#rounding-off-numbers",
    "title": "Understanding vector and dataframe",
    "section": "Rounding off numbers",
    "text": "Rounding off numbers\nThere are many ways of rounding off numerical number to the nearest integers or specify the number of decimal places. the code block below illustrate the common way to round off:\n\nchl = rnorm(n = 20, mean = .55, sd = .2)\nchl |&gt; round(digits = 2)\n\n [1] 0.32 0.74 0.58 0.31 0.59 0.85 0.76 0.53 0.32 0.72 0.63 0.38 0.47 0.52 0.98\n[16] 0.77 0.87 0.73 0.48 0.53"
  },
  {
    "objectID": "posts/vectorDataframe/index.html#number-of-elements-in-a-vector",
    "href": "posts/vectorDataframe/index.html#number-of-elements-in-a-vector",
    "title": "Understanding vector and dataframe",
    "section": "Number of elements in a vector",
    "text": "Number of elements in a vector\nSometimes you may have a long vector and want to know the numbers of elements in the object. R has length() function that allows you to query the vector and print the answer\n\nlength(chl)\n\n[1] 20"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Data Visualization\n\n\n\n\n\n\n\n\nMay 5, 2024\n\n\n\n\n\n\n\nData cleaning, merging, and appending\n\n\n\n\n\n\n\n\nMay 1, 2024\n\n\n\n\n\n\n\nTidying Data frame\n\n\n\n\n\n\n\n\nApr 29, 2024\n\n\n\n\n\n\n\nUnderstanding Descriptive Statistics\n\n\n\n\n\n\n\n\nApr 25, 2024\n\n\n\n\n\n\n\nBasic plots with ggplot2\n\n\n\n\n\n\n\n\nApr 1, 2024\n\n\n\n\n\n\n\nVisualizing data with grammar of graphics\n\n\n\n\n\n\n\n\nMar 25, 2024\n\n\n\n\n\n\n\nImporting table files into R\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\n\n\n\n\n\nUnderstanding vector and dataframe\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\n\n\n\n\n\nMastering Data Structures in R\n\n\n\n\n\n\n\n\nFeb 3, 2024\n\n\n\n\n\n\n\nUnderstanding and using Data types in R\n\n\n\n\n\n\n\n\nJan 26, 2024\n\n\n\n\n\n\n\nGetting Started with R and RStudio\n\n\n\n\n\n\n\n\nJan 24, 2024\n\n\n\n\n\n\n\nThe basics of R programming\n\n\n\n\n\n\n\n\nJan 24, 2024\n\n\n\n\n\n\n\nThe basics of R and Rstudio\n\n\n\n\n\n\n\n\nJan 24, 2024\n\n\n\n\n\n\n\nCreating a Timeline graphic using R and ggplot2\n\n\n\n\n\n\n\n\nNov 24, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "terms.html",
    "href": "terms.html",
    "title": "DATIKA",
    "section": "",
    "text": "66666"
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Agenda and Presentations for covered topics",
    "section": "",
    "text": "Our Agenda\n\nAn introduction\nGet familiar with R and Rstudio\nData types\nData structures\nReading and writing data in Rstudio\nTidying Data with tidyverse\nPlotting\n\nPlotting and Visualization_A\nPlotting and Visualization_B\nPlotting and Visualization_C\n\nData Manipulation\nDescriptive Statistics\nInferential Statistics\nModelling and simulation\nSpatial Handling and Analysis\nFurther topics\n\nGIt and Github\nReproducibility with Quarto\nWebsites and blog\nUsing python from Rstudio\nGenerating HTML, PDF and Word Reports"
  },
  {
    "objectID": "posts/dataTypes/index.html",
    "href": "posts/dataTypes/index.html",
    "title": "Understanding and using Data types in R",
    "section": "",
    "text": "In the realm of data science and statistical analysis, mastering data types is akin to understanding the building blocks of a language. In R, a powerful statistical computing language, data types form the foundation upon which all analyses are conducted. From integers to dates, each data type serves a unique purpose and understanding their nuances is critical for efficient and accurate data manipulation.\nUnderstanding and effectively utilizing these core data types in R is essential for performing data analysis, visualization, and modeling tasks. Mastery of data types empowers data scientists to manipulate data efficiently and extract valuable insights from complex datasets. Whether performing arithmetic operations, manipulating text, or handling temporal information, the versatility of R’s data types makes it a powerful tool for data analysis and statistical computing.\nIn this guide, we will delve into the core data types in R, exploring their characteristics and providing illustrative examples. Before we dive in, let pause for a moment and watch video in Figure 1\n\n\n\n\n\n\nFigure 1: Primary data types in R\n\n\n\n\n\nIntegers are whole numbers without any decimal or fractional component. In R, integers are represented by the integer class. They are commonly used for indexing and counting operations.\n\nExample 1  \n# Creating an integer variable\nx &lt;- 5L\nclass(x) # Output: \"integer\"\n\n# Arithmetic operations with integers\ny &lt;- x + 3\n\n\n\n\nNumeric data type, also known as double in other programming languages, represents numbers with decimal points. Numeric data types are used for most mathematical calculations and statistical operations in R.\n\nExample 2  \n# Creating a numeric variable\nheight &lt;- 175.5\nclass(height) # Output: \"numeric\"\n\n# Arithmetic operations with numeric variables\nbmi &lt;- weight / (height^2)\n\n\n\n\nCharacter data type represents textual data such as strings of letters, words, or sentences. In R, character values are enclosed in either single or double quotes.\n\nExample 3  \n# Creating a character variable\nname &lt;- \"John Doe\"\nclass(name) # Output: \"character\"\n\n# Concatenating character strings\ngreeting &lt;- paste(\"Hello\", name)\n\n\n\n\nLogical data type, often referred to as Boolean, represents binary values: TRUE or FALSE. Logical values are fundamental in controlling program flow and making decisions based on conditions.\n\nExample 4  \n# Creating logical variables\nis_adult &lt;- TRUE\nclass(is_adult) # Output: \"logical\"\n\n# Conditional statements with logical variables\nif (is_adult) {\n  print(\"You are an adult.\")\n} else {\n  print(\"You are not an adult.\")\n}\n\n\n\n\nFactor data type is used to represent categorical data in R. Factors are stored as integers with associated labels, making them efficient for statistical modeling and analysis.\n\nExample 5  \n# Creating a factor variable\ngender &lt;- factor(c(\"Male\", \"Female\", \"Female\", \"Male\"))\nclass(gender) # Output: \"factor\"\n\n# Summary statistics with factors\ntable(gender)\n\n\n\n\nDate and time data types are crucial for handling temporal information in R. R provides specialized classes for dates (Date) and date-time values (POSIXct, POSIXlt).\n\nExample 6  \n# Creating a date variable\ntoday &lt;- as.Date(\"2024-04-25\")\nclass(today) # Output: \"Date\"\n\n# Date arithmetic\nnext_week &lt;- today + 7\n\n# Creating a POSIXct variable (date-time)\ncurrent_time &lt;- Sys.time()\nclass(current_time) # Output: \"POSIXct\"\n\nIn this post we learned about different R data types and what kind of data do they hold. Data type is very important concept in programming and can not be ignored. We have explained about each data type with example in this article."
  },
  {
    "objectID": "posts/dataTypes/index.html#introduction",
    "href": "posts/dataTypes/index.html#introduction",
    "title": "Understanding and using Data types in R",
    "section": "",
    "text": "In the realm of data science and statistical analysis, mastering data types is akin to understanding the building blocks of a language. In R, a powerful statistical computing language, data types form the foundation upon which all analyses are conducted. From integers to dates, each data type serves a unique purpose and understanding their nuances is critical for efficient and accurate data manipulation.\nUnderstanding and effectively utilizing these core data types in R is essential for performing data analysis, visualization, and modeling tasks. Mastery of data types empowers data scientists to manipulate data efficiently and extract valuable insights from complex datasets. Whether performing arithmetic operations, manipulating text, or handling temporal information, the versatility of R’s data types makes it a powerful tool for data analysis and statistical computing.\nIn this guide, we will delve into the core data types in R, exploring their characteristics and providing illustrative examples. Before we dive in, let pause for a moment and watch video in Figure 1\n\n\n\n\n\n\nFigure 1: Primary data types in R\n\n\n\n\n\nIntegers are whole numbers without any decimal or fractional component. In R, integers are represented by the integer class. They are commonly used for indexing and counting operations.\n\nExample 1  \n# Creating an integer variable\nx &lt;- 5L\nclass(x) # Output: \"integer\"\n\n# Arithmetic operations with integers\ny &lt;- x + 3\n\n\n\n\nNumeric data type, also known as double in other programming languages, represents numbers with decimal points. Numeric data types are used for most mathematical calculations and statistical operations in R.\n\nExample 2  \n# Creating a numeric variable\nheight &lt;- 175.5\nclass(height) # Output: \"numeric\"\n\n# Arithmetic operations with numeric variables\nbmi &lt;- weight / (height^2)\n\n\n\n\nCharacter data type represents textual data such as strings of letters, words, or sentences. In R, character values are enclosed in either single or double quotes.\n\nExample 3  \n# Creating a character variable\nname &lt;- \"John Doe\"\nclass(name) # Output: \"character\"\n\n# Concatenating character strings\ngreeting &lt;- paste(\"Hello\", name)\n\n\n\n\nLogical data type, often referred to as Boolean, represents binary values: TRUE or FALSE. Logical values are fundamental in controlling program flow and making decisions based on conditions.\n\nExample 4  \n# Creating logical variables\nis_adult &lt;- TRUE\nclass(is_adult) # Output: \"logical\"\n\n# Conditional statements with logical variables\nif (is_adult) {\n  print(\"You are an adult.\")\n} else {\n  print(\"You are not an adult.\")\n}\n\n\n\n\nFactor data type is used to represent categorical data in R. Factors are stored as integers with associated labels, making them efficient for statistical modeling and analysis.\n\nExample 5  \n# Creating a factor variable\ngender &lt;- factor(c(\"Male\", \"Female\", \"Female\", \"Male\"))\nclass(gender) # Output: \"factor\"\n\n# Summary statistics with factors\ntable(gender)\n\n\n\n\nDate and time data types are crucial for handling temporal information in R. R provides specialized classes for dates (Date) and date-time values (POSIXct, POSIXlt).\n\nExample 6  \n# Creating a date variable\ntoday &lt;- as.Date(\"2024-04-25\")\nclass(today) # Output: \"Date\"\n\n# Date arithmetic\nnext_week &lt;- today + 7\n\n# Creating a POSIXct variable (date-time)\ncurrent_time &lt;- Sys.time()\nclass(current_time) # Output: \"POSIXct\"\n\nIn this post we learned about different R data types and what kind of data do they hold. Data type is very important concept in programming and can not be ignored. We have explained about each data type with example in this article."
  },
  {
    "objectID": "posts/dataTypes/index.html#references",
    "href": "posts/dataTypes/index.html#references",
    "title": "Understanding and using Data types in R",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "posts/datastructures/index.html",
    "href": "posts/datastructures/index.html",
    "title": "Mastering Data Structures in R",
    "section": "",
    "text": "In data analysis and statistical computing, mastering data structures is essential for efficient data manipulation and analysis. In R, a powerful language for statistical computing and graphics, two fundamental data structures are vectors and data frames. Additionally, the newer tibble data structure offers enhanced features for data manipulation and visualization. In this comprehensive guide, we will explore these data structures in detail, providing illustrative examples along the way.Before we dive in, let pause for a moment and watch video in Figure 1\n\n\n\n\n\n\nFigure 1: Primary data structure in R"
  },
  {
    "objectID": "posts/datastructures/index.html#introduction",
    "href": "posts/datastructures/index.html#introduction",
    "title": "Mastering Data Structures in R",
    "section": "",
    "text": "In data analysis and statistical computing, mastering data structures is essential for efficient data manipulation and analysis. In R, a powerful language for statistical computing and graphics, two fundamental data structures are vectors and data frames. Additionally, the newer tibble data structure offers enhanced features for data manipulation and visualization. In this comprehensive guide, we will explore these data structures in detail, providing illustrative examples along the way.Before we dive in, let pause for a moment and watch video in Figure 1\n\n\n\n\n\n\nFigure 1: Primary data structure in R"
  },
  {
    "objectID": "posts/datastructures/index.html#vectors",
    "href": "posts/datastructures/index.html#vectors",
    "title": "Mastering Data Structures in R",
    "section": "Vectors:",
    "text": "Vectors:\nVectors are one-dimensional arrays that can hold numeric, character, logical, or other atomic data types. They are the simplest and most basic data structure in R.\n\nCreating Vectors:\nCreating vectors in R is straightforward using the c() function, which concatenates elements into a vector.\n# Creating a numeric vector\nnumeric_vector &lt;- c(1, 2, 3, 4, 5)\n\n# Creating a character vector\ncharacter_vector &lt;- c(\"apple\", \"banana\", \"orange\", \"grape\", \"pineapple\")\n\n# Creating a logical vector\nlogical_vector &lt;- c(TRUE, FALSE, TRUE, TRUE, FALSE)\n\n\nUsing Vectors to Create Data Frames:\nData frames are two-dimensional data structures that resemble tables, where each column can be a different data type. They are commonly used for storing and analyzing structured data.\n# Using vectors to create a data frame\ndata &lt;- data.frame(\n  numeric_col = numeric_vector,\n  character_col = character_vector,\n  logical_col = logical_vector\n)\n\n# View the created data frame\nprint(data)"
  },
  {
    "objectID": "posts/datastructures/index.html#data-frames",
    "href": "posts/datastructures/index.html#data-frames",
    "title": "Mastering Data Structures in R",
    "section": "Data Frames:",
    "text": "Data Frames:\nData frames are the workhorse of R for storing tabular data. They are similar to matrices but offer more flexibility, as each column can be of a different data type.\n\nCreating Data Frames:\nData frames can be created directly using the data.frame() function, where each column is specified as a vector.\n# Creating a data frame directly\nstudent_data &lt;- data.frame(\n  name = c(\"John\", \"Alice\", \"Bob\", \"Emma\", \"Michael\"),\n  age = c(25, 23, 27, 22, 24),\n  grade = c(\"A\", \"B\", \"B\", \"C\", \"A\")\n)\n\n# View the created data frame\nprint(student_data)\n\nUsing Tibbles:\nTibbles are a modern alternative to data frames, introduced by the tidyverse ecosystem. They are more user-friendly, provide enhanced printing, and have better support for data analysis pipelines.\n# Creating a tibble from vectors\nlibrary(tibble)\n\n# Creating a tibble directly\nstudent_tibble &lt;- tibble(\n  name = c(\"John\", \"Alice\", \"Bob\", \"Emma\", \"Michael\"),\n  age = c(25, 23, 27, 22, 24),\n  grade = c(\"A\", \"B\", \"B\", \"C\", \"A\")\n)\n\n# View the created tibble\nprint(student_tibble)"
  },
  {
    "objectID": "posts/datastructures/index.html#conclusion",
    "href": "posts/datastructures/index.html#conclusion",
    "title": "Mastering Data Structures in R",
    "section": "Conclusion:",
    "text": "Conclusion:\nUnderstanding data structures such as vectors, data frames, and tibbles is crucial for effective data manipulation and analysis in R. Whether you’re working with numeric data, text data, or logical data, these data structures provide the foundation for organizing and analyzing your data efficiently. By mastering these data structures, you’ll be well-equipped to tackle a wide range of data analysis tasks in R.\nIn this guide, we’ve covered how to create vectors, use them to construct data frames, and introduced the newer tibble data structure. Armed with this knowledge, you’re ready to dive deeper into the world of data analysis and unlock the full potential of R for your projects. Whether you’re a beginner or an experienced R user, mastering these fundamental data structures will pave the way for more advanced data analysis and modeling techniques."
  },
  {
    "objectID": "posts/datastructures/index.html#references",
    "href": "posts/datastructures/index.html#references",
    "title": "Mastering Data Structures in R",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "posts/basicplots/index.html",
    "href": "posts/basicplots/index.html",
    "title": "Basic plots with ggplot2",
    "section": "",
    "text": "The ggplot2 package provides a set of functions that mirror the Grammar of Graphics (Wickham, 2016), enabling you to efficaciously specify what you want a plot to look like. To have a glimpse of ggplot2, we start with five basic types of plots that are familiar to most people. These include: scatterplot, linegraphs, boxplots, histograms, and barplots. The first four graphs works with quantitative data and barplots are appropriate for categorical data.\nThus, understanding the type of data is inevitable before you throw the variable into ggplot2 to make plot for you. In this post, we will cover the most common plot types, such as line plots, histograms, pie charts, scatter plots, and bar plots, along with several other plot types that build upon these.\n\n\nScatterplots are also called bivariate, allows you to visualize the association between two numerical variables. They are among the widely used plot in fisheries science particularly when looking for association between length and weight of a particular fish. Probably you might have come across a scatterplot like the one in Figure 1 that base R was used, but probably you have not made one based on the fundamental theorem of grammar of graphics.\n\n\n\n\n\n\n\n\nFigure 1: Length and weight relationship of Chinook Salmon sampled in Atantic Ocean\n\n\n\n\n\nWe are going to visualize the relationship between length and weight of fish measured in the coastal waters of Kenya. We use the tidy_LFQ_sample_4.csv file. Let’s import the dataset in the session using read_csv function.\n\nlfq4 = read_csv(\"../data/tidy/tidy_LFQ_sample_4.csv\")\n\nThis file contain length and weight measurements along with sex sampled in Mombasa and Voi from March 2016 to September 2020 (Table 1).\n\n\n\n\nTable 1: Sample length and weight of sampled fish\n\n\n\nsitedatetl_mmfl_mmwt_gmsexMombasa2019-04-0518416959.50MMombasa2019-04-0518516954.71MMombasa2019-04-0514513424.15MVoi2020-09-1118917465.88FVoi2020-09-1116214736.35FVoi2020-09-1116815346.13F\n\n\n\n\n\nLet’s now dive into the code of using the *grammar of graphics to create the scatterplot. We use the ggplot() function from ggplot2** package. The code highlighted in the chunk below was used to plot Figure 2\n\nggplot(data = lfq4, aes(x = tl_mm, y = wt_gm))+\n  geom_point()+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")\n\n\n\n\n\n\n\nFigure 2: Length and weight relationship\n\n\n\n\n\nLet’s explore the code above piece-by-piece\n\nThe plotting in ggplot2 begin with ggplot() function, where the two components of grammar of graphics are required. in the data component we specified the dataset by setting data = lfq4. Then the second argument aesthetic that map the plot with coordinate was set by aes(x = tl_mm, y = wt_gm)). In a nutshell, the aes() define the variable – axis specifications.\nWe then added a layer to the ggplot() function call using the + sign. The added layer specify the third part of the *grammar—the geometric component. Because we want to plot scatterplot, the appropriate geom for this case is the geom_point().\nadded a layer labs that allows us to label axis with meaningful axis titles\n\nadding regression line you can simply add the regression line by adding a geom_smooth() layer. However, Figure 2 is non-linear and hence we need to specify the modal that fits the data, the loess model is mostly used for non-linear data. Therefore, we parse the argumentmethod = \"loess\" to draw a non-linear regression line but also parse an argument se = FALSE to prevent plotting confidence error.\n\n  ggplot(data = lfq4, aes(x = tl_mm, y = wt_gm))+\n  geom_point()+\n  geom_smooth(method = \"loess\", se = FALSE)+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")\n\n\n\n\n\n\n\nFigure 3: Length and weight relationship with non-linear regression line\n\n\n\n\n\nIf we want to add a linear regression line i the scatter plot instead of the non linear shown in (ig-scatter2?), we simply replace method = \"loess\" with method = \"lm\"\n\n  ggplot(data = lfq4, aes(x = tl_mm, y = wt_gm))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")\n\n\n\n\n\n\n\nFigure 4: Length and weight relationship with linear regression line\n\n\n\n\n\nThe linear regression line we added in Figure 4 does not fit the data points. That’s is nature of the length and weight measurements of most fishes as their growth is allometric and not isometric. To make use of the linear model in such data points, we often log-transform the data points first and replot. But in ggplot framework, you do need to do that but simply add the scale_x_log10 and scale_y_log10 layer\n\n  ggplot(data = lfq4, aes(x = tl_mm, y = wt_gm))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")+\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\n\n\nFigure 5: Log-transformed length and weight relationship with linear regression line\n\n\n\n\n\nKnowing whether the relationship is positive or negative and whether is linear or non linear is one thing, but people would like to know the strength of the relationship that you have simply presented in Figure 5. Luckily, Pedro Aphalo developed a ggpmisc package (Aphalo, 2016), which extend the statistical function of ggplot2. By simply adding a layer ggpmisc::stat_correlation() in Figure 5, the function generates labels for correlation coefficients and p-value, coefficient of determination (R^2) for method “pearson” and number of observations and add them into the plot (Figure 6).\n\n  ggplot(data = lfq4, aes(x = tl_mm, y = wt_gm))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")+\n  scale_x_log10() +\n  scale_y_log10()+\n  ggpmisc::stat_correlation()\n\n\n\n\n\n\n\nFigure 6: Log-transformed length and weight relationship with linear regression line with correlation coefficient\n\n\n\n\n\nWe might be interested to distinguish the data points and the regression line based on the site. We can do that by adding the color argument in the aesthetic, which change from aes(x = tl_mm, y = wt_gm) to aes(x = tl_mm, y = wt_gm, color = site). The argument color = site will force the data points and the regression line to adhere to colors based on the site but the points and line are plotted on the same plot.\n\n  ggplot(data = lfq4, aes(x = tl_mm, y = wt_gm, color = site))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")+\n  scale_x_log10() +\n  scale_y_log10()+\n  ggpmisc::stat_correlation()\n\n\n\n\n\n\n\nFigure 7: Log-transformed length and weight relationship with linear regression line by site\n\n\n\n\n\nLooking on Figure 7, it is clear that sample from Mombasa station has relatively bigger and heavier fish than those sampled from Voi. But the problem with Figure 7 is that most of the Mombasa data points are masked by Voi data points, which are overlaid on Mombasa data points. We can overcome the issue of point cluttering by simply adding a transparency level in point with alpha = .2.\n\n  ggplot(data = lfq4, aes(x = tl_mm, y = wt_gm, color = site))+\n  geom_point(alpha = .2)+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")+\n  ggpmisc::stat_correlation()+\n  scale_x_log10() +\n  scale_y_log10()+\n  ggpmisc::stat_correlation()\n\n\n\n\n\n\n\nFigure 8: Log-transformed length and weight relationship with linear regression line by site. Points density is highlighted with transparency\n\n\n\n\n\nSometimes you may wish to plot Figure 8 as separate plot shown in Figure 9. That’s is achieved with facet_wrap function, which facet plots based on the levels that are in the variable that is specified. For instance, in our case, the variable chosen is site and there are two sites–Voi and Mombasa. Therefore by simply adding a facet_wrap(~site) layer will force ggplot to make two plots\n\n  ggplot(data = lfq4, aes(x = tl_mm, y = wt_gm))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")+\n  scale_x_log10() +\n  scale_y_log10()+\n  ggpmisc::stat_correlation()+\n  facet_wrap(~site, nrow = 1)\n\n\n\n\n\n\n\nFigure 9: Faceted Log-transformed length and weight relationship with linear regression line\n\n\n\n\n\n\n\n\nThe next basic graph of ggplot2 is the linegraph. Line graphs is similar to drawing points, except that it connects the points with line. often times you don’t show the points. Let’s illustrate how to create linegraphs using catch data in the region. We first load the dataset in the session\n\nlanding.countries = read_csv(\"../data/tidy/landings_wio_country.csv\", skip = 4)\n\nThe landing.countries dataset contain 660 records of landed fisheries catch information recorded between 1950 and 2015 from Somalia, Kenya, Mozambique, South Africa, Madagascar, Mauritius, Seychelles, Mayotte, Tanzania and Zanzibar.\n\nlanding.countries %&gt;% \n  FSA::headtail() |&gt; \n  flextable::flextable() |&gt; \n  flextable::autofit()\n\nnameyearcatchepochKenya1,95019,1541,960Kenya1,95121,3181,960Kenya1,95219,1261,960Madagascar2,013266,9532,010Madagascar2,014138,4782,010Madagascar2,015145,6292,010\n\n\nLinegraphs are used to show time series data. Its inappropriate to use the linegraphs for data that has no clear sequential ordering and should be continuous and not discrete data type. The internal structure of the catch dataset we just loaded indicate that with exception of country’s name, year, catch and epoch are numeric values.\n\nlanding.countries %&gt;% \n  glimpse() \n\nRows: 660\nColumns: 4\n$ name  &lt;chr&gt; \"Kenya\", \"Kenya\", \"Kenya\", \"Kenya\", \"Kenya\", \"Kenya\", \"Kenya\", \"…\n$ year  &lt;dbl&gt; 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960…\n$ catch &lt;dbl&gt; 19154, 21318, 19126, 20989, 17541, 19223, 23297, 28122, 28819, 2…\n$ epoch &lt;dbl&gt; 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960…\n\n\nLet’s us plot the annual landings of fish over the period with ggplot. Like the scatterplot we made earlier, where supply the data frame in data argument and specified the aesthetic mapping with x and y coordinates, but instead of using geom_point(), we use the geom_line(). The code to make the line graph of annual landing in the WIO region shown in Figure 10 is written as;\n\nggplot(data = landing.countries,\n       aes(x = year, y = catch)) +\n  geom_line()+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 10: Annual alnding in the WIO region\n\n\n\n\n\nAlthough we added a geom_line, but we notice that Figure 10 display a plot which we did not expect. The problem is that line for the ten countries are all lumped together and result in the chaotic situation. For illustration purpose, I will use the catch data from Mauritius. Let’s filter Mauritius’ catch information from the landing.countries dataset and display its rows and variables;\n\nmauritius.landings = landing.countries %&gt;% \n  filter(name == \"Mauritius\")\n\n\nmauritius.landings %&gt;% \n  FSA::headtail() |&gt; \n  flextable::flextable() |&gt; \n  flextable::autofit()\n\nnameyearcatchepochMauritius1,950183,0821,960Mauritius1,951216,1511,960Mauritius1,952181,8221,960Mauritius2,01315,7972,010Mauritius2,01413,8792,010Mauritius2,01516,3732,010\n\n\nThere are only 66 rows in Mauritius which are equivalent to 66 records each per year from 1950 to 2015. Let’s use the mauritius.landings dataset to plot\n\nggplot(data = mauritius.landings,\n       aes(x = year, y = catch)) +\n  geom_line()+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 11: Mauritius’ annual landing\n\n\n\n\n\nOften times you find that linegraphs has points. You can also do that in ggplot environment by adding a geom_point layer\n\nggplot(data = mauritius.landings,\n       aes(x = year, y = catch)) +\n  geom_line()+\n  geom_point()+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 12: Mauritius’ annual landing\n\n\n\n\n\nYou can also customize the appearance of the line and point by parsing the color argument in the geom_point and geom_line layers\n\nggplot(data = mauritius.landings,\n       aes(x = year, y = catch)) +\n  geom_line(color = \"black\")+\n  geom_point(color = \"red\")+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 13: Mauritius’ annual landing\n\n\n\n\n\nThe problem we faced in Figure 10 is that catch data for all ten countries were pooled together, and the plot was not informative. But what is we want to compare the trend of catch among the countries. That is achieved by simply distinguishing the color layer for each country. That is done by adding an argument color=name in aes function as the code below highlight\n\nggplot(data = landing.countries,\n       aes(x = year, y = catch, color = name)) +\n  geom_line()+\n  # geom_point(color = \"red\")+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 14: Annual landing by countries in the WIO region\n\n\n\n\n\nThe landings from South Africa is far higher than the rest of the WIO’s countries, which overshadow the appearance of other countries (Figure 14). There several approaches to resolve this issues where some countries have low catch values while others have relatively very high catches. For our case, we have decided to remove South Africa from the plot. We can do that by negating the selection with filter function from dplyr package. By parsing filter(!name == \"South Africa\"), note the exclamation mark before name tell to reverse selection and therefore select all countries except South Africa.\n\nother.countries = landing.countries %&gt;% \n  filter(!name == \"South Africa\")\n\nWe then plot and parse the argument data = other.countries instead of data = landing.countries to make Figure 15.\n\nggplot(data = other.countries,\n       aes(x = year, y = catch, color = name)) +\n  geom_line()+\n  # geom_point(color = \"red\")+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 15: Annual landing by countries in the WIO region with South Africa ommited\n\n\n\n\n\nWe notice that Tanzania and Zanzibar are presented as separate entity. Although the two states report to the FAO separate, but would be interested to know the landing of the combined Tanzania and Zanzibar catches. But before we combine these two states, lets see how their catches vary over the period. First we need to select only records for Tanzania and Zanzibar using a filter function as illustrated below;\n\ntanzania.zanzibar = landing.countries %&gt;% \n  filter(name %in% c(\"Tanzania\", \"Zanzibar\")) \n\nOnce we have created a tanzania.zanzibar object, we can use it to make plots that compare catch trend of Tanzania and Zanzibar over the last 66 years. The code in this chunk is used to make Figure 16\n\n  ggplot(data = tanzania.zanzibar,\n       aes(x = year, y = catch, color = name)) +\n  geom_line()+\n  # geom_point(color = \"red\")+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 16: Annual landing for mainland Tanzania and Zanzibar\n\n\n\n\n\n\nlanding.countries %&gt;% \n  mutate(name = str_replace(string = name, \n                            pattern = \"Zanzibar\", \n                            replacement = \"Tanzania\")) %&gt;% \n  filter(!name == \"South Africa\") %&gt;% \n  group_by(name, year) %&gt;% \n  summarise(catch_new = sum(catch, na.rm = TRUE)) %&gt;% \n  ggplot(\n       aes(x = year, y = catch_new, color = name)) +\n  geom_line()+\n  # geom_point(color = \"red\")+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 17: Annual landing for WIO where mainland Tanzania and Zanzibar are combined\n\n\n\n\n\n\n\n\nThe geom_area method is used to create an area plot. It can be used as a component in the ggplot method. The alpha parameter in the geom_area method is used to depict the opacity of a genome, the value ranges from zero to one integral values. In case, we choose a lower value, this means that a more transparent color version will be chosen to depict the plot and its smoothness. We have used the value for the alpha parameter to be one by two means it is somewhat translucent in nature.\n\n  ggplot(data = tanzania.zanzibar,\n       aes(x = year, y = catch, fill = name)) +\n  geom_area(alpha = 0.6, position=\"identity\")+\n  # geom_point(color = \"red\")+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 18: Area plot showing Annual landing for mainland Tanzania and Zanzibar\n\n\n\n\n\n\n\n\nA histogram is a plot that can be used to examine the shape and spread of continuous data. It looks very similar to a bar graph and organized in intervals or classes. It divides the range of the data into bin equal intervals (also called bins or classes), count the number of observations in each bin, and display the frequency distribution of observations as a bar plot. Such histogram plots provide valuable information on the characteristics of the data, such as the central tendency, the dispersion and the general shape of the distribution. With lfq4 dataset, we can plot the histogram of tl_mm. Since histogram works for single variable that contains quantitative values, you can not bother looking for relationship as we have seen in previous plots, but histogram offers an opportunity to answer question like\n\nWhat are the smallest and largest values of tl_mm?\nWhat is the center value? 3 How does these values spread out?\n\nWe can make a histogram shown in Figure 19 by simply setting aes(x = tl_mm) and add geom_histogram(). Within the geom_histogram(), we simply specify the number of bins bins = 30, fill color for the colum and also the color separating each columns of the histogram with col == \"red\" and fill = \"red\". However, a word of caution regarding histograms—bin size matters. The reproducible code to plot Figure 19 is written as;\n\n  ggplot(data = lfq4,\n       aes(x = tl_mm)) +\n  geom_histogram(bins = 30, fill = \"red\", color = \"red\", alpha = 0.4)+\n  labs(x = \"Total length (mm)\", y = \"Frequency\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 19: Histogram of total length\n\n\n\n\n\nThe resulting histogram gives us an idea of the range of total length of fish we can expect from the sample. You may be interested to compare histogram of the data values sampled from two or sites. For example, in our case, we are interested to compare the distribution of total length using samples collected from Mombasa and Voi sites. We simply add the fill = site argument in the aes function\n\n  ggplot(data = lfq4,\n       aes(x = tl_mm, fill = site)) +\n  geom_histogram(bins = 50, alpha = 0.6)+\n  labs(x = \"Total length (mm)\", y = \"Frequency\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 20: Histogram of total length by sites\n\n\n\n\n\nThe histogram of Mombasa and Voi is plotted as shown in Figure 20, however, despite the transparency level of the bins is set to 0.6 (alpha = .6), yet the bins from Mombasa are masked with bins from Voi. The voi bins are plotted over the Mombasa ones and prevent us to visualize the underneath Mombasa bins. To correct for this issue, we need to parse position = \"identity\"in the geom_bin, which create an different color where the Mombasa and Voi bins are intersected.\n\n  ggplot(data = lfq4,\n       aes(x = tl_mm, fill = site)) +\n  geom_histogram(bins = 50, alpha = 0.6, position = \"identity\")+\n  labs(x = \"Total length (mm)\", y = \"Frequency\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 21: Histogram of total length by sites\n\n\n\n\n\n\n\n\nIt is often useful to visualise the distribution of a numerical variable. Comparing the distributions of different groups can lead to important insights. Visualising distributions is also essential when checking assumptions used for various statistical tests (sometimes called initial data analysis). In this section we will illustrate how this can be done using the diamonds data from the ggplot2 package, which you started to explore in Chapter 2.\nAn advantage with frequency polygons is that they can be used to compare groups, e.g. diamonds with different cuts, without facetting:\n\n  ggplot(data = lfq4,\n       aes(x = tl_mm, color = site)) +\n  geom_freqpoly(alpha = 0.6, position = \"identity\")+\n  labs(x = \"Total length (mm)\", y = \"Frequency\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 22: Frequency polygon of total length by sites\n\n\n\n\n\nIt is clear from this figure that the total length of fish from Voi is larger in size than those from Mombasa. The polygons have roughly the same shape, except the shape of Mombasa a long right tail indicating the presence of outlier points.\n\n\n\nIn some cases, we are more interested in the shape of the distribution than in the actual counts in the different bins. Density plots are similar to frequency polygons but show an estimate of the density function of the underlying random variable. These estimates are smooth curves that are scaled so that the area below them is 1 (i.e. scaled to be proper density functions):\n\n#|\n\n  ggplot(data = lfq4,\n       aes(x = tl_mm, fill = site)) +\n  geom_density(alpha = 0.4, position = \"identity\")+\n  labs(x = \"Total length (mm)\", y = \"Frequency\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 23: Density plot of total length by sites\n\n\n\n\n\nFrom Figure 23, it’s clear that small size fish tend to have better total length, which wasn’t obvious from the frequency polygons. However, the plot does not provide any information about how common different total length are.\n\n\n\nThe boxplot is a standardized way of displaying the distribution of data based on the five number summary: minimum, first quantile, median, third quantile, and maximum. Boxplots are useful for detecting outliers and for comparing distributions. These five number summary also called the 25th percentile, median, and 75th percentile of the quantitative data. The whisker (vertical lines) capture roungly 99% of a distribution, and observation outside this range are plotted as points representing outliers as shown in Figure 24.\n\n\n\n\n\n\n\n\nFigure 24: Conceputal boxplot diagram\n\n\n\n\n\nBoxplots is one of statistical plot that present continuous variable and in ggplot a geom_boxplot() function is dedicated for that. The aes function always have at least two arguments. The first argument should be a categrial variable and the second one is numeric.\n\n  ggplot(data = lfq4,\n       aes(x = site, y = tl_mm)) +\n  geom_boxplot(alpha = 0.6, position = \"identity\")+\n  labs(x = \"Sites\", y = \"Total length (mm)\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 25: Boxplot of total length by sites\n\n\n\n\n\nthe geom_boxplot() has outlier_ arguments that allows to highlight and modify the color, shape, size, alpha … etc of outliers —extreme observation. For instance, you can highlight the outlier with;\n\n  ggplot(data = lfq4,\n       aes(x = site, y = tl_mm, fill = site)) +\n  geom_boxplot(alpha = 0.6, position = \"identity\", outlier.colour = \"red\", outlier.color = )+\n  labs(x = \"Sites\", y = \"Total length (mm)\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 26: Boxplot of total length by sites\n\n\n\n\n\nWe can also map the fill and color to variable in to distinguish boxplot. for example, we can specify the fill = site argument in the aes() to fill the boxplot based on site.\n\n  ggplot(data = lfq4,\n       aes(x = site, y = tl_mm, fill = site)) +\n  geom_boxplot(alpha = 0.6, position = \"identity\", outlier.colour = \"red\", outlier.color = )+\n  labs(x = \"Sites\", y = \"Total length (mm)\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 27: Boxplot of total length and color to distinguish sites\n\n\n\n\n\nWe can add the points on top of the boxplot with the geom_jitter(). It also allows for specifying other arguments like colors and width of the points.\n\n  ggplot(data = lfq4,\n       aes(x = site, y = tl_mm, fill = site)) +\n  geom_boxplot(alpha = 0.6, position = \"identity\", \n               outlier.colour = \"red\", outlier.color = )+\n  geom_jitter(width = .1, height = .5, alpha = 0.1)+\n  labs(x = \"Sites\", y = \"Total length (mm)\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 28: Boxplot with points of total length by sites\n\n\n\n\n\n\n\n\nInstead of using a boxplot, we can use a violin plot. Each group is represented by a “violin”, given by a rotated and duplicated density plot:\n\n  ggplot(data = lfq4,\n       aes(x = site, y = tl_mm, fill = site)) +\n  geom_violin(alpha = 0.6, position = \"identity\")+\n  labs(x = \"Sites\", y = \"Total length (mm)\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 29: Violin of total length by sites\n\n\n\n\n\n\n\n\nBar graphs are perhaps the widely used plot. They are typically used to display count values on the y-axis for different groups on the x-axis. There is an important distinction you should be aware of when making bar graphs. The height of a bar in barplot may represent either the counts or percentage of elements in the dataset. Let’s begin with the former—count. We use the shrimps_cleaned.csv dataset, which contains weight and length of four shrimp species. To access the variable and values of this file we need to load the file using a read_csv function as the code in the chunk below highlight;\n\nshrimp = read_csv(\"../data/tidy/shrimps_cleaned.csv\")\n\nThe sample dataset of shrimp is shown in Table 2. It contain six variables year, season, tide, species, weight (total_wt_kg) and length (tl_mm).\n\n\n\n\nTable 2: Shrimp dataset\n\n\n\n\n\n\n\n\n\n\nyear\nseason\ntide\nspecies\ntotal_wt_kg\ntl_mm\n\n\n\n\n2008\nWET\nSTF\nMetapenaeus monoceros\n2.0\n21\n\n\n2008\nWET\nSTF\nMetapenaeus monoceros\n2.0\n20\n\n\n2008\nWET\nSTF\nMetapenaeus monoceros\n2.0\n19\n\n\n2012\nDRY\nSTN\nPenaeus monodon\n1.7\n12\n\n\n2012\nDRY\nSTN\nFenneropenaeus indicus\n1.7\n14\n\n\n2012\nDRY\nSTN\nPenaeus monodon\n1.7\n11\n\n\n\n\n\n\n\n\n\n\n\nWe realize that the scientific names are too long and may not fit into the plotting area. Therefore, we use a case_when function from dplyr package to change species name and assign it as a new variable called species.short\n\nshrimp = shrimp %&gt;% \n  mutate(species.short = case_when(\n    species == \"Metapenaeus monoceros\"~ \"M.monoceros\",\n    species == \"Penaeus monodon\"~ \"P.monodon\",\n    species == \"Fenneropenaeus indicus\"~ \"F.indicus\",\n    species == \"Penaeus semisulcatus\"~ \"P.semisulcatus\")\n    ) %&gt;% \n  relocate(species.short, .after = species)\n\n\n\nTo make the bar graph that show the number of shrimp per species over the sampling period you you simply specify the the variable species in the x coordinates in the aesthetic and add the geom_bar()\n\nggplot(data = shrimp, aes(x = species.short))+\n  geom_bar()+\n  labs(x = \"Species\", y= \"Frequency\")\n\n\n\n\n\n\n\nFigure 30: Frequency of shrimp species\n\n\n\n\n\nThen to stack the bar based on the sampling season, we add the argument fill = season in aes() part\n\nggplot(data = shrimp, aes(x = species.short, fill = season))+\n  geom_bar()+\n  labs(x = \"Species\", y =\"Frequency\")\n\n\n\n\n\n\n\nFigure 31: Frequency of shrimp species by season\n\n\n\n\n\nYou can flip the order of bar with position = position_stack(reverse = TRUE)\n\nggplot(data = shrimp, aes(x = species.short, fill = season))+\n  geom_bar(position = position_stack(reverse = TRUE))+\n  labs(x = \"Species\", y =\"Frequency\")\n\n\n\n\n\n\n\nFigure 32: Frequency of shrimp species by season\n\n\n\n\n\nInstead of stacking, you can dodge the bar with position = position_dodge() argument\n\nggplot(data = shrimp, aes(x = species.short, fill = season))+\n  geom_bar(position = position_dodge())+\n  labs(x = \"Species\", y =\"Frequency\")\n\n\n\n\n\n\n\nFigure 33: Frequency of shrimp species by season with span\n\n\n\n\n\nWe notice that the species that only appear one season, the count for that species is span across and make the bar wideer than those species occur in both seasons. We can fix that by parsing position = position_dodge(preserve = \"single\") in the geom_bar function\n\nggplot(data = shrimp, aes(x = species.short, fill = season))+\n  geom_bar(position = position_dodge(preserve = \"single\"))+\n  labs(x = \"Species\", y =\"Frequency\")\n\n\n\n\n\n\n\nFigure 34: Frequency of shrimp species by season without span\n\n\n\n\n\nTo add a black stroke color of the bar, add the argument col = \"black\" inside the geom_bar()\n\nggplot(data = shrimp, aes(x = species.short, fill = season))+\n  geom_bar(position = position_dodge(preserve = \"single\"), color = \"black\")+\n  labs(x = \"Species\", y =\"Frequency\")\n\n\n\n\n\n\n\nFigure 35: Frequency of shrimp species by season without span with black bar color\n\n\n\n\n\nAnd to specify the width of the bar you specify a value in width=.75 argument in geom_bar()\n\nggplot(data = shrimp, aes(x = species.short, fill = season))+\n  geom_bar(position = position_dodge(preserve = \"single\"), color = \"black\", width = .75)+\n  labs(x = \"Species\", y =\"Frequency\")\n\n\n\n\n\n\n\nFigure 36: Frequency of shrimp species by season without span with black bar color\n\n\n\n\n\n\n\n\nWe have seen how to make barplot that show the count with geom_bar(). You can also use the barplot to show the values with the geom_col() function and specify what variables you want on the x and y axis. For instance, we want to show percentage of shrimp species by season. Because the geom_col() requires summarized statistics, we need to compute the percentage for each season as the chunk below highlight.\n\nshrimp.pct = shrimp %&gt;% \n  group_by(species.short, season) %&gt;% \n  summarise(n = n()) %&gt;% \n  mutate(pct = n/sum(n), \n         pct = (pct * 100) %&gt;% round(2))\n\nshrimp.pct\n\n# A tibble: 6 × 4\n# Groups:   species.short [4]\n  species.short  season     n   pct\n  &lt;chr&gt;          &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;\n1 F.indicus      DRY      404 100  \n2 M.monoceros    DRY      539  86.9\n3 M.monoceros    WET       81  13.1\n4 P.monodon      DRY      738  52.4\n5 P.monodon      WET      671  47.6\n6 P.semisulcatus DRY      227 100  \n\n\nOnce we have computed the statistics, we can use them to make barplot. Note that unlike the geom_bar(), which need only the x variable, geom_col() requires x and y variables specified. For illustration, we specified the x = species, and y = pct in the aes() to make a barplot that show the percentage of shrimp by season (Figure 37).\n\nggplot(data = shrimp.pct, aes(x = species.short, y = pct, fill = season))+\n  geom_col(position = position_dodge(preserve = \"single\"), color = \"black\", width = .75)+\n  labs(x = \"Species\", y =\"Percentage\")\n\n\n\n\n\n\n\nFigure 37: Percentage of shrimp species by season without span with black bar color\n\n\n\n\n\n\n\n\n\nA pie chart is a disk divided into pie-shaped pieces proportional to the relative frequencies of the classes. To obtain angle for any class, we multiply the relative frequencies by 360 degree, which corresponds to the complete circle. Either variables or attributes can be portrayed in this manner, but a pie chart is especially useful for attributes. A pie diagram for contribution of different fish groups/species to the total fish landings at a landing site of a river is shown in Figure 38.\n\nshrimp %&gt;% \n  group_by(species) %&gt;% \n  summarise(n = n()) %&gt;% \n  mutate(pct = round(n/sum(n)*100), 2) %&gt;% \n  mutate(species = str_replace(string = species, pattern = \" \", replacement = \"\\n\")) %&gt;% \n  mutate(label = paste0(\"(\",pct,\"%\",\")\")) %&gt;% \n  ggpubr::ggpie(x = \"pct\", label = \"label\", fill = \"species\", lab.pos = \"in\", palette = \"jama\", color = \"ivory\", ggtheme = theme_void(), )\n\n\n\n\n\n\n\nFigure 38: Percentage composition of prawn species\n\n\n\n\n\nAn extended pie chart is donut shown in Figure 39\n\nshrimp %&gt;% \n  group_by(species) %&gt;% \n  summarise(n = n()) %&gt;% \n  mutate(pct = round(n/sum(n)*100), 2) %&gt;% \n  mutate(species = str_replace(string = species, pattern = \" \", replacement = \"\\n\")) %&gt;% \n  mutate(label = paste0(\"(\",pct,\"%\",\")\")) %&gt;% \n  ggpubr::ggdonutchart(x = \"pct\", label = \"label\", fill = \"species\", lab.pos = \"in\", palette = \"jama\", color = \"ivory\", ggtheme = theme_void())\n\n\n\n\n\n\n\nFigure 39: Percentage composition of prawn species\n\n\n\n\n\n\n\n\n\nshrimp %&gt;% \n  group_by(species.short) %&gt;% \n  count() %&gt;% \n  arrange(-n) %&gt;% \n  ggplot(aes(x = reorder(species.short,n), y = n, \n             fill = species.short), stat = \"identity\")+\n  geom_col() +\n  coord_polar(theta = \"y\")+\n  theme_bw() +\n  theme(axis.title = element_blank(), legend.position = \"right\", axis.text.y = element_blank(), axis.ticks = element_blank())+\n  scale_fill_brewer(palette = \"Set2\", name = \"Species\")\n\n\n\n\n\n\n\nFigure 40: Barplot with polar transformation\n\n\n\n\n\n\nshrimp %&gt;% \n  ggplot() +\n  geom_bar(aes(x = tide, fill = species.short),\n           color = \"ivory\") +\n  labs(x = \"Tide\", y = \"Count\") +\n  coord_polar()+\n  theme_bw() +\n  theme(axis.title = element_blank(), \n        legend.position = \"right\")+\n  scale_fill_brewer(palette = \"Set2\", name = \"Species\")\n\n\n\n\n\n\n\nFigure 41: Stacked barplot with polar transformation\n\n\n\n\n\n\n\n\nAlthough the ggridges package provides geom_ridgeline and geom_density_ridges, we focus on the latter because it has ability to estimates data densities and then draws those using ridgelines.The geom geom_density_ridges calculates density estimates from the provided data and then plots those, using the ridgeline visualization.\n\nlfq4 %&gt;% \n  mutate(months = lubridate::month(date, label = TRUE)) %&gt;%\n  ggplot()+\n  ggridges::geom_density_ridges(aes(x = tl_mm, y = months, fill = site), alpha = .7)+\n  scale_fill_brewer(palette = \"Set2\", name = \"Sampling\\nsite\")+\n  theme_minimal()+\n  theme(legend.position = c(.85,.2), legend.background = element_rect())+\n  labs(y = \"Months\", x = \"Total length (mm.)\")"
  },
  {
    "objectID": "posts/basicplots/index.html#introduction",
    "href": "posts/basicplots/index.html#introduction",
    "title": "Basic plots with ggplot2",
    "section": "",
    "text": "The ggplot2 package provides a set of functions that mirror the Grammar of Graphics (Wickham, 2016), enabling you to efficaciously specify what you want a plot to look like. To have a glimpse of ggplot2, we start with five basic types of plots that are familiar to most people. These include: scatterplot, linegraphs, boxplots, histograms, and barplots. The first four graphs works with quantitative data and barplots are appropriate for categorical data.\nThus, understanding the type of data is inevitable before you throw the variable into ggplot2 to make plot for you. In this post, we will cover the most common plot types, such as line plots, histograms, pie charts, scatter plots, and bar plots, along with several other plot types that build upon these.\n\n\nScatterplots are also called bivariate, allows you to visualize the association between two numerical variables. They are among the widely used plot in fisheries science particularly when looking for association between length and weight of a particular fish. Probably you might have come across a scatterplot like the one in Figure 1 that base R was used, but probably you have not made one based on the fundamental theorem of grammar of graphics.\n\n\n\n\n\n\n\n\nFigure 1: Length and weight relationship of Chinook Salmon sampled in Atantic Ocean\n\n\n\n\n\nWe are going to visualize the relationship between length and weight of fish measured in the coastal waters of Kenya. We use the tidy_LFQ_sample_4.csv file. Let’s import the dataset in the session using read_csv function.\n\nlfq4 = read_csv(\"../data/tidy/tidy_LFQ_sample_4.csv\")\n\nThis file contain length and weight measurements along with sex sampled in Mombasa and Voi from March 2016 to September 2020 (Table 1).\n\n\n\n\nTable 1: Sample length and weight of sampled fish\n\n\n\nsitedatetl_mmfl_mmwt_gmsexMombasa2019-04-0518416959.50MMombasa2019-04-0518516954.71MMombasa2019-04-0514513424.15MVoi2020-09-1118917465.88FVoi2020-09-1116214736.35FVoi2020-09-1116815346.13F\n\n\n\n\n\nLet’s now dive into the code of using the *grammar of graphics to create the scatterplot. We use the ggplot() function from ggplot2** package. The code highlighted in the chunk below was used to plot Figure 2\n\nggplot(data = lfq4, aes(x = tl_mm, y = wt_gm))+\n  geom_point()+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")\n\n\n\n\n\n\n\nFigure 2: Length and weight relationship\n\n\n\n\n\nLet’s explore the code above piece-by-piece\n\nThe plotting in ggplot2 begin with ggplot() function, where the two components of grammar of graphics are required. in the data component we specified the dataset by setting data = lfq4. Then the second argument aesthetic that map the plot with coordinate was set by aes(x = tl_mm, y = wt_gm)). In a nutshell, the aes() define the variable – axis specifications.\nWe then added a layer to the ggplot() function call using the + sign. The added layer specify the third part of the *grammar—the geometric component. Because we want to plot scatterplot, the appropriate geom for this case is the geom_point().\nadded a layer labs that allows us to label axis with meaningful axis titles\n\nadding regression line you can simply add the regression line by adding a geom_smooth() layer. However, Figure 2 is non-linear and hence we need to specify the modal that fits the data, the loess model is mostly used for non-linear data. Therefore, we parse the argumentmethod = \"loess\" to draw a non-linear regression line but also parse an argument se = FALSE to prevent plotting confidence error.\n\n  ggplot(data = lfq4, aes(x = tl_mm, y = wt_gm))+\n  geom_point()+\n  geom_smooth(method = \"loess\", se = FALSE)+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")\n\n\n\n\n\n\n\nFigure 3: Length and weight relationship with non-linear regression line\n\n\n\n\n\nIf we want to add a linear regression line i the scatter plot instead of the non linear shown in (ig-scatter2?), we simply replace method = \"loess\" with method = \"lm\"\n\n  ggplot(data = lfq4, aes(x = tl_mm, y = wt_gm))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")\n\n\n\n\n\n\n\nFigure 4: Length and weight relationship with linear regression line\n\n\n\n\n\nThe linear regression line we added in Figure 4 does not fit the data points. That’s is nature of the length and weight measurements of most fishes as their growth is allometric and not isometric. To make use of the linear model in such data points, we often log-transform the data points first and replot. But in ggplot framework, you do need to do that but simply add the scale_x_log10 and scale_y_log10 layer\n\n  ggplot(data = lfq4, aes(x = tl_mm, y = wt_gm))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")+\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\n\n\nFigure 5: Log-transformed length and weight relationship with linear regression line\n\n\n\n\n\nKnowing whether the relationship is positive or negative and whether is linear or non linear is one thing, but people would like to know the strength of the relationship that you have simply presented in Figure 5. Luckily, Pedro Aphalo developed a ggpmisc package (Aphalo, 2016), which extend the statistical function of ggplot2. By simply adding a layer ggpmisc::stat_correlation() in Figure 5, the function generates labels for correlation coefficients and p-value, coefficient of determination (R^2) for method “pearson” and number of observations and add them into the plot (Figure 6).\n\n  ggplot(data = lfq4, aes(x = tl_mm, y = wt_gm))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")+\n  scale_x_log10() +\n  scale_y_log10()+\n  ggpmisc::stat_correlation()\n\n\n\n\n\n\n\nFigure 6: Log-transformed length and weight relationship with linear regression line with correlation coefficient\n\n\n\n\n\nWe might be interested to distinguish the data points and the regression line based on the site. We can do that by adding the color argument in the aesthetic, which change from aes(x = tl_mm, y = wt_gm) to aes(x = tl_mm, y = wt_gm, color = site). The argument color = site will force the data points and the regression line to adhere to colors based on the site but the points and line are plotted on the same plot.\n\n  ggplot(data = lfq4, aes(x = tl_mm, y = wt_gm, color = site))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")+\n  scale_x_log10() +\n  scale_y_log10()+\n  ggpmisc::stat_correlation()\n\n\n\n\n\n\n\nFigure 7: Log-transformed length and weight relationship with linear regression line by site\n\n\n\n\n\nLooking on Figure 7, it is clear that sample from Mombasa station has relatively bigger and heavier fish than those sampled from Voi. But the problem with Figure 7 is that most of the Mombasa data points are masked by Voi data points, which are overlaid on Mombasa data points. We can overcome the issue of point cluttering by simply adding a transparency level in point with alpha = .2.\n\n  ggplot(data = lfq4, aes(x = tl_mm, y = wt_gm, color = site))+\n  geom_point(alpha = .2)+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")+\n  ggpmisc::stat_correlation()+\n  scale_x_log10() +\n  scale_y_log10()+\n  ggpmisc::stat_correlation()\n\n\n\n\n\n\n\nFigure 8: Log-transformed length and weight relationship with linear regression line by site. Points density is highlighted with transparency\n\n\n\n\n\nSometimes you may wish to plot Figure 8 as separate plot shown in Figure 9. That’s is achieved with facet_wrap function, which facet plots based on the levels that are in the variable that is specified. For instance, in our case, the variable chosen is site and there are two sites–Voi and Mombasa. Therefore by simply adding a facet_wrap(~site) layer will force ggplot to make two plots\n\n  ggplot(data = lfq4, aes(x = tl_mm, y = wt_gm))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")+\n  scale_x_log10() +\n  scale_y_log10()+\n  ggpmisc::stat_correlation()+\n  facet_wrap(~site, nrow = 1)\n\n\n\n\n\n\n\nFigure 9: Faceted Log-transformed length and weight relationship with linear regression line\n\n\n\n\n\n\n\n\nThe next basic graph of ggplot2 is the linegraph. Line graphs is similar to drawing points, except that it connects the points with line. often times you don’t show the points. Let’s illustrate how to create linegraphs using catch data in the region. We first load the dataset in the session\n\nlanding.countries = read_csv(\"../data/tidy/landings_wio_country.csv\", skip = 4)\n\nThe landing.countries dataset contain 660 records of landed fisheries catch information recorded between 1950 and 2015 from Somalia, Kenya, Mozambique, South Africa, Madagascar, Mauritius, Seychelles, Mayotte, Tanzania and Zanzibar.\n\nlanding.countries %&gt;% \n  FSA::headtail() |&gt; \n  flextable::flextable() |&gt; \n  flextable::autofit()\n\nnameyearcatchepochKenya1,95019,1541,960Kenya1,95121,3181,960Kenya1,95219,1261,960Madagascar2,013266,9532,010Madagascar2,014138,4782,010Madagascar2,015145,6292,010\n\n\nLinegraphs are used to show time series data. Its inappropriate to use the linegraphs for data that has no clear sequential ordering and should be continuous and not discrete data type. The internal structure of the catch dataset we just loaded indicate that with exception of country’s name, year, catch and epoch are numeric values.\n\nlanding.countries %&gt;% \n  glimpse() \n\nRows: 660\nColumns: 4\n$ name  &lt;chr&gt; \"Kenya\", \"Kenya\", \"Kenya\", \"Kenya\", \"Kenya\", \"Kenya\", \"Kenya\", \"…\n$ year  &lt;dbl&gt; 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960…\n$ catch &lt;dbl&gt; 19154, 21318, 19126, 20989, 17541, 19223, 23297, 28122, 28819, 2…\n$ epoch &lt;dbl&gt; 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960…\n\n\nLet’s us plot the annual landings of fish over the period with ggplot. Like the scatterplot we made earlier, where supply the data frame in data argument and specified the aesthetic mapping with x and y coordinates, but instead of using geom_point(), we use the geom_line(). The code to make the line graph of annual landing in the WIO region shown in Figure 10 is written as;\n\nggplot(data = landing.countries,\n       aes(x = year, y = catch)) +\n  geom_line()+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 10: Annual alnding in the WIO region\n\n\n\n\n\nAlthough we added a geom_line, but we notice that Figure 10 display a plot which we did not expect. The problem is that line for the ten countries are all lumped together and result in the chaotic situation. For illustration purpose, I will use the catch data from Mauritius. Let’s filter Mauritius’ catch information from the landing.countries dataset and display its rows and variables;\n\nmauritius.landings = landing.countries %&gt;% \n  filter(name == \"Mauritius\")\n\n\nmauritius.landings %&gt;% \n  FSA::headtail() |&gt; \n  flextable::flextable() |&gt; \n  flextable::autofit()\n\nnameyearcatchepochMauritius1,950183,0821,960Mauritius1,951216,1511,960Mauritius1,952181,8221,960Mauritius2,01315,7972,010Mauritius2,01413,8792,010Mauritius2,01516,3732,010\n\n\nThere are only 66 rows in Mauritius which are equivalent to 66 records each per year from 1950 to 2015. Let’s use the mauritius.landings dataset to plot\n\nggplot(data = mauritius.landings,\n       aes(x = year, y = catch)) +\n  geom_line()+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 11: Mauritius’ annual landing\n\n\n\n\n\nOften times you find that linegraphs has points. You can also do that in ggplot environment by adding a geom_point layer\n\nggplot(data = mauritius.landings,\n       aes(x = year, y = catch)) +\n  geom_line()+\n  geom_point()+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 12: Mauritius’ annual landing\n\n\n\n\n\nYou can also customize the appearance of the line and point by parsing the color argument in the geom_point and geom_line layers\n\nggplot(data = mauritius.landings,\n       aes(x = year, y = catch)) +\n  geom_line(color = \"black\")+\n  geom_point(color = \"red\")+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 13: Mauritius’ annual landing\n\n\n\n\n\nThe problem we faced in Figure 10 is that catch data for all ten countries were pooled together, and the plot was not informative. But what is we want to compare the trend of catch among the countries. That is achieved by simply distinguishing the color layer for each country. That is done by adding an argument color=name in aes function as the code below highlight\n\nggplot(data = landing.countries,\n       aes(x = year, y = catch, color = name)) +\n  geom_line()+\n  # geom_point(color = \"red\")+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 14: Annual landing by countries in the WIO region\n\n\n\n\n\nThe landings from South Africa is far higher than the rest of the WIO’s countries, which overshadow the appearance of other countries (Figure 14). There several approaches to resolve this issues where some countries have low catch values while others have relatively very high catches. For our case, we have decided to remove South Africa from the plot. We can do that by negating the selection with filter function from dplyr package. By parsing filter(!name == \"South Africa\"), note the exclamation mark before name tell to reverse selection and therefore select all countries except South Africa.\n\nother.countries = landing.countries %&gt;% \n  filter(!name == \"South Africa\")\n\nWe then plot and parse the argument data = other.countries instead of data = landing.countries to make Figure 15.\n\nggplot(data = other.countries,\n       aes(x = year, y = catch, color = name)) +\n  geom_line()+\n  # geom_point(color = \"red\")+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 15: Annual landing by countries in the WIO region with South Africa ommited\n\n\n\n\n\nWe notice that Tanzania and Zanzibar are presented as separate entity. Although the two states report to the FAO separate, but would be interested to know the landing of the combined Tanzania and Zanzibar catches. But before we combine these two states, lets see how their catches vary over the period. First we need to select only records for Tanzania and Zanzibar using a filter function as illustrated below;\n\ntanzania.zanzibar = landing.countries %&gt;% \n  filter(name %in% c(\"Tanzania\", \"Zanzibar\")) \n\nOnce we have created a tanzania.zanzibar object, we can use it to make plots that compare catch trend of Tanzania and Zanzibar over the last 66 years. The code in this chunk is used to make Figure 16\n\n  ggplot(data = tanzania.zanzibar,\n       aes(x = year, y = catch, color = name)) +\n  geom_line()+\n  # geom_point(color = \"red\")+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 16: Annual landing for mainland Tanzania and Zanzibar\n\n\n\n\n\n\nlanding.countries %&gt;% \n  mutate(name = str_replace(string = name, \n                            pattern = \"Zanzibar\", \n                            replacement = \"Tanzania\")) %&gt;% \n  filter(!name == \"South Africa\") %&gt;% \n  group_by(name, year) %&gt;% \n  summarise(catch_new = sum(catch, na.rm = TRUE)) %&gt;% \n  ggplot(\n       aes(x = year, y = catch_new, color = name)) +\n  geom_line()+\n  # geom_point(color = \"red\")+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 17: Annual landing for WIO where mainland Tanzania and Zanzibar are combined\n\n\n\n\n\n\n\n\nThe geom_area method is used to create an area plot. It can be used as a component in the ggplot method. The alpha parameter in the geom_area method is used to depict the opacity of a genome, the value ranges from zero to one integral values. In case, we choose a lower value, this means that a more transparent color version will be chosen to depict the plot and its smoothness. We have used the value for the alpha parameter to be one by two means it is somewhat translucent in nature.\n\n  ggplot(data = tanzania.zanzibar,\n       aes(x = year, y = catch, fill = name)) +\n  geom_area(alpha = 0.6, position=\"identity\")+\n  # geom_point(color = \"red\")+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 18: Area plot showing Annual landing for mainland Tanzania and Zanzibar\n\n\n\n\n\n\n\n\nA histogram is a plot that can be used to examine the shape and spread of continuous data. It looks very similar to a bar graph and organized in intervals or classes. It divides the range of the data into bin equal intervals (also called bins or classes), count the number of observations in each bin, and display the frequency distribution of observations as a bar plot. Such histogram plots provide valuable information on the characteristics of the data, such as the central tendency, the dispersion and the general shape of the distribution. With lfq4 dataset, we can plot the histogram of tl_mm. Since histogram works for single variable that contains quantitative values, you can not bother looking for relationship as we have seen in previous plots, but histogram offers an opportunity to answer question like\n\nWhat are the smallest and largest values of tl_mm?\nWhat is the center value? 3 How does these values spread out?\n\nWe can make a histogram shown in Figure 19 by simply setting aes(x = tl_mm) and add geom_histogram(). Within the geom_histogram(), we simply specify the number of bins bins = 30, fill color for the colum and also the color separating each columns of the histogram with col == \"red\" and fill = \"red\". However, a word of caution regarding histograms—bin size matters. The reproducible code to plot Figure 19 is written as;\n\n  ggplot(data = lfq4,\n       aes(x = tl_mm)) +\n  geom_histogram(bins = 30, fill = \"red\", color = \"red\", alpha = 0.4)+\n  labs(x = \"Total length (mm)\", y = \"Frequency\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 19: Histogram of total length\n\n\n\n\n\nThe resulting histogram gives us an idea of the range of total length of fish we can expect from the sample. You may be interested to compare histogram of the data values sampled from two or sites. For example, in our case, we are interested to compare the distribution of total length using samples collected from Mombasa and Voi sites. We simply add the fill = site argument in the aes function\n\n  ggplot(data = lfq4,\n       aes(x = tl_mm, fill = site)) +\n  geom_histogram(bins = 50, alpha = 0.6)+\n  labs(x = \"Total length (mm)\", y = \"Frequency\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 20: Histogram of total length by sites\n\n\n\n\n\nThe histogram of Mombasa and Voi is plotted as shown in Figure 20, however, despite the transparency level of the bins is set to 0.6 (alpha = .6), yet the bins from Mombasa are masked with bins from Voi. The voi bins are plotted over the Mombasa ones and prevent us to visualize the underneath Mombasa bins. To correct for this issue, we need to parse position = \"identity\"in the geom_bin, which create an different color where the Mombasa and Voi bins are intersected.\n\n  ggplot(data = lfq4,\n       aes(x = tl_mm, fill = site)) +\n  geom_histogram(bins = 50, alpha = 0.6, position = \"identity\")+\n  labs(x = \"Total length (mm)\", y = \"Frequency\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 21: Histogram of total length by sites\n\n\n\n\n\n\n\n\nIt is often useful to visualise the distribution of a numerical variable. Comparing the distributions of different groups can lead to important insights. Visualising distributions is also essential when checking assumptions used for various statistical tests (sometimes called initial data analysis). In this section we will illustrate how this can be done using the diamonds data from the ggplot2 package, which you started to explore in Chapter 2.\nAn advantage with frequency polygons is that they can be used to compare groups, e.g. diamonds with different cuts, without facetting:\n\n  ggplot(data = lfq4,\n       aes(x = tl_mm, color = site)) +\n  geom_freqpoly(alpha = 0.6, position = \"identity\")+\n  labs(x = \"Total length (mm)\", y = \"Frequency\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 22: Frequency polygon of total length by sites\n\n\n\n\n\nIt is clear from this figure that the total length of fish from Voi is larger in size than those from Mombasa. The polygons have roughly the same shape, except the shape of Mombasa a long right tail indicating the presence of outlier points.\n\n\n\nIn some cases, we are more interested in the shape of the distribution than in the actual counts in the different bins. Density plots are similar to frequency polygons but show an estimate of the density function of the underlying random variable. These estimates are smooth curves that are scaled so that the area below them is 1 (i.e. scaled to be proper density functions):\n\n#|\n\n  ggplot(data = lfq4,\n       aes(x = tl_mm, fill = site)) +\n  geom_density(alpha = 0.4, position = \"identity\")+\n  labs(x = \"Total length (mm)\", y = \"Frequency\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 23: Density plot of total length by sites\n\n\n\n\n\nFrom Figure 23, it’s clear that small size fish tend to have better total length, which wasn’t obvious from the frequency polygons. However, the plot does not provide any information about how common different total length are.\n\n\n\nThe boxplot is a standardized way of displaying the distribution of data based on the five number summary: minimum, first quantile, median, third quantile, and maximum. Boxplots are useful for detecting outliers and for comparing distributions. These five number summary also called the 25th percentile, median, and 75th percentile of the quantitative data. The whisker (vertical lines) capture roungly 99% of a distribution, and observation outside this range are plotted as points representing outliers as shown in Figure 24.\n\n\n\n\n\n\n\n\nFigure 24: Conceputal boxplot diagram\n\n\n\n\n\nBoxplots is one of statistical plot that present continuous variable and in ggplot a geom_boxplot() function is dedicated for that. The aes function always have at least two arguments. The first argument should be a categrial variable and the second one is numeric.\n\n  ggplot(data = lfq4,\n       aes(x = site, y = tl_mm)) +\n  geom_boxplot(alpha = 0.6, position = \"identity\")+\n  labs(x = \"Sites\", y = \"Total length (mm)\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 25: Boxplot of total length by sites\n\n\n\n\n\nthe geom_boxplot() has outlier_ arguments that allows to highlight and modify the color, shape, size, alpha … etc of outliers —extreme observation. For instance, you can highlight the outlier with;\n\n  ggplot(data = lfq4,\n       aes(x = site, y = tl_mm, fill = site)) +\n  geom_boxplot(alpha = 0.6, position = \"identity\", outlier.colour = \"red\", outlier.color = )+\n  labs(x = \"Sites\", y = \"Total length (mm)\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 26: Boxplot of total length by sites\n\n\n\n\n\nWe can also map the fill and color to variable in to distinguish boxplot. for example, we can specify the fill = site argument in the aes() to fill the boxplot based on site.\n\n  ggplot(data = lfq4,\n       aes(x = site, y = tl_mm, fill = site)) +\n  geom_boxplot(alpha = 0.6, position = \"identity\", outlier.colour = \"red\", outlier.color = )+\n  labs(x = \"Sites\", y = \"Total length (mm)\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 27: Boxplot of total length and color to distinguish sites\n\n\n\n\n\nWe can add the points on top of the boxplot with the geom_jitter(). It also allows for specifying other arguments like colors and width of the points.\n\n  ggplot(data = lfq4,\n       aes(x = site, y = tl_mm, fill = site)) +\n  geom_boxplot(alpha = 0.6, position = \"identity\", \n               outlier.colour = \"red\", outlier.color = )+\n  geom_jitter(width = .1, height = .5, alpha = 0.1)+\n  labs(x = \"Sites\", y = \"Total length (mm)\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 28: Boxplot with points of total length by sites\n\n\n\n\n\n\n\n\nInstead of using a boxplot, we can use a violin plot. Each group is represented by a “violin”, given by a rotated and duplicated density plot:\n\n  ggplot(data = lfq4,\n       aes(x = site, y = tl_mm, fill = site)) +\n  geom_violin(alpha = 0.6, position = \"identity\")+\n  labs(x = \"Sites\", y = \"Total length (mm)\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 29: Violin of total length by sites\n\n\n\n\n\n\n\n\nBar graphs are perhaps the widely used plot. They are typically used to display count values on the y-axis for different groups on the x-axis. There is an important distinction you should be aware of when making bar graphs. The height of a bar in barplot may represent either the counts or percentage of elements in the dataset. Let’s begin with the former—count. We use the shrimps_cleaned.csv dataset, which contains weight and length of four shrimp species. To access the variable and values of this file we need to load the file using a read_csv function as the code in the chunk below highlight;\n\nshrimp = read_csv(\"../data/tidy/shrimps_cleaned.csv\")\n\nThe sample dataset of shrimp is shown in Table 2. It contain six variables year, season, tide, species, weight (total_wt_kg) and length (tl_mm).\n\n\n\n\nTable 2: Shrimp dataset\n\n\n\n\n\n\n\n\n\n\nyear\nseason\ntide\nspecies\ntotal_wt_kg\ntl_mm\n\n\n\n\n2008\nWET\nSTF\nMetapenaeus monoceros\n2.0\n21\n\n\n2008\nWET\nSTF\nMetapenaeus monoceros\n2.0\n20\n\n\n2008\nWET\nSTF\nMetapenaeus monoceros\n2.0\n19\n\n\n2012\nDRY\nSTN\nPenaeus monodon\n1.7\n12\n\n\n2012\nDRY\nSTN\nFenneropenaeus indicus\n1.7\n14\n\n\n2012\nDRY\nSTN\nPenaeus monodon\n1.7\n11\n\n\n\n\n\n\n\n\n\n\n\nWe realize that the scientific names are too long and may not fit into the plotting area. Therefore, we use a case_when function from dplyr package to change species name and assign it as a new variable called species.short\n\nshrimp = shrimp %&gt;% \n  mutate(species.short = case_when(\n    species == \"Metapenaeus monoceros\"~ \"M.monoceros\",\n    species == \"Penaeus monodon\"~ \"P.monodon\",\n    species == \"Fenneropenaeus indicus\"~ \"F.indicus\",\n    species == \"Penaeus semisulcatus\"~ \"P.semisulcatus\")\n    ) %&gt;% \n  relocate(species.short, .after = species)\n\n\n\nTo make the bar graph that show the number of shrimp per species over the sampling period you you simply specify the the variable species in the x coordinates in the aesthetic and add the geom_bar()\n\nggplot(data = shrimp, aes(x = species.short))+\n  geom_bar()+\n  labs(x = \"Species\", y= \"Frequency\")\n\n\n\n\n\n\n\nFigure 30: Frequency of shrimp species\n\n\n\n\n\nThen to stack the bar based on the sampling season, we add the argument fill = season in aes() part\n\nggplot(data = shrimp, aes(x = species.short, fill = season))+\n  geom_bar()+\n  labs(x = \"Species\", y =\"Frequency\")\n\n\n\n\n\n\n\nFigure 31: Frequency of shrimp species by season\n\n\n\n\n\nYou can flip the order of bar with position = position_stack(reverse = TRUE)\n\nggplot(data = shrimp, aes(x = species.short, fill = season))+\n  geom_bar(position = position_stack(reverse = TRUE))+\n  labs(x = \"Species\", y =\"Frequency\")\n\n\n\n\n\n\n\nFigure 32: Frequency of shrimp species by season\n\n\n\n\n\nInstead of stacking, you can dodge the bar with position = position_dodge() argument\n\nggplot(data = shrimp, aes(x = species.short, fill = season))+\n  geom_bar(position = position_dodge())+\n  labs(x = \"Species\", y =\"Frequency\")\n\n\n\n\n\n\n\nFigure 33: Frequency of shrimp species by season with span\n\n\n\n\n\nWe notice that the species that only appear one season, the count for that species is span across and make the bar wideer than those species occur in both seasons. We can fix that by parsing position = position_dodge(preserve = \"single\") in the geom_bar function\n\nggplot(data = shrimp, aes(x = species.short, fill = season))+\n  geom_bar(position = position_dodge(preserve = \"single\"))+\n  labs(x = \"Species\", y =\"Frequency\")\n\n\n\n\n\n\n\nFigure 34: Frequency of shrimp species by season without span\n\n\n\n\n\nTo add a black stroke color of the bar, add the argument col = \"black\" inside the geom_bar()\n\nggplot(data = shrimp, aes(x = species.short, fill = season))+\n  geom_bar(position = position_dodge(preserve = \"single\"), color = \"black\")+\n  labs(x = \"Species\", y =\"Frequency\")\n\n\n\n\n\n\n\nFigure 35: Frequency of shrimp species by season without span with black bar color\n\n\n\n\n\nAnd to specify the width of the bar you specify a value in width=.75 argument in geom_bar()\n\nggplot(data = shrimp, aes(x = species.short, fill = season))+\n  geom_bar(position = position_dodge(preserve = \"single\"), color = \"black\", width = .75)+\n  labs(x = \"Species\", y =\"Frequency\")\n\n\n\n\n\n\n\nFigure 36: Frequency of shrimp species by season without span with black bar color\n\n\n\n\n\n\n\n\nWe have seen how to make barplot that show the count with geom_bar(). You can also use the barplot to show the values with the geom_col() function and specify what variables you want on the x and y axis. For instance, we want to show percentage of shrimp species by season. Because the geom_col() requires summarized statistics, we need to compute the percentage for each season as the chunk below highlight.\n\nshrimp.pct = shrimp %&gt;% \n  group_by(species.short, season) %&gt;% \n  summarise(n = n()) %&gt;% \n  mutate(pct = n/sum(n), \n         pct = (pct * 100) %&gt;% round(2))\n\nshrimp.pct\n\n# A tibble: 6 × 4\n# Groups:   species.short [4]\n  species.short  season     n   pct\n  &lt;chr&gt;          &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;\n1 F.indicus      DRY      404 100  \n2 M.monoceros    DRY      539  86.9\n3 M.monoceros    WET       81  13.1\n4 P.monodon      DRY      738  52.4\n5 P.monodon      WET      671  47.6\n6 P.semisulcatus DRY      227 100  \n\n\nOnce we have computed the statistics, we can use them to make barplot. Note that unlike the geom_bar(), which need only the x variable, geom_col() requires x and y variables specified. For illustration, we specified the x = species, and y = pct in the aes() to make a barplot that show the percentage of shrimp by season (Figure 37).\n\nggplot(data = shrimp.pct, aes(x = species.short, y = pct, fill = season))+\n  geom_col(position = position_dodge(preserve = \"single\"), color = \"black\", width = .75)+\n  labs(x = \"Species\", y =\"Percentage\")\n\n\n\n\n\n\n\nFigure 37: Percentage of shrimp species by season without span with black bar color\n\n\n\n\n\n\n\n\n\nA pie chart is a disk divided into pie-shaped pieces proportional to the relative frequencies of the classes. To obtain angle for any class, we multiply the relative frequencies by 360 degree, which corresponds to the complete circle. Either variables or attributes can be portrayed in this manner, but a pie chart is especially useful for attributes. A pie diagram for contribution of different fish groups/species to the total fish landings at a landing site of a river is shown in Figure 38.\n\nshrimp %&gt;% \n  group_by(species) %&gt;% \n  summarise(n = n()) %&gt;% \n  mutate(pct = round(n/sum(n)*100), 2) %&gt;% \n  mutate(species = str_replace(string = species, pattern = \" \", replacement = \"\\n\")) %&gt;% \n  mutate(label = paste0(\"(\",pct,\"%\",\")\")) %&gt;% \n  ggpubr::ggpie(x = \"pct\", label = \"label\", fill = \"species\", lab.pos = \"in\", palette = \"jama\", color = \"ivory\", ggtheme = theme_void(), )\n\n\n\n\n\n\n\nFigure 38: Percentage composition of prawn species\n\n\n\n\n\nAn extended pie chart is donut shown in Figure 39\n\nshrimp %&gt;% \n  group_by(species) %&gt;% \n  summarise(n = n()) %&gt;% \n  mutate(pct = round(n/sum(n)*100), 2) %&gt;% \n  mutate(species = str_replace(string = species, pattern = \" \", replacement = \"\\n\")) %&gt;% \n  mutate(label = paste0(\"(\",pct,\"%\",\")\")) %&gt;% \n  ggpubr::ggdonutchart(x = \"pct\", label = \"label\", fill = \"species\", lab.pos = \"in\", palette = \"jama\", color = \"ivory\", ggtheme = theme_void())\n\n\n\n\n\n\n\nFigure 39: Percentage composition of prawn species\n\n\n\n\n\n\n\n\n\nshrimp %&gt;% \n  group_by(species.short) %&gt;% \n  count() %&gt;% \n  arrange(-n) %&gt;% \n  ggplot(aes(x = reorder(species.short,n), y = n, \n             fill = species.short), stat = \"identity\")+\n  geom_col() +\n  coord_polar(theta = \"y\")+\n  theme_bw() +\n  theme(axis.title = element_blank(), legend.position = \"right\", axis.text.y = element_blank(), axis.ticks = element_blank())+\n  scale_fill_brewer(palette = \"Set2\", name = \"Species\")\n\n\n\n\n\n\n\nFigure 40: Barplot with polar transformation\n\n\n\n\n\n\nshrimp %&gt;% \n  ggplot() +\n  geom_bar(aes(x = tide, fill = species.short),\n           color = \"ivory\") +\n  labs(x = \"Tide\", y = \"Count\") +\n  coord_polar()+\n  theme_bw() +\n  theme(axis.title = element_blank(), \n        legend.position = \"right\")+\n  scale_fill_brewer(palette = \"Set2\", name = \"Species\")\n\n\n\n\n\n\n\nFigure 41: Stacked barplot with polar transformation\n\n\n\n\n\n\n\n\nAlthough the ggridges package provides geom_ridgeline and geom_density_ridges, we focus on the latter because it has ability to estimates data densities and then draws those using ridgelines.The geom geom_density_ridges calculates density estimates from the provided data and then plots those, using the ridgeline visualization.\n\nlfq4 %&gt;% \n  mutate(months = lubridate::month(date, label = TRUE)) %&gt;%\n  ggplot()+\n  ggridges::geom_density_ridges(aes(x = tl_mm, y = months, fill = site), alpha = .7)+\n  scale_fill_brewer(palette = \"Set2\", name = \"Sampling\\nsite\")+\n  theme_minimal()+\n  theme(legend.position = c(.85,.2), legend.background = element_rect())+\n  labs(y = \"Months\", x = \"Total length (mm.)\")"
  },
  {
    "objectID": "posts/basicplots/index.html#combining-multiple-plots",
    "href": "posts/basicplots/index.html#combining-multiple-plots",
    "title": "Basic plots with ggplot2",
    "section": "2 Combining multiple plots",
    "text": "2 Combining multiple plots\nWhen exploring data with many variables, you’ll often want to make the same kind of plot (e.g. a violin plot) for several variables. It will frequently make sense to place these side-by-side in the same plot window. The patchwork package extends ggplot2 by letting you do just that. Let’s install it:\ninstall.packages(\"patchwork\")\nThen load a package in the workspace\n\nrequire(patchwork)\n\nTo use patchwork (Pedersen, 2020), save each plot as a plot object :\n\nplot.tl = ggplot(data = lfq4,\n       aes(x = tl_mm, fill = site)) +\n  geom_density(alpha = 0.4, position = \"identity\")+\n  labs(x = \"Total length (mm)\", y = \"Frequency\")+\n  theme_minimal()\n\n plot.wt = ggplot(data = lfq4,\n       aes(x = wt_gm, fill = site)) +\n  geom_density(alpha = 0.4, position = \"identity\")+\n  labs(x = \"Weight (gram)\", y = \"Frequency\")+\n  theme_minimal()+\n   theme(legend.position = \"none\")\n\nthen add them together\n\nplot.tl + plot.wt\n\n\n\n\n\n\n\nFigure 42: Density plot by sites for total length (left panel) and weight (right panel)\n\n\n\n\n\n`\n\nplot.tl / plot.wt\n\n\n\n\n\n\n\nFigure 43: Density plot by sites for total length (top panel) and weight (bottom panel)\n\n\n\n\n\nWe need first to extract monsoon seasons from the dataset. We know from literature that the coasal waters of East Africa is affected by monsoon season, which is influenced trade winds, which is broadly categorized as;\n\nNortheast monsoon season — November through March\nSoutheast monsoon season — May to september\nIntermonsoon season — April and October\n\nWe can use the month information to break our dataset into three monsoon seasons as;\n\nlfq4.season = lfq4 %&gt;% \n  mutate(month = lubridate::month(date),\n         season = case_when(month &gt; 10 | month &lt; 4 ~ \"NE\",\n                            month &gt;=5 & month &lt; 10 ~ \"SE\",\n                            month == 4 | month == 10 ~ \"INT\")) %&gt;% select(-month)\n\n\nplot.int =lfq4.season %&gt;% \n  filter(site == \"Voi\" & season == \"INT\") %&gt;% \n  ggplot(aes(x = tl_mm, y = wt_gm))+\n  geom_point(alpha = .2)+\n  theme_bw()+\n  scale_x_continuous(name = \"Total length (mm)\")+\n  scale_y_continuous(name = \"Weight (gram)\")+\n  annotate(geom = \"label\",x = 120, y = 80, label = \"Northeast\\nSeason\")\n\nplot.ne = lfq4.season %&gt;% \n  filter(site == \"Voi\" & season == \"NE\") %&gt;% \n  ggplot(aes(x = tl_mm, y = wt_gm))+\n  geom_point(alpha = .2)+\n  theme_bw()+\n  scale_x_continuous(name = \"Total length (mm)\")+\n  scale_y_continuous(name = \"Weight (gram)\")+\n  annotate(geom = \"label\",x = 140, y = 80, label = \"Southeast\\nSeason\")\n\nplot.se = lfq4.season %&gt;% \n  filter(site == \"Voi\" & season == \"SE\") %&gt;% \n  ggplot(aes(x = tl_mm, y = wt_gm))+\n  geom_point(alpha = .2)+\n  theme_bw()+\n  scale_x_continuous(name = \"Total length (mm)\")+\n  scale_y_continuous(name = \"Weight (gram)\")+\n  annotate(geom = \"label\",x = 120, y = 80, label = \"Inter\\n Monsoon\")\n\nplot.all = lfq4.season %&gt;% \n  filter(site == \"Voi\") %&gt;% \n  ggplot(aes(x = tl_mm, y = wt_gm, color = season))+\n  geom_point(alpha = .2)+\n  theme_bw()+\n  scale_x_continuous(name = \"Total length (mm)\")+\n  scale_y_continuous(name = \"Weight (gram)\")+\n  theme(legend.position = c(.2,.8))\n\nYuo may plot One row with three plots and one row with a single plot\n\n(plot.ne + plot.se + plot.int)/\n  plot.all\n\n\n\n\n\n\n\n\nOr one column with three plots and one column with a single plot `\n\nplot.all | (plot.ne / plot.se / plot.int)"
  },
  {
    "objectID": "posts/basicplots/index.html#labelling-outliers",
    "href": "posts/basicplots/index.html#labelling-outliers",
    "title": "Basic plots with ggplot2",
    "section": "3 Labelling outliers",
    "text": "3 Labelling outliers\nInteractive plots are great when exploring a dataset but are not always possible to use in other contexts, e.g. for printed reports and some presentations. In these other cases, we can instead annotate the plot with notes about outliers. One way to do this is to use a geom called geom_text.\n\nlfq4.season %&gt;% \n  filter(!site == \"Voi\") %&gt;% \n  ggplot(aes(x = season, y = wt_gm, fill = season))+\n  geom_boxplot(alpha = .4, width = .29)+\n  ggrepel::geom_text_repel(aes(label = if_else( wt_gm &gt; 175, site, \"\"))) +\n  theme_bw()+\n  scale_x_discrete(name = \"Monsoon season\")+\n  scale_y_continuous(name = \"Weight (gram)\")+\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/basicplots/index.html#add-on-packages",
    "href": "posts/basicplots/index.html#add-on-packages",
    "title": "Basic plots with ggplot2",
    "section": "4 Add-on packages",
    "text": "4 Add-on packages\nThe R community has developed packages that extend the capability of ggplot2. Some of the packages include:\n\nmetR: Provide addition tools for plotting filled contour, and label contour lines\nggrepel: Contains tools for automatically position non-overlapping text labels\nggspatial: Spatial Data Framework for ggplot2\nRcolorBrewer: Contains color palettes for continuous and discrete plots\ncowplot: Contains addition themes and tools to combine ggplot2 plots in one panel\negg: Provide tools for plot aligning and symmetrised ggplot2 plots\noce: Provide color pallete for visualization of Oceanographic Data\nggsn: Provide tools for mapping North symbols and scale bars on maps created with ggplot2\ngganimate: convert static ggplot2 plots to animations\nggformula: adds some additional plot options to ggplot2\nsf : Add capabilities of ggplot2 to map spatial data such as simple features\nggthemes: contains extra themes, scales, and geoms, and functions for and related to ggplot2\nggridges: extend the geom_density function by plotiing closed polygons insted of ridgelines\nggpmisc"
  },
  {
    "objectID": "posts/timeline/index.html",
    "href": "posts/timeline/index.html",
    "title": "Creating a Timeline graphic using R and ggplot2",
    "section": "",
    "text": "In this post we’re going to be using R and ggplot2 to create a project timeline with milestones and milestone statuses.\nThe finished product will look like as illustrated in Figure 1\n\n\n\n\n\n\nFigure 1: A timeline of the Milestone for learning Modern\n\n\n\n\n\n\nCitationBibTeX citation:@online{semba2023,\n  author = {Semba, Masumbuko},\n  title = {Creating a {Timeline} Graphic Using {R} and Ggplot2},\n  date = {2023-11-24},\n  url = {https://lugoga.github.io/kitaa/posts/timeline/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSemba, M., 2023. Creating a Timeline graphic using R and ggplot2 [WWW\nDocument]. URL https://lugoga.github.io/kitaa/posts/timeline/"
  },
  {
    "objectID": "posts/dataviz/index.html",
    "href": "posts/dataviz/index.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Data visualization is the graphical representation of information and data. By using visual elements like charts, graphs, and maps, data visualization tools provide an accessible way to see and understand trends, outliers, and patterns in data. Additionally, it provides an excellent way for employees or business owners to present data to non-technical audiences without confusion.\nIn the world of Big Data, data visualization tools and technologies are essential to analyze massive amounts of information and make data-driven decisions."
  },
  {
    "objectID": "posts/dataviz/index.html#what-is-data-visualization",
    "href": "posts/dataviz/index.html#what-is-data-visualization",
    "title": "Data Visualization",
    "section": "",
    "text": "Data visualization is the graphical representation of information and data. By using visual elements like charts, graphs, and maps, data visualization tools provide an accessible way to see and understand trends, outliers, and patterns in data. Additionally, it provides an excellent way for employees or business owners to present data to non-technical audiences without confusion.\nIn the world of Big Data, data visualization tools and technologies are essential to analyze massive amounts of information and make data-driven decisions."
  },
  {
    "objectID": "posts/dataviz/index.html#overview",
    "href": "posts/dataviz/index.html#overview",
    "title": "Data Visualization",
    "section": "Overview",
    "text": "Overview\nAfter completing this section, we will:\n\nunderstand the importance of data visualization for communicating data-driven findings.\nbe able to use distributions to summarize data.\nbe able to use the average and the standard deviation to understand the normal distribution\nbe able to access how well a normal distribution fit the data using a quantile-quantile plot.\nbe able to interpret data from a box plot"
  },
  {
    "objectID": "posts/dataviz/index.html#introduction-to-data-visualization",
    "href": "posts/dataviz/index.html#introduction-to-data-visualization",
    "title": "Data Visualization",
    "section": "Introduction to Data Visualization",
    "text": "Introduction to Data Visualization\n\nKey Point:\n\nPlots of data easily communicate information that is difficult to extract from table of raw values.\nData visualization is a key component of exploratory data analysis (EDA), in which the properties of data are explored through visualization and summarization techniques.\nData visualization can help discover biases, systematic errors, mistakes and other unexpected problems in data before those data are incorporated into potentially flawed analysis.\nBasics of data visualization and EDA will be covered in R by using the ggplot2 package and motivating examples from world health, economics and infections disease.\n\n\n\nCode:\n\nlibrary(dslabs)\n\nWarning: package 'dslabs' was built under R version 4.3.3\n\ndata(murders)\nhead(murders)\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65"
  },
  {
    "objectID": "posts/dataviz/index.html#introduction-to-distributions",
    "href": "posts/dataviz/index.html#introduction-to-distributions",
    "title": "Data Visualization",
    "section": "Introduction to Distributions",
    "text": "Introduction to Distributions\n\nKey Points:\n(Variance/Deviation Var)方差: 方差越大，数据的波动越大；方差越小，数据的波动就越小。\n(Standard Deviation)标准差: 方差开根号。\n\nThe most basic statistical summary of a list of object is its distribution.\nWe will learn ways to visualize and analyze distributions in the upcoming videos.\nIn some cases, data can be summarized by two-number summary: the average and standard deviation.I will learn to use data visualization to determine when that is appropriate.\n\n\n\nData Types\nIn R, there are 6 basic data types:\n\nlogical\nnumeric\ninteger\ncomplex\ncharacter\nraw\n\n\n\n\n\n\n\nImportant\n\n\n\nCategorical data are variables that are defined by a small number of groups.\n\nOrdinal categorical data have an inherent order to the categories (mild/medium/hot, for example).\nNon-ordinal categorical data have no order to the categories.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNumerical data take a variety of numeric values.\n\nContinuous variables can take any value.\nDiscrete variables are limited to sets of specific values.\n\n\n\n\n\n\n\n\nflowchart LR\n  A[Main variable types] --&gt; B{Catrgorical}\n  A[Main variable types] --&gt; C{Numeric}\n  B{Catrgorical} --&gt; D[ordinal]\n  B{Catrgorical} --&gt; E[non-ordinal]\n  C{Numeric} --&gt; F[continuous]\n  C{Numeric} --&gt; G[discrete]\n\n\n\n\n\n\n\n\nExercise\n\n# extract the variable names from a dataset\nnames(x)\n# explore how many unique values are used in dataset\nunique(x)\n# determine how many variable were reported\nlength(x)\n# determine how many unique variable were reported\nlength(unique(x))\n# to compute the frequencies of each unique value\ntable(x)"
  },
  {
    "objectID": "posts/dataviz/index.html#describe-heights-to-et",
    "href": "posts/dataviz/index.html#describe-heights-to-et",
    "title": "Data Visualization",
    "section": "Describe Heights to ET",
    "text": "Describe Heights to ET\n\nkey point:\n\nA distribution is a function or description that shows the possible values of a variable and how often those values occur.\nFor categorical variables, the distribution describes the proportions of each category.\nA frequency table is the simplest way to show a categorical distribution. Use prop.table() to convert a table of counts to a frequency table. Barplots display the distribution of categorical variables and are a way to visualize the information in frequency tables.\nFor continuous numerical data, reporting the frequency of each unique entry is not an effective summary as many or most values are unique. Instead, a distribution function is required.\nThe cumulative distribution function (CDF) is a function that reports the proportion of data below a value \\(a\\) for all values of \\(a\\) :\\(F(a)=Pr(x≤a)\\).\nThe proportion of observations between any two values \\(a\\) and \\(b\\) can be computed from the CDF as \\(F(b)-F(a)\\).\nA histogram divides data into non-overlapping bins of the same size and plots the counts of number of values that fall in that interval.\n\n\n\nCode:\nR 语言学习 - table() 结果提取.\n\n# load the dataset\nlibrary(dslabs)\ndata(heights)\n# make a table of category proportions\nprop.table(table(heights$sex))"
  },
  {
    "objectID": "posts/dataviz/index.html#cumulative-distribution-function",
    "href": "posts/dataviz/index.html#cumulative-distribution-function",
    "title": "Data Visualization",
    "section": "Cumulative Distribution Function",
    "text": "Cumulative Distribution Function\nEvery continuous distribution has cumulative distribution function (CDF). The CDF defines the proportion of the data below a given value for all values of \\(a\\) :\n\n\n\nCumulative Distribution Function (CDF)\n\n\nAs defined above, this plot of the CDF for male heights has height value a on the x-axis and the proportion of student with heights of that value or lower(F(a)) on the y-axis.\nThe CDF is essential for calculating probabilities related to continuous data. In a continuous dataset, the probability of a specific exact value is not informative because most entries are unique. For example, in the student heights data, only one individual reported a height of 68.8976377952726 inches, but many students rounded similar heights to 69 inches. If we computed exact value probabilities, we would find that being exactly 69 inches is much more likely than being a non-integer exact height, which does not match our understanding that height is continuous. We can instead use the CDF to obtain a useful summary, such as the probability that a student is between 68.5 and 69.5 inches.\nFor datasets that are not normal, the CDF can be calculated manually by defining a function to compute the probability above. This function can then be applied to a range of values across the range of the dataset to calculate a CDF. Given a datasetmy_data, the CDF can be calculated and plotted like this:\nR语言中的[apply()]，[lapply()]，[sapply()]，tapply()函数以及示例\n\nCode for CDF:\n\n# Cumulative Distribution Function \na &lt;- seq(min(x), max(x), length) # define range of the values\ncdf_function &lt;- function(x) {\n    mean(my_data &lt;= x)\n}\ncdf_values &lt;- sapply(a, cdf_function)\nplot(a, cdf_values)\n\n\n\nCode for student height:\n\n# example for student heights\na &lt;- seq(min(heights$height), max(heights$height), length = 100)\ncdf_function &lt;- function(x){\n  mean(heights$height &lt;= x)\n}\ncdf_value &lt;- sapply(a, cdf_function)\nplot(a, cdf_value)\n\n\n\n\n\n\n\n\nThe CDF defines that proportion of data below a cut-off \\(a\\). To define the proportion of values above \\(a\\), we compute: \\(1-F(a)\\)\nTo define the proportion of values between \\(a\\) and \\(b\\), we compute: \\(F(b)-F(a)\\)\nNote that the CDF can help compute probabilities. The probability of observing a randomly chosen value between \\(a\\) and \\(b\\) is equal to the proportion of values between \\(a\\) and \\(b\\), which we compute with the CDF."
  },
  {
    "objectID": "posts/dataviz/index.html#smooth-density-plots",
    "href": "posts/dataviz/index.html#smooth-density-plots",
    "title": "Data Visualization",
    "section": "Smooth Density Plots",
    "text": "Smooth Density Plots\n\nKey Point:\n\n\n\n\n\n\nA further note on histograms\n\n\n\nThe choice of binwidth has a determinative effect on sharp. There is no “correct” choice for binwidth, and you can sometimes gain insights into the data by experimenting with binwidths.\n\n\n\nSmooth density plots can be thought of as histograms where the binwidth is extremely or infinitely small. The smoothing function makes estimates of the true continuous trend of the data given the available sample of data points.\nThe degree of smoothness can be controlled by an argument in the plotting function.\nWhile the histogram is an assumption-free summary, the smooth density plot is shaped by assumptions and choices you make as a data analyst.\nThe y-axis is scaled so that the area under the density curve sums to 1. This means that interpreting value on the y-axis is not straightforward. To determine the proportion of data in between two values, compute the area under the smooth density curve in the region between those values.\nAn advantage of smooth densities over histograms is that densities are easier to compare visually."
  },
  {
    "objectID": "posts/dataviz/index.html#normal-distribution",
    "href": "posts/dataviz/index.html#normal-distribution",
    "title": "Data Visualization",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\nKey Points:\n\nThe normal distribution:\n\nis centered around one value, the mean\nis symmetric(对称) around the mean.\nis defined completely by its mean(\\(\\mu\\)) and standard deviation(\\(\\sigma\\))\nAlways has the same proportion of observations within a given distance of the mean (for example, 95% with 2\\(\\sigma\\))\n\nThe standard deviation is the average distance between a value and the mean value.\nCalculate the mean using the mean() function.\nCalculate the standard deviation using the sd() function or manually.\nStandard units describe how many standard deviations a value is away from the mean. The z-score, or number of standard deviation an observation is away from the mean \\(\\mu\\):\n\\[\n  z = (x-\\mu)/\\sigma\n  \\]\nComputer standard units with the scale() function.\nImportant: to calculate the proportion of value that meet a certain condition, use the mean function on a logical vector. Because TRUE is converted to 1 and FALSE is converted to 0, taking the mean of this vector yields the proportion of TURE."
  },
  {
    "objectID": "posts/dataviz/index.html#equation-for-the-normal-distribution",
    "href": "posts/dataviz/index.html#equation-for-the-normal-distribution",
    "title": "Data Visualization",
    "section": "Equation for the normal distribution",
    "text": "Equation for the normal distribution\nThe normal distribution is mathematically defined by the following formula for any mean \\(\\mu\\) and standard deviation \\(\\sigma\\):\n\\[\nPr(a &lt; x &lt; b) = \\int_{a}^b\\frac{1}{\\sqrt{2\\pi\\mu}}{e}^{-\\frac{1}{2}(\\frac{x-\\mu^2}{\\sigma})}dx\n\\]\nWhen standard unites \\(z=0\\), the normal distribution is at a maximum, the mean \\(\\mu\\). The function is defined to be symmetric around \\(z=0\\).\nThe normal distribution of z-score is called the standard normal distribution and is defined by \\(\\mu=0\\) and \\(\\sigma=1\\).\nZ-score are useful to quickly evalute whether an observation is average or extreme. Z-scores near 0 are average. Z-score above 2 or below -2 are significantly above or blew the mean, and z-scores above 3 or below -3 are extrmely rate.\n\nCode:\n\n# define x as vector of male heights\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(heights)\nindex &lt;- heights$sex==\"Male\"\nx &lt;- heights$height[index]\n\n# calculate the mean and standard deviation manually\naverage &lt;- sum(x)/length(x)\nSD &lt;- sqrt(sum((x-average)^2)/length(x))\n\n# built-in mean and sd functions - note that the audio and printed value disagree\naverage &lt;- mean(x)\nSD &lt;- sd(x)\nc(average = average, SD = SD)\n\n# calculate standard units\nz &lt;- scale(x)\n\n# calculate proportion of value within 2 SD of mean\nmean(abs(z) &lt; 2)\n\nfunction sd():The built-in R function sd() calculates the standard deviation, but it divides by length(x)-1 instead of length(x). When the length of the list is large, this difference is negligible and you can use the built-in sd() function. Otherwise, you should compute σ by hand. For this course series, assume that you should use the sd() function unless you are told not to do so.\nHere we will learn more about benchmark z-score value and their corresponding probabilities.\n\n\nThe 68-95-99.7 Rule\nThe normal distribution is associated with the 68-95-99.7 rule. This rule describes the probability of observing events within a ceration number of standard deviations of the mean.\n\n\n\nNormal Distribution Probabilities\n\n\nThe probability distribution function for the normal distribution is defined such that:\n\nAbout 68% of observations will be within one standard deviation of the mean(\\(\\mu\\pm\\sigma\\)). In standard units, this is equivalent to a z-score of \\(|z|\\leq2\\)\n\n\n\n\nProbability of an observation within 1 SD of mean\n\n\n\nAbout 95% of observations will be within two standard seviations of the mean(\\(\\mu\\pm2\\sigma\\)). In standard units, this is equivalent to a z-sore of \\(|z|\\leq2\\).\n\n\n\n\nProbability of an ovservation within 2 SD of mean\n\n\n\nAbout 99.7% of observations will be within three standard deviations of the mean(\\(\\mu\\pm3\\sigma\\)). In standard units, this is equivalent to a z-score of \\(|z|\\leq3\\).\n\n\n\n\nProbability of an observation within 3 SD of mean"
  },
  {
    "objectID": "posts/dataviz/index.html#the-normal-cdf-and-pnorm",
    "href": "posts/dataviz/index.html#the-normal-cdf-and-pnorm",
    "title": "Data Visualization",
    "section": "The Normal CDF and pnorm",
    "text": "The Normal CDF and pnorm\n\nKey points:\n\nThe normal distribution has a mathematically defined CDF which can be computed in R with the function pnorm.\npnom(a, avg, s) gives the value of the cumculative distribution function F(a) for the normal distribution defined by average avg and standard deviation s.\nwe say that a random quantity is normally distributed with average avg and standard deviation s if the approximate pnorm(a, avg, s) holds for all values of a.\nIf we are willing to use the normal approximation for height, we can estimate the distribution simply from the mean and standard deviation of our values.\nIf we treat the height data as discrete rather than categorical, we see that the data are not very useful because integer values are more common that expected due to rounding. This is called discretization.\nWith rounded data, the normal approximation is particularly useful when computing probabilities of intervals of length 1 that include exactly over integer.\n\n\n\nCode: Using pnorm to calculate probabilities\nGiven male heights x:\n\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(\"heights\")\nx &lt;- heights %&gt;% filter(sex==\"Male\") %&gt;% pull(height)\n\nwe can estimate the probability that a male is taller than 70.5 inches with:\n\n1 - pnorm(70.5, mean(x), sd(x))\n\n\n\nCode: Discretization and the normal approximation\n\n# plot distribution of exact heights in data\nplot(prop.table(table(x)), xlab = \"a = Height in inches\", ylab = \"Pr(x = a)\")\n\n\n\n\n\n\n\n\n\n# probabilities in actual data over length 1 ranges containing a integer\nmean(x &lt;= 68.5) - mean(x &lt;= 67.5)\nmean(x &lt;= 69.5) - mean(x &lt;= 68.5)\nmean(x &lt;= 70.5) - mean(x &lt;= 69.5)\n\n# probabilities in normal approximation match well\npnorm(68.5, mean(x), sd(x)) - pnorm(67.5, mean(x), sd(x))\npnorm(69.5, mean(x), sd(x)) - pnorm(68.5, mean(x), sd(x))\npnorm(70.5, mean(x), sd(x)) - pnorm(69.5, mean(x), sd(x))\n\n# probabilities in actual data over other ranges don't match normal approx as well\nmean(x &lt;= 70.9) - mean(x &lt;= 70.1)\npnorm(70.9, mean(x), sd(x)) - pnorm(70.1, mean(x), sd(x))"
  },
  {
    "objectID": "posts/dataviz/index.html#definition-of-quantiles",
    "href": "posts/dataviz/index.html#definition-of-quantiles",
    "title": "Data Visualization",
    "section": "Definition of quantiles",
    "text": "Definition of quantiles\n\nDefinition of quantiles\nQuantiles are cut off points that divide a dataset into intervals with set probability. The qth quantile is the value at which q% of the observation are equal to or less than that value.\n\n\nUsing the quantile function\nGiven a dataset data and desired quantile q, you can find the q the quantile of data with:\n\nquantile(data,q)\n\n\n\nPercentiles\nPercentiles are the quantiles that divide a dataset into 100 intervals each with 1% probability. You can determine all percentiles of a dataset data like this:\n\np &lt;- seq(0.01, 0.09, 0.01)\nquantile(data, p)\n\n\n\nQuartiles\nQuartiles divide a dataset into 4 parts each with 25% probability. They are equal to the 25th, 50th and 75th percentiles. The 25th percentile is also known as the 1st quartile, the 50th percentile is also konwn as the median, and the 75th percentile is also knowns as the 3rd quartile.\nThe summary() function returns the minimum, quartiles and maximum of a vector.\n\n\nExamples\nLoad the heights dataset from the dslabs package:\n\nlibrary(dslabs)\ndata(\"heights\")\n\nUsesummaryon the heights$height variable to find the quartiles:\n\nsummary(heights$height)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  50.00   66.00   68.50   68.32   71.00   82.68 \n\n\nFind the percentiles of height$height:\n\np &lt;- seq(0.01, 0.99, 0.01)\npercentiles &lt;- quantile(heights$height, p)\n\nConfirm that the 25th and 75th percentiles match that 1st and 3rd quartiles. Note that quantile() returns a named vector. You can access the 25th and 75th percentiles like this (adapt the code for other percentile value):\n\npercentiles[names(percentiles) == \"25%\"]\n\n25% \n 66 \n\npercentiles[names(percentiles) == \"75%\"]\n\n75% \n 71"
  },
  {
    "objectID": "posts/dataviz/index.html#finding-quantile-with-qnorm",
    "href": "posts/dataviz/index.html#finding-quantile-with-qnorm",
    "title": "Data Visualization",
    "section": "Finding quantile with qnorm",
    "text": "Finding quantile with qnorm\n\nDefiniton of qnorm\n简单来说,qnorm是正态分布累积分布函数(CDF)的反函数， 也就是说它可以视为pnorm的反函数, 这里q指的是quantile, 即分位数\nThe qnorm() function gives the theoretical value of a quantile with probability p of observing a value equal to or less than that quantile value a normal distribution with mean mu and standard deviation sigma:\n\nqnorm(p, mu, sigma)\n\nBy default, mu=0 and sigma=1. Therefore, calling qnorm() with no arguments gives quantiles for the standard normal distribution.\n\nqnorm(p)\n\nRecall that quantiles are defined such that \\(p\\) is the probability of a random observation less than or equal to the quantile.\n\n\nRealation to pnorm\nThe pnorm() function gives the probability that a value from a standard normal distribution will be less than or equal to a z-score value z. consider: \\[pnorm(-1.96)\\approx0.025\\] The result of pnorm() is the quantile. Note that: \\[qnorm(0.025)\\approx-1.96\\] qnorm() and pnorm are inverse functions: \\[pnorm(qnorm(0.025))\\equiv0.025\\]\n\n\nTheoretical quantiles\nYou can use qnorm() to determine the theoretical quantiles of a dataset: that is, the theoretical value of quantiles assuming that a dataset follows a normal distribution. Run the qnorm() function with the desired probabilities p, mean mu and standard deviation sigma.\nSuppose male heights follow a normal distribution with a mean of 69 inches and standard deviation of 3 inches. The theoretical quantiles are:\n\np &lt;- seq(0.01, 0.99, 0.01)\ntheoretical_quantiles &lt;- qnorm(p, 69, 3)\n\nTheoretical quantiles can be compared to sample quantiles determined with the quantile function in order to evaluate whether the sample follows a normal distribution."
  },
  {
    "objectID": "posts/dataviz/index.html#quantile-quantile-plots",
    "href": "posts/dataviz/index.html#quantile-quantile-plots",
    "title": "Data Visualization",
    "section": "Quantile-Quantile Plots",
    "text": "Quantile-Quantile Plots\n\nKey Points:\n\nQuantile-quantile plots, or QQ-plot, are used to check whether distributions are well-approximated by a normal distribution.\nGiven a proportion p, the quantile q is the value such that the proportion of values in the data blew q is p.\nIn a QQ-plot, the sample quantiles in the observed data are compared to the theoretical quantiles expected from the normal distribution. If the data are well-approximated by the normal distribution, then the points on the QQ-plot will fall near the identity line(sample = theoretical).\nCalculate sample quantiles (observed quantiles) using the quantile() function.\nCalculate theoretical quantiles with the qnorm() function. qnorm() will caculate quantiles for the standard normal distribution (\\(\\mu=0, \\sigma=1\\)) by default, but it can calculate quantiles for any normal distribution given mean() and sd() arguments.\n\n\n\nCode:\n\n# define x and z\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(heights)\n\nindex &lt;- heights$sex==\"Male\"\nx &lt;- heights$height[index]\nz &lt;- scale(x)\n\n# proportion of data below 69.5\nmean(x &lt;= 69.5)\n\n[1] 0.5147783\n\n# calculate observed and theoretical quantiles\np &lt;- seq(0.05, 0.95, 0.05)\nobserved_quantiles &lt;- quantile(x, p)\ntheoretical_quantiles &lt;- qnorm(p, mean = mean(x), sd = sd(x))\n\n# make QQ-plot\nplot(theoretical_quantiles, observed_quantiles)\nabline(0,1)\n\n\n\n\n\n\n\n# make QQ-plot with scaled values\nobserved_quantiles &lt;- quantile(z, p)\ntheoretical_quantiles &lt;- qnorm(p)\nplot(theoretical_quantiles, observed_quantiles)\nabline(0,1)"
  },
  {
    "objectID": "posts/dataviz/index.html#percentiles-1",
    "href": "posts/dataviz/index.html#percentiles-1",
    "title": "Data Visualization",
    "section": "Percentiles",
    "text": "Percentiles\n\nKey Points:\n\nPercentiles are the quantiles obtained when defining \\(p\\) as 0.01, 0.02,…,0.99. They summarize the values at which a certain percent of the observations are equal to or less than that value.\nThe 50th percentile is also known as the median.\nThe quartiles are the 25th, 50th and 75th percentiles."
  },
  {
    "objectID": "posts/dataviz/index.html#boxplots",
    "href": "posts/dataviz/index.html#boxplots",
    "title": "Data Visualization",
    "section": "Boxplots",
    "text": "Boxplots\nR语言如何绘制箱线图\n\nKey Points:\n\nWhen data do not follow a normal distribution and cannot be succinctly summarized by only the mean and standard deviation, an alternative is to report a five-number summary: range (ignoring outliers) and the quartiles (25th, 50th, 75th percentile).\nIn a boxplot, the box is defined by the 25th and 75th percentiles and the median is a horizontal line through the box. The whiskers show the range excluding outliers, and outliers are plotted separately as individual points.\nThe interquartile range is the distance between the 25th and 75th percentiles.\nBoxplots are particularly useful when comparing multiple distributions."
  },
  {
    "objectID": "posts/dataviz/index.html#distribution-of-female-heights",
    "href": "posts/dataviz/index.html#distribution-of-female-heights",
    "title": "Data Visualization",
    "section": "Distribution of Female Heights",
    "text": "Distribution of Female Heights\n\nKey Points:\n\nIf a distribution is not normal, it cannot be summarized with only the mean and standard seviation. Provide a histogram, smooth density or boxplot instead.\nA plot can force us to see unexpected results that make us question the quality or implication of our data."
  },
  {
    "objectID": "posts/dataviz/index.html#overview-1",
    "href": "posts/dataviz/index.html#overview-1",
    "title": "Data Visualization",
    "section": "Overview",
    "text": "Overview\nAfter completing ggplot2, we will:\n\nbe able to use ggplot2 to create data visualizations in R.\nbe able to explain what the data component of a graph is.\nbe able to identify the geometry component of a graph and know when to use which type of geometry. be able to explain what the aesthetic mapping component of a graph is.\nbe able to understand the scale component of a graph and select an appropriate scale component to use."
  },
  {
    "objectID": "posts/dataviz/index.html#ggplot",
    "href": "posts/dataviz/index.html#ggplot",
    "title": "Data Visualization",
    "section": "ggplot",
    "text": "ggplot\n\nggplot2\n\nData visualization with ggolot2\nData visualization with ggplot2: Cheat Sheet\nThe R graph gallery example\n\n\n\nkey Points:\n\nThroughout the series, we will create plots with the ggplot2 package. ggplot2 is part of the tidyverse suite of package, which you can load with library(tidyverse).\nNote that you can also load ggplot2 alone using the command library(ggplot2), instead of loading the entire tidyverse.\nggplot2 uses a grammar of graphics to break plots into building blocks that have intuitive syntax, making it easy to create relatively complex and aesthetically pleasing plots with relatively simple and readable code.\nggplot2 is designed to work excusively with tidy data (rows are observations and columns are variables)."
  },
  {
    "objectID": "posts/dataviz/index.html#graph-components",
    "href": "posts/dataviz/index.html#graph-components",
    "title": "Data Visualization",
    "section": "Graph Components",
    "text": "Graph Components\n\nKey Points:\n\nPlots in ggplot2 consist of 3 main components:\n\nData: The dataset being summarized\nGeometry: The type of plot(scatterplot, boxplot, barplot, histogram, qqplot, smooth desity, etc.)\nAesthetic mapping: Variable mapped to visual cues, such as x-axis and y-axis values and color.\n\n\n\n\nCode:\n\nlibrary(dslabs)\ndata(murders)"
  },
  {
    "objectID": "posts/dataviz/index.html#creating-a-new-plot",
    "href": "posts/dataviz/index.html#creating-a-new-plot",
    "title": "Data Visualization",
    "section": "Creating a New Plot",
    "text": "Creating a New Plot\n\nKey Points:\n\nYou can associated a dataset x with a ggplot object with any of the 3 commands:\n\nggplot(data = x)\nggplot(x)\nx %&gt;% ggplot()\n\nYou can assign a ggplot object to a variable. If the object is not assigned to a variable, it will automatically be displayed.\nYou can display a ggplot object assigned to a variable by printing that variable.\n\nCode:\n\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(murders)\n\nggplot(data = murders)\n\nmurders %&gt;% ggplot()\n\np &lt;- ggplot(data = murders)\n\nclass(p)\n\nprint(p) # this is equivalent to simply typing p\np"
  },
  {
    "objectID": "posts/dataviz/index.html#layers",
    "href": "posts/dataviz/index.html#layers",
    "title": "Data Visualization",
    "section": "Layers",
    "text": "Layers\n\nKey Points:\n\nIn ggplot2, graphs are created by adding layers to the ggplot object: DATA %&gt;% ggplot() + LAYER_1 + LAYER_2 + … + LAYER_N\nThe geometry layer defines that plot type and takes the format geom_x where x is the plot type.\nAesthetic mappings describe how properties of the data connect with features of the graph (axis position, color, size, etc.) define aesthetic mapping with aes() function.\naes() uses variable names from the object component (for example, total rather than murders$total).\ngeom_point() creates a scatterplot and requires x and y aesthetic mappings.\ngeom_text() and geom_label add text to a scatterplot and require x, y, and label aesthetic mappings.\nTo determine which aesthetic mappings are required for a geometry, read the help file for that geometry.\nYou can add layers with different aesthetic mappings to the same graph.\n\nCode: Adding layers to a plot\n\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(murders)\n\nmurders %&gt;% ggplot() +\n  geom_point(aes(x = population/10^6, y = total))\n\n\n# add points layer to predefined ggplot object\np &lt;- ggplot(data = murders)\np + geom_point(aes(population/10^6, total))\n\n\n\n\n\n\n\n# add text layer to scatterplot\np + geom_point(aes(population/10^6, total)) +\n  geom_text(aes(population/10^6, total, label = abb))\n\n\n\n\n\n\n\n\nCode: Example of aes behavior\n\n# no error from this call\np_test &lt;- p + geom_text(aes(population/10^6, total, lable = abb))\n\n# error - \"abb\" is not a globally defined variable and cannot be found outside of aes\np_test &lt;- p + geom_text(aes(population/10^6, total), label = abb)"
  },
  {
    "objectID": "posts/dataviz/index.html#thinkering",
    "href": "posts/dataviz/index.html#thinkering",
    "title": "Data Visualization",
    "section": "Thinkering",
    "text": "Thinkering\n\nKey Points:\n\nYou can modify arguments to geometry functions others than aes() and the data.\nThese arguments are not aesthetic mappings: the affect all data points the same way.\nGlobal aesthetic mappings apply to all geometries and can be defined when you initially call ggplot(). All the geometries added as layers will default to this mapping. Local aesthetic mapping add additional information or override the default mappings.\n\n\n\n\n\n\n\nNudge points a fixed distance\n\n\n\nposition_nudge(x = 0, y = 0) is generally useful for adjusting the position of items on discrete scales by a small amount. Nudging is built in to geom_text() because it’s so useful for moving labels a small distance from what they’re labeling.\n\n\nCode:\n\n# change the size of the points\np + geom_point(aes(population/10^6, total), size = 3) +\n    geom_text(aes(population/10^6, total, label = abb))\n\n\n\n\n\n\n\n# move text labels slightly to the right\np + geom_point(aes(population/10^6, total), size = 3) +\n    geom_text(aes(population/10^6, total, label = abb), nudge_x = 1)\n\n\n\n\n\n\n\n# simplify code by adding global aesthetic\np &lt;- murders %&gt;% ggplot(aes(population/10^6, total, label = abb))\np + geom_point(size = 3) +\n    geom_text(nudge_x = 1.5)\n\n\n\n\n\n\n\n# local aesthetics override global aesthetics\np + geom_point(size = 3) +\n  geom_text(aes(x = 10, y = 800, label = \"Hello there!\"))\n\nWarning in geom_text(aes(x = 10, y = 800, label = \"Hello there!\")): All aesthetics have length 1, but the data has 51 rows.\nℹ Did you mean to use `annotate()`?"
  },
  {
    "objectID": "posts/dataviz/index.html#scales-labels-and-colors",
    "href": "posts/dataviz/index.html#scales-labels-and-colors",
    "title": "Data Visualization",
    "section": "Scales, Labels, and Colors",
    "text": "Scales, Labels, and Colors\n\nTextbook links:\n\nTextbook section on scales\nTextbook section on labels and titles\nTextbook section on categories as colors\nTextbook section on annotation, shapes and adjustments\n\n\n\nKey Points:\n\nConvert the x-axis to log scale with scale_x_continuous(trans = \"log10\") or scale_x_log10(). Similar function exist for the y-axis.\nAdd axis title with xlab() and ylab() function. Add a plot title with the ggtitle() function.\nAdd a color mapping that colors points by a varaibale by defining col argument within aes(). To color all pints the same way, define col outside of aes().\nAdd a line with the geom_abline() geometry. geom_abline() takes arguments slop (default = 1) and intercept(default = 0). Change the color with col or color and line type with lty.\nPlacing the line layer after the point layer will overlay the the line on top of the points. To overlay points on the line, place the line layer before the point layer.\nThere are many additional ways to tweak your graph that can be found in the ggplot2 documentation, cheat sheet or on the internet. For example, you can change the legend title with scale_color_discrete.\n\n\n\nCode: Log-scale the x-axis and y-axis\n\n# define p\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(murders)\n\np &lt;- murders %&gt;% ggplot(aes(population/10^6, total, label = abb))\n\n# log base 10 scale the x-axis and y-axis\np + geom_point(size = 3) +\n    geom_text(nudge_x = 0.05) +\n    scale_x_continuous(trans = \"log10\") +\n    scale_y_continuous(trans = \"log10\")\n\n\n# efficient log scaling of the axes\np + geom_point(size = 3) +\n    geom_text(nudge_x = 0.05) +\n    scale_x_log10() +\n    scale_y_log10()\n\n\n\n\n\n\n\n\n\n\nCode: Add labels and title\n\np + geom_point(size = 3) +\n    geom_text(nudge_x = 0.05) +\n    scale_x_log10() +\n    scale_y_log10() +\n    xlab(\"Population in million(log scale)\") +\n    ylab(\"Total number of murders(log scale)\") +\n    ggtitle(\"US Gun Murders in 2010\")\n\n\n\n\n\n\n\n\n\n\nCode: Change color of the points\n\n# redefine p to be everything except the points layer\np &lt;- murders %&gt;% \n     ggplot(aes(population/10^6, total, label = abb)) +\n     geom_text(nudge_x = 0.075) +\n     scale_x_log10() +\n     scale_y_log10() +\n     xlab(\"Population in million(log scale)\") +\n     ylab(\"Total number of murders(log scale)\") +\n     ggtitle(\"US Gun Murders in 2010\")\n\n\n# make all points blue\np + geom_point(size = 3, color = \"blue\")\n\n\n\n\n\n\n\n\n\n# color points by region\np + geom_point(aes(col = region), size = 3)\n\n\n\n\n\n\n\n\n\n\nCode: Add a line with average murder rate\n\nr &lt;- murders %&gt;% \n     summarize(rate = sum(total) / sum(population) * 10^6) %&gt;%      pull(rate)\n\np &lt;- p + geom_point(aes(col = region), size = 3) +\n         geom_abline(intercept = log10(r)) # slop is default of 1\n\n# change line to dashed and dark grey, line under points\np + geom_abline(intercept = log(r), lty = 2, color = \"darkgrey\") +\n    geom_point(aes(col = region), size = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLine types in R: Ity\n\n\n\nThe different line types available in R are shown in the figure hereafter. The argument lty can be used to specify the line type. To change line width, the argument lwd can be used.\n\n\n\n\nCode: Change legend title\n\n# capitalize legend title\np &lt;- p + scale_color_discrete(name = \"Region\")\np"
  },
  {
    "objectID": "posts/dataviz/index.html#add-on-packages",
    "href": "posts/dataviz/index.html#add-on-packages",
    "title": "Data Visualization",
    "section": "Add-on packages",
    "text": "Add-on packages\n\nTextbook links:\n\nTextbook section on add-on packages\nTextbook section on putting it all together\n\n\n\nKey Points\n\nThe style of a ggplot graph can be changed using the theme() function.\nThe ggthemes package adds additional themes.\nThe ggrepel package includes a geometry that repels text labels, ensuring they do not overlap with each other: geom_text_repel().\n\n\n\nCode: Adding themes\n\n# theme used for graphs in the textbook and course\nlibrary(dslabs)\nds_theme_set()\n\n\n# themes from ggthemes\nlibrary(ggthemes)\n\n\np + theme_economist()    # style of the Economist magazine\n\n\n\n\n\n\n\np + theme_fivethirtyeight()    # style of the FiveThirtyEight website\n\n\n\n\n\n\n\n\n\n\nCode: Putting it all together to assemble the plot\n\n# load libraries\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(ggthemes)\nlibrary(dslabs)\ndata(murders)\n\n\n# define the intercept\nr &lt;- murders %&gt;%\n    summarize(rate = sum(total) / sum(population) * 10^6) %&gt;%\n    .$rate\n    \n# make the plot, combining all elements\nmurders %&gt;%\n    ggplot(aes(population/10^6, total, label = abb)) +\n    geom_abline(intercept = log10(r), lty = 2, color = \"darkgrey\") +\n    geom_point(aes(col = region), size = 3) +\n    geom_text_repel() +\n    scale_x_log10() +\n    scale_y_log10() +\n    xlab(\"Population in millions (log scale)\") +\n    ylab(\"Total number of murders (log scale)\") +\n    ggtitle(\"US Gun Murders in 2010\") +\n    scale_color_discrete(name = \"Region\") +\n    theme_economist()"
  },
  {
    "objectID": "posts/dataviz/index.html#other-examples",
    "href": "posts/dataviz/index.html#other-examples",
    "title": "Data Visualization",
    "section": "Other Examples",
    "text": "Other Examples\n\nTextbook links:\n\nTextbook section on histograms\nTextbook section on density plots\nTextbook section on grids of plots\n\n\n\nKey points\n\ngeom_histogram() creates a histogram. Use the binwidth argument to change the width of bins, the fill argument to change the bar fill color, and the col argument to change bar outline color.\ngeom_density() creates smooth density plots. Change the fill color of the plot with the fill argument.\ngeom_qq() creates a quantile-quantile plot. This geometry requires the sample argument. By default, the data are compared to a standard normal distribution with a mean of 0 and standard deviation of 1. This can be changed with the dparams argument, or the sample data can be scaled.\nPlots can be arranged adjacent to each other using the grid.arrange() function from the gridExtra package. First, create the plots and save them to objects (p1, p2, …). Then pass the plot objects to grid.arrange().\n\n\n\nCode: Histograms in ggplot2\n\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(heights)\n\n# define p\np &lt;- heights %&gt;% \n  filter(sex == \"Male\") %&gt;% \n  ggplot(aes(x=height))\n\n\n# basic histograms\np + geom_histogram() + ggtitle(\"binwidth is default\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\np + geom_histogram(binwidth = 1) + ggtitle(\"binwidth is 1\")\n\n\n\n\n\n\n\n# histogram with blue fill, black outline, labels and title\np + geom_histogram(binwidth = 1, fill =\"blue\", col = \"black\") + \n  xlab(\"Male heights in inches\") +\n  ggtitle(\"histogram\")\n\n\n\n\n\n\n\n\n\n\nCode: Smooth density plots in ggplot2\n\np + geom_density()\n\n\n\n\n\n\n\np + geom_density(fill = \"blue\", col = \"red\") +\n  xlab(\"Male heights in inches\") +\n  ylab(\"proportion of Male heights\") +\n  ggtitle(\"Male heights distribution\")\n\n\n\n\n\n\n\n\n\n\nCode: Quantile-quantile plots in ggplot2\n\n# basic QQ-plot\np &lt;- heights %&gt;% filter(sex == \"Male\") %&gt;% \n  ggplot(aes(sample = height))\np + geom_qq()\n\n\n\n\n\n\n\n# QQ-plot against a normal distribution with same mean/sd as data\nparams &lt;- heights %&gt;% \n  filter(sex == \"Male\") %&gt;% \n  summarize(mean = mean(height), sd = sd(height))\np + geom_qq(dparams = params) +\n  geom_abline()\n\n\n\n\n\n\n\n# QQ-plot of scaled data against the standard normal distribution\nheights %&gt;% \n  ggplot(aes(sample = scale(height))) +\n  geom_qq() +\n  geom_abline()\n\n\n\n\n\n\n\n\n\n# define plots p1, p2, p3\np &lt;- heights %&gt;% filter(sex == \"Male\") %&gt;% ggplot(aes(x = height))\np1 &lt;- p + geom_histogram(binwidth = 1, fill = \"blue\", col = \"black\")\np2 &lt;- p + geom_histogram(binwidth = 2, fill = \"blue\", col = \"black\")\np3 &lt;- p + geom_histogram(binwidth = 3, fill = \"blue\", col = \"black\")\n\n\n# arrange plots next to each other in 1 row, 3 columns\nlibrary(gridExtra)\n\n\ngrid.arrange(p1, p2, p3, ncol = 3)"
  },
  {
    "objectID": "posts/dataviz/index.html#overview-2",
    "href": "posts/dataviz/index.html#overview-2",
    "title": "Data Visualization",
    "section": "Overview",
    "text": "Overview\nAfter completing Gapminder, you will: - understand how Hans Rosling and the Gapminder Foundation use effective data visualization to convey data-based trends.\n\nbe able to apply the ggplot2 techniques from the previous section to answer questions using data.\nunderstand how fixed scales across plots can ease comparisons.\nbe able to modify graphs to improve data visualization."
  },
  {
    "objectID": "posts/dataviz/index.html#introduction-to-gapminder",
    "href": "posts/dataviz/index.html#introduction-to-gapminder",
    "title": "Data Visualization",
    "section": "Introduction to Gapminder",
    "text": "Introduction to Gapminder\nCase study: Trends in World Health and Economics\nData Source form Gapminder\nWe will use this data to answer the following questions about World Health and Economics: - Is it still fair to consider the world as divided into the West and the developing world? - Has income inequality across countries worsened over the last 40 years?"
  },
  {
    "objectID": "posts/dataviz/index.html#gapminder-dataset",
    "href": "posts/dataviz/index.html#gapminder-dataset",
    "title": "Data Visualization",
    "section": "Gapminder Dataset",
    "text": "Gapminder Dataset\n\nKey Points\n\nA selection of world health and economics statistics from the Gapminder project can be found in the dslabs package as data(gapminder).\nMost people have misconceptions about world health and economics, which can be addressed by considering real data.\n\n\n\nCode\n\nlibrary(dslabs)\ndata(\"gapminder\")\n\n\nhead(gapminder)\n\n              country year infant_mortality life_expectancy fertility\n1             Albania 1960           115.40           62.87      6.19\n2             Algeria 1960           148.20           47.50      7.65\n3              Angola 1960           208.00           35.98      7.32\n4 Antigua and Barbuda 1960               NA           62.97      4.43\n5           Argentina 1960            59.87           65.39      3.11\n6             Armenia 1960               NA           66.86      4.55\n  population          gdp continent          region\n1    1636054           NA    Europe Southern Europe\n2   11124892  13828152297    Africa Northern Africa\n3    5270844           NA    Africa   Middle Africa\n4      54681           NA  Americas       Caribbean\n5   20619075 108322326649  Americas   South America\n6    1867396           NA      Asia    Western Asia\n\nnames(gapminder)\n\n[1] \"country\"          \"year\"             \"infant_mortality\" \"life_expectancy\" \n[5] \"fertility\"        \"population\"       \"gdp\"              \"continent\"       \n[9] \"region\"          \n\n\n\ngapminder %&gt;% \n  filter(year == 2015 & country %in% c(\"Sri Lanka\", \"Turkey\")) %&gt;% \n  select(country, infant_mortality)\n\n    country infant_mortality\n1 Sri Lanka              8.4\n2    Turkey             11.6"
  },
  {
    "objectID": "posts/dataviz/index.html#life-expectancy-and-fertility-rates",
    "href": "posts/dataviz/index.html#life-expectancy-and-fertility-rates",
    "title": "Data Visualization",
    "section": "Life Expectancy and Fertility Rates",
    "text": "Life Expectancy and Fertility Rates\n\nKey Points\n\nA prevalent worldview is that the world is divided into two groups of countries:\n\nWestern world: high life expectancy, low fertility rate\nDeveloping world: lower life expectancy, higher fertility rate\n\nGapminder data can be used to evaluate the validity of this view.\nA scatterplot of life expectancy versus fertility rate in 1962 suggests that this viewpoint was grounded in reality 50 years ago. Is it still the case today?\n\n\n\nCode\n\n# basic scatterplot of life expectancy versus fertility\nds_theme_set() # set plot theme\nfilter(gapminder, year == 1962) %&gt;% \n  ggplot(aes(fertility, life_expectancy)) +\n  geom_point()\n\n\n\n\n\n\n\n# add color as continent\nfilter(gapminder, year == 1962) %&gt;% \n  ggplot(aes(fertility, life_expectancy, color = continent)) +\n  geom_point()"
  },
  {
    "objectID": "posts/dataviz/index.html#faceting",
    "href": "posts/dataviz/index.html#faceting",
    "title": "Data Visualization",
    "section": "Faceting",
    "text": "Faceting\n\nKey Points\n\nFaceting makes multiple side-by-side plots stratified by some variable. This is a way to ease comparisons.\nThe facet_grid() function allows faceting by up to two variables, with rows faceted by one variable and columns faceted by the other variable. To facet by only one variable, use the dot operator as the other variable.\nThe facet_wrap() function facets by one variable and automatically wraps the series of plots so they have readable dimensions.\nFaceting keeps the axes fixed across all plots, easing comparisons between plots.\nThe data suggest that the developing versus Western world view no longer makes sense in 2012.\n\n ggplot2-分面(facet) 一页多图数据可视化章节学习facet\n\n\nCode\n\n# facet by continent and year\nfilter(gapminder, year %in% c(1962, 2012)) %&gt;% \n  ggplot(aes(fertility, life_expectancy, col = continent)) +\n  geom_point() +\n  facet_grid(continent ~ year)\n\n\n\n\n\n\n\n# facet by year only \nfilter(gapminder, year %in% c(1962, 2012)) %&gt;% \n  ggplot(aes(fertility, life_expectancy, col = continent)) +\n  geom_point() +\n  facet_grid(. ~ year)\n\n\n\n\n\n\n\n# facet by year, plots wrapped onto multiple rows\nyears &lt;- c(1962, 1980, 1990, 2000, 2012)\ncontinents &lt;- c(\"Europ\", \"Asia\")\ngapminder %&gt;% \n  filter(year %in% years & continent %in% continent) %&gt;% \n  ggplot(aes(fertility, life_expectancy, col = continent)) +\n  geom_point() +\n  facet_wrap(. ~ year)"
  },
  {
    "objectID": "posts/dataviz/index.html#time-series-plots",
    "href": "posts/dataviz/index.html#time-series-plots",
    "title": "Data Visualization",
    "section": "Time Series Plots",
    "text": "Time Series Plots\n\nKey Points\n\nTime series plots have time on the x-axis and a variable of interest on the y-axis.\nThe geom_line() geometry connects adjacent data points to form a continuous line. A line plot is appropriate when points are regularly spaced, densely packed and from a single data series.\nYou can plot multiple lines on the same graph. Remember to group or color by a variable so that the lines are plotted independently.\nLabeling is usually preferred over legends. However, legends are easier to make and appear by default. Add a label with geom_text(), specifying the coordinates where the label should appear on the graph.\n\n\n\nCode: Single Time Series\n\n# scatterplot of US fertility by year\ngapminder %&gt;% \n  filter(country == \"United States\") %&gt;% \n  ggplot(aes(year, fertility)) +\n  geom_point()\n\n\n\n\n\n\n\n# line plot of US fertility by year\ngapminder %&gt;% \n  filter(country == \"United States\") %&gt;% \n  ggplot(aes(year, fertility)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\nCode: Multiple Time Series\n\n# line plot fertility time series for two countries- only one line (incorrect)\ncountries &lt;- c(\"South Korea\", \"Germany\")\ngapminder %&gt;% filter(country %in% countries) %&gt;%\n    ggplot(aes(year, fertility)) +\n    geom_line()\n\n\n\n\n\n\n\n# line plot fertility time series for two countries - one line per country\ngapminder %&gt;% filter(country %in% countries) %&gt;%\n    ggplot(aes(year, fertility, group = country)) +\n    geom_line()\n\n\n\n\n\n\n\n# fertility time series for two countries - lines colored by country\ngapminder %&gt;% filter(country %in% countries) %&gt;%\n    ggplot(aes(year, fertility, col = country)) +\n    geom_line()\n\n\n\n\n\n\n\n\n\n\nCode: Adding text labels to a plot\n\n\n\n\n\n\nNote\n\n\n\nlabels data frame as the data to ensure where to start label text \n\n\n\n# life expectancy time series - lines colored by country and labeled, no legend\nlabels &lt;- data.frame(country = countries, x = c(1975, 1965), y = c(60, 72))\ngapminder %&gt;% filter(country %in% countries) %&gt;%\n    ggplot(aes(year, life_expectancy, col = country)) +\n    geom_line() +\n    geom_text(data = labels, aes(x, y, label = country), size = 5) +\n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/dataviz/index.html#transformations",
    "href": "posts/dataviz/index.html#transformations",
    "title": "Data Visualization",
    "section": "Transformations",
    "text": "Transformations\n\nKey Points\n\nWe use GDP data to compute income in US dollars per day, adjusted for inflation.\nLog transformations covert multiplicative changes into additive changes.\ncommon transformations are the log base 2 transformation and the log base 10 transformation. The choice of base depends on the range of the data. The natural log is not recommended for visualization because it is difficult to interpret.\nThe mode of a distribution is the value with the highest frequency. The mode of a normal distribution is the average. A distribution can have multiple local modes.\nThere are two ways to use log transformations in plots: transform the data before plotting or transform the axes of the plot. Log scales have the advantage of showing the original values as axis labels, while log transformed values ease interpretation of intermediate values between labels.\nScale the x-axis using scale_x_continuous() or scale_x_log10() layers in ggplot2. Similar functions exist for the y-axis.\nIn 1970, income distribution is bimodal, consistent with the dichotomous Western versus developing worldview.\n\n\n\nCode\n\n# add dollars per day variable\ngapminder &lt;- gapminder %&gt;% \n  mutate(dollars_per_day = gdp/population/365)\n\n# histogram of dollars per day\npast_year &lt;- 1970\ngapminder %&gt;% \n  filter(year == past_year & !is.na(gdp)) %&gt;% \n  ggplot(aes(dollars_per_day)) +\n  geom_histogram(binwidth = 1, color = \"black\")\n\n\n\n\n\n\n\n# repeat histogram with log2 scaled data\ngapminder %&gt;%\n    filter(year == past_year & !is.na(gdp)) %&gt;%\n    ggplot(aes(log2(dollars_per_day))) +\n    geom_histogram(binwidth = 1, color = \"black\")\n\n\n\n\n\n\n\n# repeat histogram with log2 scaled x-axis\ngapminder %&gt;%\n    filter(year == past_year & !is.na(gdp)) %&gt;%\n    ggplot(aes(dollars_per_day)) +\n    geom_histogram(binwidth = 1, color = \"black\") +\n    scale_x_continuous(trans = \"log2\")"
  },
  {
    "objectID": "posts/dataviz/index.html#stratify-and-boxplot",
    "href": "posts/dataviz/index.html#stratify-and-boxplot",
    "title": "Data Visualization",
    "section": "Stratify and Boxplot",
    "text": "Stratify and Boxplot\n\nKey Points\n\nMake boxplots stratified by a categorical variable using the geom_boxplot() geometry.\nRotate axis labels by changing the theme through element_text(). You can change the angle and justification of the text labels.\nConsider ordering your factors by a meaningful value with the reorder function, which changes the order of factor levels based on a related numeric vector. This is a way to ease comparisons.\nShow the data by adding data points to the boxplot with a geom_point layer. This adds information beyond the five-number summary to your plot, but too many data points it can obfuscate your message.\n\n\n\nCode: Boxplot of GDP by region\n\n# add dollars per day variable\ngapminder &lt;- gapminder %&gt;% \n  mutate(dollars_per_day = gdp/population/365)\n\n# number of regions\nlength(levels(gapminder$region))\n\n[1] 22\n\n# boxplot of GDP by region in 1970\npast_year &lt;- 1970\np &lt;- gapminder %&gt;% \n     filter(year == past_year & !is.na(gdp)) %&gt;% \n     ggplot(aes(region, dollars_per_day))\np + geom_boxplot()\n\n\n\n\n\n\n\n# roation name on x-axis\np + geom_boxplot() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n\n\n\nCode: The reorder function\n\n\n\n\n\n\nTip\n\n\n\nReorder a variable with ggplot2\n\n\n\n# by default, factor order is alphabetical\nfac &lt;- factor(c(\"Asia\", \"Asia\", \"West\", \"West\", \"West\"))\nlevels(fac)\n\n[1] \"Asia\" \"West\"\n\n# reorder factor by the category means\nvalue &lt;- c(10, 11, 12, 6, 4)\nfac &lt;- reorder(fac, value, FUN = mean)\nlevels(fac)\n\n[1] \"West\" \"Asia\"\n\n\n\n\nCode: Enhanced boxplot ordered by median income, scaled, and showing data\n\n# reorder by median income and color by continent \np &lt;- gapminder %&gt;%\n    filter(year == past_year & !is.na(gdp)) %&gt;%\n    mutate(region = reorder(region, dollars_per_day, FUN = median)) %&gt;%  # reorder\n    ggplot(aes(region, dollars_per_day, fill = continent)) + # color by continent \n    geom_boxplot() +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n    xlab(\"\")\np\n\n\n\n\n\n\n\n# log2 scale y-axis\np + scale_y_continuous(trans = \"log2\")\n\n\n\n\n\n\n\n# add data points\np + scale_y_continuous(trans = \"log2\") + geom_point(show.legend = FALSE)"
  },
  {
    "objectID": "posts/dataviz/index.html#comparing-distributions",
    "href": "posts/dataviz/index.html#comparing-distributions",
    "title": "Data Visualization",
    "section": "Comparing Distributions",
    "text": "Comparing Distributions\n\n\n\n\n\n\nImportant\n\n\n\nintersect(交集);union(并集);setdiff(找不同);setequal(判断相同)\n\n\n\nKey Points\n\nUse intersect to find the overlap between two vectors.\nTo make boxplots where grouped variables are adjacaent, color the boxplot by a factor instead of faceting by that factor. This is a way to ease comparisions.\nThe data suggest that the income gap between rich and poor countries has narrowed, not expended.\n\n\n\nCode: Histogram of income in West versus developing world, 1970 and 2010\n\n# add dollars per day variable and define past year\ngapminder &lt;- gapminder %&gt;% \n  mutate(dollars_per_day = gdp/population/365)\npast_year &lt;- 1970\n\n# define Western countries\nwest &lt;- c(\"Western Europe\", \"Northern Europe\", \"Southern Europe\", \"Northern America\", \"Australia and New Zealand\")\n\n# facet by West vs Devloping \ngapminder %&gt;% \n  filter(year == past_year & !is.na(gdp)) %&gt;% \n  mutate(group = ifelse(region %in% west, \"West\", \"Developing\")) %&gt;% \n  ggplot(aes(dollars_per_day)) +\n  geom_histogram(binwidth = 1, color = \"black\") +\n  scale_x_continuous(trans = \"log2\") +\n  facet_grid(. ~group)\n\n\n\n\n\n\n\n# facet by West/Developing and year\npresent_year &lt;- 2010\ngapminder %&gt;%\n    filter(year %in% c(past_year, present_year) & !is.na(gdp)) %&gt;%\n    mutate(group = ifelse(region %in% west, \"West\", \"Developing\")) %&gt;%\n    ggplot(aes(dollars_per_day)) +\n    geom_histogram(binwidth = 1, color = \"black\") +\n    scale_x_continuous(trans = \"log2\") +\n    facet_grid(year ~ group)\n\n\n\n\n\n\n\n\n\n\nCode: Income distribution of West verseus Developing world, only countries with data\n\n# define countries that have data available in both years\ncountry_list_1 &lt;- gapminder %&gt;% \n  filter(year == past_year & !is.na(dollars_per_day)) %&gt;% .$country\n\ncountry_list_2 &lt;- gapminder %&gt;% \n  filter(year == present_year & !is.na(dollars_per_day)) %&gt;% .$country\n\ncountry_list &lt;- intersect(country_list_1, country_list_2)\n\n# make histogram including only countries with data availabe in both years\ngapminder %&gt;% \n  filter(year %in% c(past_year, present_year) & country %in% country_list) %&gt;% # keep only selected countries\n  mutate(group = ifelse(region %in% west, \"West\", \"Developing\")) %&gt;% \n  ggplot(aes(dollars_per_day)) +\n  geom_histogram(binwidth = 1, color = \"black\") +\n  scale_x_continuous(trans = \"log2\") +\n  facet_grid(year ~ group)\n\n\n\n\n\n\n\n\n\n\nCode: Boxplots of income in West versus Developing world, 1970 and 2010\n\np &lt;- gapminder %&gt;% \n  filter(year %in% c(past_year, present_year) & country %in% country_list) %&gt;%\n  mutate(region = reorder(region, dollars_per_day, FUN = median)) %&gt;% \n  ggplot() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  xlab(\"\") + scale_y_continuous(trans = \"log2\") \n\np + geom_boxplot(aes(region, dollars_per_day, fill = continent)) +\n  facet_grid(year ~ .)\n\n\n\n\n\n\n\n# arrange matching boxplots next to each other, colored by year\np + geom_boxplot(aes(region, dollars_per_day, fill = factor(year)))"
  },
  {
    "objectID": "posts/dataviz/index.html#density-plots",
    "href": "posts/dataviz/index.html#density-plots",
    "title": "Data Visualization",
    "section": "Density Plots",
    "text": "Density Plots\n\n\n\n\n\n\nTip\n\n\n\n\ndplyr处理数据时常用的的函数\n在 R Dplyr 包中使用 case when 语句\n\n\n\n\nKey Points\n\nChange the y-axis of density plots to variable counts using ..count.. as the y argument.\nThe case_when() function defines a factor whose levels are defined by a variety of logical operations to group data.\nPlot stacked density plots using position=\"stack\".\nDefine a weight aesthetic mapping to change the relative weights of density plots-for example, this allow weighting of plots by population rather than number of countries.\n\n\n\nCode: Faceted smooth density plots\n\n# see the code below the previous video for variable definitions\n\n# smooth density plots - area under each curve adds to 1\ngapminder %&gt;% \n  filter(year == past_year & country %in% country_list) %&gt;% \n  mutate(group = ifelse(region %in% west, \"West\", \"Developing\")) %&gt;% group_by(group) %&gt;% \n  summarize(n = n()) %&gt;% knitr::kable()\n\n\n\n\ngroup\nn\n\n\n\n\nDeveloping\n87\n\n\nWest\n21\n\n\n\n\n# smooth density plots - variable counts on y-axis\np &lt;- gapminder %&gt;% \n  filter(year == past_year & country %in% country_list) %&gt;% \n  mutate(group = ifelse(region %in% west, \"West\", \"Developing\")) %&gt;%\n  ggplot(aes(dollars_per_day, y = ..count.., fill = group)) +\n  scale_x_continuous(trans = \"log2\")\np + geom_density(alpha = 0.2, bw = 0.75) + facet_grid(year ~ .)\n\n\n\n\n\n\n\n\n\n\nCode: Add new region group with case_when\n\n# add group as a factor, grouping regions\ngapminder &lt;- gapminder %&gt;% \n  mutate(group = case_when(\n    .$region %in% west ~ \"West\",\n    .$region %in% c(\"Eastern Asia\", \"South-Eastern Asia\") ~ \"East Asia\", \n    .$region %in% c(\"Caribbean\", \"Central America\", \"South America\") ~ \"Latin America\",\n    .$continent == \"Africa\" & .$region != \"Northern Africa\" ~ \"Sub-Saharan Africa\", TRUE ~ \"Others\"))\n\n# reorder factor levels\ngapminder &lt;- gapminder %&gt;% \n  mutate(group = factor(group, levels = c(\"Others\", \"Latin America\", \"East Asia\", \"Sub-Saharan Africa\", \"West\")))\n\n\n\nCode: Stacked density plot\n\n# note you must redefine p with the new gapminder object first\np &lt;- gapminder %&gt;% \n  filter(year %in% c(past_year, present_year) & country %in% country_list) %&gt;% \n  ggplot(aes(dollars_per_day, fill = group)) +\n  scale_x_continuous(trans = \"log2\")\n\n# stacked density plot\np + geom_density(alpha = 0.2, bw = 0.75, position = \"stack\") +\n  facet_grid(year ~ .)\n\n\n\n\n\n\n\n\n\n\nCode: Weighted stacked density plot\n\ngapminder %&gt;% \n  filter(year %in% c(past_year, present_year) & country %in% country_list) %&gt;% \n  group_by(year) %&gt;% \n  mutate(weight = population/sum(population*2)) %&gt;% \n  ungroup() %&gt;% \n  ggplot(aes(dollars_per_day, fill = group, weight = weight)) +\n  scale_x_continuous(trans = \"log2\") +\n  geom_density(alpha = 0.2, bw = 0.75, position = \"stack\") + facet_grid(year ~ .)"
  },
  {
    "objectID": "posts/dataviz/index.html#ecological-fallacy",
    "href": "posts/dataviz/index.html#ecological-fallacy",
    "title": "Data Visualization",
    "section": "Ecological Fallacy",
    "text": "Ecological Fallacy\n\nTextbook link\nEcological Fallacy\n\n\nKey Points\n\nThe breaks argument allows us to set the location of the axis labels and tick marks.\nthe logistic or logit transformation is defined as \\(f(p)=log\\frac{1}{1-p}\\), or the log of odds. This scale is useful for highlighting difference near 0 or near 1 and converts fold changes into constant increase.\nThe ecological fallacy is assuming that conclusion made from the average of a group apply to all members of that group.\n\n\n\nCode\n\n# define gapminder\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(gapminder)\n\n\n# add additional cases\ngapminder &lt;- gapminder %&gt;%\n    mutate(group = case_when(\n        .$region %in% west ~ \"The West\",\n        .$region %in% \"Northern Africa\" ~ \"Northern Africa\",\n        .$region %in% c(\"Eastern Asia\", \"South-Eastern Asia\") ~ \"East Asia\",\n        .$region == \"Southern Asia\" ~ \"Southern Asia\",\n        .$region %in% c(\"Central America\", \"South America\", \"Caribbean\") ~ \"Latin America\",\n        .$continent == \"Africa\" & .$region != \"Northern Africa\" ~ \"Sub-Saharan Africa\",\n        .$region %in% c(\"Melanesia\", \"Micronesia\", \"Polynesia\") ~ \"Pacific Islands\"))\n\n# define a data frame with group average income and average infant survival rate\nsurv_income &lt;- gapminder %&gt;%\n    filter(year %in% present_year & !is.na(gdp) & !is.na(infant_mortality) & !is.na(group)) %&gt;%\n    group_by(group) %&gt;%\n    summarize(income = sum(gdp)/sum(population)/365,\n                        infant_survival_rate = 1 - sum(infant_mortality/1000*population)/sum(population))\nsurv_income %&gt;% arrange(income)\n\n# A tibble: 7 × 3\n  group              income infant_survival_rate\n  &lt;chr&gt;               &lt;dbl&gt;                &lt;dbl&gt;\n1 Sub-Saharan Africa   1.76                0.936\n2 Southern Asia        2.07                0.952\n3 Pacific Islands      2.70                0.956\n4 Northern Africa      4.94                0.970\n5 Latin America       13.2                 0.983\n6 East Asia           13.4                 0.985\n7 The West            77.1                 0.995\n\n# plot infant survival versus income, with transformed axes\nsurv_income %&gt;% ggplot(aes(income, infant_survival_rate, label = group, color = group)) +\n    scale_x_continuous(trans = \"log2\", limit = c(0.25, 150)) +\n    scale_y_continuous(trans = \"logit\", limit = c(0.875, .9981),\n                                       breaks = c(.85, .90, .95, .99, .995, .998)) +\n    geom_label(size = 3, show.legend = FALSE)"
  },
  {
    "objectID": "posts/dataviz/index.html#overview-3",
    "href": "posts/dataviz/index.html#overview-3",
    "title": "Data Visualization",
    "section": "Overview",
    "text": "Overview\nData visualization principles covers some general principles that can serve as guides for effective data visualization.\nAfter completing this section, you will:\n\nunderstand basic principles of effective data visualization.\nunderstand the importance of keeping your goal in mind when deciding on a visualization approach.\nunderstand principles for encoding data, including position, aligned lengths, angles, area, brightness, and color hue.\nknow when to include the number zero in visualizations.\nbe able to use techniques to ease comparisons, such as using common axes, putting visual cues to be compared adjacent to one another, and using color effectively."
  },
  {
    "objectID": "posts/dataviz/index.html#encoding-data-using-visual-cues",
    "href": "posts/dataviz/index.html#encoding-data-using-visual-cues",
    "title": "Data Visualization",
    "section": "Encoding Data Using Visual Cues",
    "text": "Encoding Data Using Visual Cues\n\nKey Points\n\nVisual cues for encoding data include position, length, angle, area, brightness and color hue.\nPosition and length are the preferred way to display quantities, followed by angles, which are preferred over area. Brightness and color are even harder to quantify but can sometimes be useful.\nPie charts represent visual cues as both angles and area, while donut charts use only area. Humans are not good at visually quantifying angles and are even worse at quantifying area. Therefore pie and donut charts should be avoided - use a bar plot instead. If you must make a pie chart, include percentages as labels.\nBar plots represent visual cues as position and length. Humans are good at visually quantifying linear measures, making bar plots a strong alternative to pie or donut charts."
  },
  {
    "objectID": "posts/dataviz/index.html#know-when-to-include-zero",
    "href": "posts/dataviz/index.html#know-when-to-include-zero",
    "title": "Data Visualization",
    "section": "Know when to Include Zero",
    "text": "Know when to Include Zero\n\nKey Points\n\nWhen using bar plots, always start at 0. It is deceptive not to start at 0 because bar plots imply length is proportional to the quantity displayed. Cutting off the y-axis can make differences look bigger than they actually are.\nWhen using position rather than length, it is not necessary to include 0 (scatterplot, dot plot, boxplot)."
  },
  {
    "objectID": "posts/dataviz/index.html#do-not-distort-quantitles",
    "href": "posts/dataviz/index.html#do-not-distort-quantitles",
    "title": "Data Visualization",
    "section": "Do not Distort Quantitles",
    "text": "Do not Distort Quantitles\n\nKey Points\n\nMake sure your visualizations encode the correct quantities.\nFor example, if you are using a plot that relies on circle area, make sure the area (rather than the radius) is proportional to the quantity."
  },
  {
    "objectID": "posts/dataviz/index.html#order-by-a-meaningful-value",
    "href": "posts/dataviz/index.html#order-by-a-meaningful-value",
    "title": "Data Visualization",
    "section": "Order by a Meaningful Value",
    "text": "Order by a Meaningful Value\n\nKey Points\n\nIt is easiest to visually extract information from a plot when categories are ordered by a meaningful value. The exact value on which to order will depend on your data and the message you wish to convey with your plot.\nThe default ordering for categories is alphabetical if the categories are strings or by factor level if factors. However, we rarely want alphabetical order."
  },
  {
    "objectID": "posts/dataviz/index.html#show-the-data",
    "href": "posts/dataviz/index.html#show-the-data",
    "title": "Data Visualization",
    "section": "Show the Data",
    "text": "Show the Data\n\nKey Points\n\nA dynamite plot - a bar graph of group averages with error bars denoting standard errors - provides almost no information about a distribution.\nBy showing the data, you provide viewers extra information about distributions.\nJitter is adding a small random shift to each point in order to minimize the number of overlapping points. To add jitter, use the geom_jitter() geometry instead of geom_point(). (See example below.)\nAlpha blending is making points somewhat transparent, helping visualize the density of overlapping points. Add an alpha argument to the geometry.\n\n\n\nCode\n\n# dot plot showing the data\nheights %&gt;% ggplot(aes(sex, height)) + geom_point()\n\n\n\n\n\n\n\n# jittered, alpha blended point plot\nheights %&gt;% ggplot(aes(sex, height)) + geom_jitter(width = 0.1, alpha = 0.2)"
  },
  {
    "objectID": "posts/dataviz/index.html#ease-comparisons-use-common-axes",
    "href": "posts/dataviz/index.html#ease-comparisons-use-common-axes",
    "title": "Data Visualization",
    "section": "Ease Comparisons: Use Common Axes",
    "text": "Ease Comparisons: Use Common Axes\n\nKey Points\n\nEase comparisons by keeping axes the same when comparing data across multiple plots.\nAlign plots vertically to see horizontal changes. Align plots horizontally to see vertical changes.\nBar plots are useful for showing one number but not useful for showing distributions."
  },
  {
    "objectID": "posts/dataviz/index.html#consider-transformations",
    "href": "posts/dataviz/index.html#consider-transformations",
    "title": "Data Visualization",
    "section": "Consider Transformations",
    "text": "Consider Transformations\n\nKey Points\n\nUse transformations when warranted to ease visual interpretation.\nThe log transformation is useful for data with multiplicative changes. The logistic transformation is useful for fold changes in odds. The square root transformation is useful for count data."
  },
  {
    "objectID": "posts/dataviz/index.html#ease-comparisons-compared-visual-cues-should-be-adjacent",
    "href": "posts/dataviz/index.html#ease-comparisons-compared-visual-cues-should-be-adjacent",
    "title": "Data Visualization",
    "section": "Ease Comparisons: Compared Visual Cues Should Be Adjacent",
    "text": "Ease Comparisons: Compared Visual Cues Should Be Adjacent\n\nTextbook links\n\nTextbook section on compared visual cues being adjacent\nTextbook section on using color\nTextbook section on considering the color blind\n\n\n\nKey Points\n\nWhen two groups are to be compared, it is optimal to place them adjacent in the plot.\nUse color to encode groups to be compared.\nConsider using a color blind friendly palette.\n\n\n\nCode\n\ncolor_blind_friendly_cols &lt;- c(\"#999999\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\np1 &lt;- data.frame(x = 1:8, y = 1:8, col = as.character(1:8)) %&gt;%\n    ggplot(aes(x, y, color = col)) +\n    geom_point(size = 5)\np1 + scale_color_manual(values = color_blind_friendly_cols)"
  },
  {
    "objectID": "posts/dataviz/index.html#slope-charts",
    "href": "posts/dataviz/index.html#slope-charts",
    "title": "Data Visualization",
    "section": "Slope Charts",
    "text": "Slope Charts\n\nTextbook link\nPlots for two variables\n\n\nKey Points\n\nConsider using a slope chart or Bland-Altman plot when comparing one variable at two different time points, especially for a small number of observations.\nSlope charts use angle to encode change. Use geom_line() to create slope charts. It is useful when comparing a small number of observations.\nThe Bland-Altman plot (Tukey mean difference plot, MA plot) graphs the difference between conditions on the y-axis and the mean between conditions on the x-axis. It is more appropriate for large numbers of observations than slope charts.\n\n\n\nCode: Slope chart\n\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(gapminder)\n\n\nwest &lt;- c(\"Western Europe\", \"Northern Europe\", \"Southern Europe\", \"Northern America\", \"Australia and New Zealand\")\n\ndat &lt;- gapminder %&gt;%\n    filter(year %in% c(2010, 2015) & region %in% west & !is.na(life_expectancy) & population &gt; 10^7)\n\ndat %&gt;%\n    mutate(location = ifelse(year == 2010, 1, 2),\n           location = ifelse(year == 2015 & country %in% c(\"United Kingdom\", \"Portugal\"),\n                             location + 0.22, location),\n           hjust = ifelse(year == 2010, 1, 0)) %&gt;%\n    mutate(year = as.factor(year)) %&gt;%\n    ggplot(aes(year, life_expectancy, group = country)) +\n    geom_line(aes(color = country), show.legend = FALSE) +\n    geom_text(aes(x = location, label = country, hjust = hjust), show.legend = FALSE) +\n    xlab(\"\") +\n    ylab(\"Life Expectancy\") \n\n\n\n\n\n\n\n\n\n\nCode: Bland-Altman Plot\n\nlibrary(ggrepel)\n\n\ndat %&gt;%\n    mutate(year = paste0(\"life_expectancy_\", year)) %&gt;%\n    select(country, year, life_expectancy) %&gt;% spread(year, life_expectancy) %&gt;%\n    mutate(average = (life_expectancy_2015 + life_expectancy_2010)/2,\n                difference = life_expectancy_2015 - life_expectancy_2010) %&gt;%\n    ggplot(aes(average, difference, label = country)) +\n    geom_point() +\n    geom_text_repel() +\n    geom_abline(lty = 2) +\n    xlab(\"Average of 2010 and 2015\") +\n    ylab(\"Difference between 2015 and 2010\")"
  },
  {
    "objectID": "posts/dataviz/index.html#encoding-a-third-variable",
    "href": "posts/dataviz/index.html#encoding-a-third-variable",
    "title": "Data Visualization",
    "section": "Encoding a Third Variable",
    "text": "Encoding a Third Variable\n\nTextbook link\nEncoding a third variable\n\n\nKey Points\n\nEncode a categorical third variable on a scatterplot using color hue or shape. Use the shape argument to control shape.\nEncode a continuous third variable on a using color intensity or size."
  },
  {
    "objectID": "posts/dataviz/index.html#case-study-vaccines",
    "href": "posts/dataviz/index.html#case-study-vaccines",
    "title": "Data Visualization",
    "section": "Case Study: Vaccines",
    "text": "Case Study: Vaccines\n\nTextbook link\nCase study: vaccines and infectious diseases\ngeom_vline: Add vertical lines\n\n\nKey Points\n\nVaccines save millions of lives, but misinformation has led some to question the safety of vaccines. The data support vaccines as safe and effective. We visualize data about measles incidence in order to demonstrate the impact of vaccination programs on disease rate.\nThe RColorBrewer package offers several color palettes. Sequential color palettes are best suited for data that span from high to low. Diverging color palettes are best suited for data that are centered and diverge towards high or low values.\nThe geom_tile() geometry creates a grid of colored tiles. Position and length are stronger cues than color for numeric values, but color can be appropriate sometimes.\n\n\n\nCode: Tile plot of measles rate by year and state\n\n# import data and inspect\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(us_contagious_diseases)\nstr(us_contagious_diseases)\n\n\n# assign dat to the per 10,000 rate of measles, removing Alaska and Hawaii and adjusting for weeks reporting\nthe_disease &lt;- \"Measles\"\ndat &lt;- us_contagious_diseases %&gt;%\n    filter(!state %in% c(\"Hawaii\", \"Alaska\") & disease == the_disease) %&gt;%\n    mutate(rate = count / population * 10000 * 52/weeks_reporting) %&gt;%\n    mutate(state = reorder(state, rate))\n\n# plot disease rates per year in California\ndat %&gt;% filter(state == \"California\" & !is.na(rate)) %&gt;%\n    ggplot(aes(year, rate)) +\n    geom_line() +\n    ylab(\"Cases per 10,000\") +\n    geom_vline(xintercept=1963, col = \"blue\")\n\n\n\n\n\n\n\n# tile plot of disease rate by state and year\ndat %&gt;% ggplot(aes(year, state, fill=rate)) +\n    geom_tile(color = \"grey50\") +\n    scale_x_continuous(expand = c(0,0)) +\n    scale_fill_gradientn(colors = RColorBrewer::brewer.pal(9, \"Reds\"), trans = \"sqrt\") +\n    geom_vline(xintercept = 1963, col = \"blue\") +\n    theme_minimal() + theme(panel.grid = element_blank()) +\n    ggtitle(the_disease) +\n    ylab(\"\") +\n    xlab(\"\")\n\n\n\n\n\n\n\n\n\n\nCode: Line plot of measles rate by year and state\n\n# compute US average measles rate by year\navg &lt;- us_contagious_diseases %&gt;%\n    filter(disease == the_disease) %&gt;% group_by(year) %&gt;%\n    summarize(us_rate = sum(count, na.rm = TRUE)/sum(population, na.rm = TRUE)*10000)\n\n# make line plot of measles rate by year by state\ndat %&gt;%\n    filter(!is.na(rate)) %&gt;%\n    ggplot() +\n    geom_line(aes(year, rate, group = state), color = \"grey50\", \n        show.legend = FALSE, alpha = 0.2, size = 1) +\n    geom_line(mapping = aes(year, us_rate), data = avg, size = 1, col = \"black\") +\n    scale_y_continuous(trans = \"sqrt\", breaks = c(5, 25, 125, 300)) +\n    ggtitle(\"Cases per 10,000 by state\") +\n    xlab(\"\") +\n    ylab(\"\") +\n    geom_text(data = data.frame(x = 1955, y = 50),\n        mapping = aes(x, y, label = \"US average\"), color = \"black\") +\n    geom_vline(xintercept = 1963, col = \"blue\")"
  },
  {
    "objectID": "posts/dataviz/index.html#avoid-pseudo-and-gratuitous-3d-plots",
    "href": "posts/dataviz/index.html#avoid-pseudo-and-gratuitous-3d-plots",
    "title": "Data Visualization",
    "section": "Avoid Pseudo and Gratuitous 3D Plots",
    "text": "Avoid Pseudo and Gratuitous 3D Plots\n\nTextbook link\nAvoid pseudo-three-dimensional plots\n\n\nKey Points\nIn general, pseudo-3D plots and gratuitous 3D plots only add confusion. Use regular 2D plots instead."
  },
  {
    "objectID": "posts/dataviz/index.html#avoid-too-many-significant-digits",
    "href": "posts/dataviz/index.html#avoid-too-many-significant-digits",
    "title": "Data Visualization",
    "section": "Avoid Too Many Significant Digits",
    "text": "Avoid Too Many Significant Digits\n\nTextbook link\nAvoid too many significant digits\n\n\nKey points\n\nIn tables, avoid using too many significant digits. Too many digits can distract from the meaning of your data.\nReduce the number of significant digits globally by setting an option. For example, options(digits = 3) will cause all future computations that session to have 3 significant digits.\nReduce the number of digits locally using round() or signif()."
  },
  {
    "objectID": "posts/rbasics/index.html",
    "href": "posts/rbasics/index.html",
    "title": "The basics of R programming",
    "section": "",
    "text": "In this section, I will introduce you to R Basics, Functions, and Datatypes.\nIn this part, you will learn to:\n\nAppreciate the rationale for data analysis using R.\nDefine objects and perform basic arithmetic and logical operations.\nUse pre-defined functions to perform operations on objects.\nDistinguish between various data types.\n\n\n\n\n\n\nTo complete this course, you should install R locally on your computer. We also highly recommend installing RStudio, an integrated development environment (IDE), to edit and test your code.\nIn order to complete some assignments in the course, you will need your own copy of R. You may also find it helpful to follow along with the course videos in R or RStudio.\nBoth R and RStudio can be freely downloaded and installed.\n\n\n\n\n\nYou need to install R before using RStudio, which is an interactive desktop environment.\nSelect base subdirectory in CRAN and click download.\nSelect all default choices in the installation process.\nWe recommend selecting English for language to help you better follow the course.\nYou can try using the R console, but for productivity purposes, we can switch to RStudio.\n\n\n\n\n\n\n\n\nYou can download the latest version of RStudio at the RStudio website.\nThe free desktop version is more than enough for this course.\nMake sure to choose the version for your own operating system.\nChoose “Yes” for all defaults in the installation process.\n\n\n\n\n\n\n\n\nThe free desktop version of RStudio can be launched like other applications on your computer.\nWhen you start RStudio for the first time, you will see three panes. The left pane shows you the R console. On the right, the top pane includes three tabs, while the bottom pane shows you five tabs, file, plots, packages, help, and viewer.\nYou can download a cheat sheet of the most common RStudio commands directly from RStudio by going to “Help -&gt; Cheat Sheets -&gt; RStudio IDE Cheat Sheet.”\n\n\n\n\n\n\n\n\nR was developed by statisticians and data analysts as an interactive environment for data analysis.\nSome of the advantages of R are that:\n\nit is free and open source;\nit has the capability to save scripts;\nthere are numerous resources for learning;\nit is easy for developers to share software implementation.\n\nExpressions are evaluated in the R console when you type the expression into the console and hit Return.\nA great advantage of R over point and click analysis software is that you can save your work as scripts.\n“Base R” is what you get after you first install R. Additional components are available via packages.\n\n\n\n\nIn RStudio, you can upload additional functions and datasets in addition to the base R functions and datasets that come with R automatically. A common way to do this is by installing packages, which often contain extra functions and datasets. For this course, there are a few packages you will need to install. You only need to install each individual package once, but after you install a package, there are other steps you have to do whenever you want to use something from that package.\nTo install a package, you use the code install.packages(\"package_name\", dependencies = TRUE).\nTo load a package, you use the code library(package_name).\nIf you also want to use a dataset from a package you have loaded, then you use the code data(dataset_name). To see the dataset, you can take the additional step of View(dataset_name).\n\n\n\n\n\n\nWe recommend installing packages through RStudio, rather than through R, and the code provided works in both R and RStudio. Once a package has been installed, it is technically added onto R (even if you use RStudio to install it), which is why packages must be re-installed when R is updated. However, since we use R through RStudio, any packages that are installed can be used in both R and RStudio, regardless of which one was used to install the packages.\n\n\n\n\nThe base version of R is quite minimal, but you can supplement its functions by installing additional packages.\nWe will be using tidyverse and dslabs packages for this course.\nInstall packages from R console: install.packages(\"pkg_name\")\nInstall packages from RStudio interface: Tools &gt; Install Packages (allows autocomplete)\nOnce installed, we can use library(pkg_name) to load a package each time we want to use it\n\n\n\n\n\nIf you try to load a package with library(blahblah) and get a message like Error in library(blahblah) : there is no package called ‘blahblah’, it means you need to install that package first with install.packages().\nOn the DataCamp interface we use for some problems in the course, you cannot install additional packages. The problems have been set up with the packages you need to solve them.\nYou can add the option dependencies = TRUE, which tells R to install the other things that are necessary for the package or packages to run smoothly. Otherwise, you may need to install additional packages to unlock the full functionality of a package.\nThroughout the course materials and textbook, package names are in bold.\n\n\n\n\n\ninstall.packages(\"dslabs\") # to install a single package\ninstall.packages(c(\"tidyverse\", \"dslabs\")) # to install two packages at the same time\ninstalled.packages() # to see the list of all installed packages\n\n\n\n\n\n\n\n\nRStudio has many useful features as an R editor, including the ability to test code easily as we write scripts and several auto complete features.\nKeyboard shortcuts:\n\nSave a script: Ctrl+S on Windows and Command+S on Mac\nRun an entire script: Ctrl+Shift+Enter on Windows Command+Shift+Return on Mac, or click “Source” on the editor pane\nRun a single line of script: Ctrl+Enter on Windows and Command+Return on Mac while the cursor is pointing to that line, or select the chunk and click “run”\nOpen a new script: Ctrl+Shift+N on Windows and Command+Shift+N on Mac\n\n\n\n\n\n\n# Here is an example how to running commends while editing scripts\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(murders)\n\nmurders %&gt;% \n  ggplot(aes(population, total, label=abb, color=region)) +\n  geom_label()\n\n\n\n\n\n\n\n\nTo define a variable, we may use the assignment symbol, &lt;-.\n\n(1) type the variable name into the console and hit Return;\n\n\nuse the print() function by typing print(variable_name) and hitting Return.\n\n\nObjects are things that are stored in named containers in R. They can be variables, functions, etc.\nThe ls() function shows the names of the objects saved in your work space.\n\n\n\n\n\n# assigning values to variables\na &lt;- 1\nb &lt;- 1\nc &lt;- -1\n\n# solving the quadratic equation\n(-b + sqrt(b^2 - 4*a*c))/(2*a)\n(-b - sqrt(b^2 - 4*a*c))/(2*a)\n\n\n\n\n\n\n\n\nIn general, to evaluate a function we need to use parentheses. If we type a function without parenthesis, R shows us the code for the function. Most functions also require an argument, that is, something to be written inside the parenthesis.\nTo access help files, we may use the help function, help(function_name), or write the question mark followed by the function name, ?function_name.\nThe help file shows you the arguments the function is expecting, some of which are required and some are optional. If an argument is optional, a default value is assigned with the equal sign. The args() function also shows the arguments a function needs.\nTo specify arguments, we use the equals sign. If no argument name is used, R assumes you’re entering arguments in the order shown in the help file.\nCreating and saving a script makes code much easier to execute.\nTo make your code more readable, use intuitive variable names and include comments (using the “#” symbol) to remind yourself why you wrote a particular line of code.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe code data(\"dataset_name\") and data(dataset_name) do the same thing. The code will work regardless of whether the quotes are present. It is a bit faster to leave out the quotes (as we do in the Code at the bottom of this page), so that is usually what we recommend, but it is your choice.\n\n\n\n\n\nThe function class() helps us determine the type of an object.\nData frames can be thought of as tables with rows representing observations and columns representing different variables.\nTo access data from columns of a data frame, we use the dollar sign symbol, $, which is called the accessor.\nA vector is an object consisting of several entries and can be a numeric vector, a character vector, or a logical vector.\nWe use quotes to distinguish between variable names and character strings.\nFactors are useful for storing categorical data, and are more memory efficient than storing characters.\n\n\n\n\n\n\n\nKnowledge Extension\n\n\n\n\n\n\n\n\nflowchart LR\n  A{Data Type}---&gt; B[numeric]\n  A{Data Type}---&gt;C[integer]\n  A{Data Type} ---&gt;D[complex]\n  A{Data Type}---&gt;E[character]\n  A{Data Type}---&gt;F[logical]\n\n\n\n\n\n\n\n\nExplanation:Numeric: all real numbers with or without decimal values. e.g. 1, 2, 8, 1.1.Integer(整数): specifies real values without decimal points. we use the suffixL to specify integer data.Complex: specify purely imaginary values in R. We use the suffix i to specify the imaginary part. e.g. 3 + 2i.Character:specify character or string values in a variable. '' for character variables; \"\" for string variables.Logical: is known as boolean data type. It can only have two values: TRUE and FALSE\n\n\n\n\n# loading the dslabs package and the murders dataset\nlibrary(dslabs)\ndata(murders)\n\n# determining that the murders dataset is of the \"data frame\" class\nclass(murders)\n# finding out more about the structure of the object\nstr(murders)\n# showing the first 6 lines of the dataset\nhead(murders)\n\n# using the accessor operator to obtain the population column\nmurders$population\n# displaying the variable names in the murders dataset\nnames(murders)\n# determining how many entries are in a vector\npop &lt;- murders$population\nlength(pop)\n# vectors can be of class numeric and character\nclass(pop)\nclass(murders$state)\n\n# logical vectors are either TRUE or FALSE\nz &lt;- 3 == 2\nz\nclass(z)\n\n# factors are another type of class\nclass(murders$region)\n# obtaining the levels of a factor\nlevels(murders$region)"
  },
  {
    "objectID": "posts/rbasics/index.html#installing-r",
    "href": "posts/rbasics/index.html#installing-r",
    "title": "The basics of R programming",
    "section": "",
    "text": "To complete this course, you should install R locally on your computer. We also highly recommend installing RStudio, an integrated development environment (IDE), to edit and test your code.\nIn order to complete some assignments in the course, you will need your own copy of R. You may also find it helpful to follow along with the course videos in R or RStudio.\nBoth R and RStudio can be freely downloaded and installed.\n\n\n\n\n\nYou need to install R before using RStudio, which is an interactive desktop environment.\nSelect base subdirectory in CRAN and click download.\nSelect all default choices in the installation process.\nWe recommend selecting English for language to help you better follow the course.\nYou can try using the R console, but for productivity purposes, we can switch to RStudio."
  },
  {
    "objectID": "posts/rbasics/index.html#installing-rstudio",
    "href": "posts/rbasics/index.html#installing-rstudio",
    "title": "The basics of R programming",
    "section": "",
    "text": "You can download the latest version of RStudio at the RStudio website.\nThe free desktop version is more than enough for this course.\nMake sure to choose the version for your own operating system.\nChoose “Yes” for all defaults in the installation process."
  },
  {
    "objectID": "posts/rbasics/index.html#using-rstudio-for-the-first-time",
    "href": "posts/rbasics/index.html#using-rstudio-for-the-first-time",
    "title": "The basics of R programming",
    "section": "",
    "text": "The free desktop version of RStudio can be launched like other applications on your computer.\nWhen you start RStudio for the first time, you will see three panes. The left pane shows you the R console. On the right, the top pane includes three tabs, while the bottom pane shows you five tabs, file, plots, packages, help, and viewer.\nYou can download a cheat sheet of the most common RStudio commands directly from RStudio by going to “Help -&gt; Cheat Sheets -&gt; RStudio IDE Cheat Sheet.”"
  },
  {
    "objectID": "posts/rbasics/index.html#getting-started-using-r",
    "href": "posts/rbasics/index.html#getting-started-using-r",
    "title": "The basics of R programming",
    "section": "",
    "text": "R was developed by statisticians and data analysts as an interactive environment for data analysis.\nSome of the advantages of R are that:\n\nit is free and open source;\nit has the capability to save scripts;\nthere are numerous resources for learning;\nit is easy for developers to share software implementation.\n\nExpressions are evaluated in the R console when you type the expression into the console and hit Return.\nA great advantage of R over point and click analysis software is that you can save your work as scripts.\n“Base R” is what you get after you first install R. Additional components are available via packages.\n\n\n\n\nIn RStudio, you can upload additional functions and datasets in addition to the base R functions and datasets that come with R automatically. A common way to do this is by installing packages, which often contain extra functions and datasets. For this course, there are a few packages you will need to install. You only need to install each individual package once, but after you install a package, there are other steps you have to do whenever you want to use something from that package.\nTo install a package, you use the code install.packages(\"package_name\", dependencies = TRUE).\nTo load a package, you use the code library(package_name).\nIf you also want to use a dataset from a package you have loaded, then you use the code data(dataset_name). To see the dataset, you can take the additional step of View(dataset_name)."
  },
  {
    "objectID": "posts/rbasics/index.html#installing-packahes",
    "href": "posts/rbasics/index.html#installing-packahes",
    "title": "The basics of R programming",
    "section": "",
    "text": "We recommend installing packages through RStudio, rather than through R, and the code provided works in both R and RStudio. Once a package has been installed, it is technically added onto R (even if you use RStudio to install it), which is why packages must be re-installed when R is updated. However, since we use R through RStudio, any packages that are installed can be used in both R and RStudio, regardless of which one was used to install the packages.\n\n\n\n\nThe base version of R is quite minimal, but you can supplement its functions by installing additional packages.\nWe will be using tidyverse and dslabs packages for this course.\nInstall packages from R console: install.packages(\"pkg_name\")\nInstall packages from RStudio interface: Tools &gt; Install Packages (allows autocomplete)\nOnce installed, we can use library(pkg_name) to load a package each time we want to use it\n\n\n\n\n\nIf you try to load a package with library(blahblah) and get a message like Error in library(blahblah) : there is no package called ‘blahblah’, it means you need to install that package first with install.packages().\nOn the DataCamp interface we use for some problems in the course, you cannot install additional packages. The problems have been set up with the packages you need to solve them.\nYou can add the option dependencies = TRUE, which tells R to install the other things that are necessary for the package or packages to run smoothly. Otherwise, you may need to install additional packages to unlock the full functionality of a package.\nThroughout the course materials and textbook, package names are in bold.\n\n\n\n\n\ninstall.packages(\"dslabs\") # to install a single package\ninstall.packages(c(\"tidyverse\", \"dslabs\")) # to install two packages at the same time\ninstalled.packages() # to see the list of all installed packages"
  },
  {
    "objectID": "posts/rbasics/index.html#running-commands-while-editing-scripts",
    "href": "posts/rbasics/index.html#running-commands-while-editing-scripts",
    "title": "The basics of R programming",
    "section": "",
    "text": "RStudio has many useful features as an R editor, including the ability to test code easily as we write scripts and several auto complete features.\nKeyboard shortcuts:\n\nSave a script: Ctrl+S on Windows and Command+S on Mac\nRun an entire script: Ctrl+Shift+Enter on Windows Command+Shift+Return on Mac, or click “Source” on the editor pane\nRun a single line of script: Ctrl+Enter on Windows and Command+Return on Mac while the cursor is pointing to that line, or select the chunk and click “run”\nOpen a new script: Ctrl+Shift+N on Windows and Command+Shift+N on Mac\n\n\n\n\n\n\n# Here is an example how to running commends while editing scripts\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(murders)\n\nmurders %&gt;% \n  ggplot(aes(population, total, label=abb, color=region)) +\n  geom_label()"
  },
  {
    "objectID": "posts/rbasics/index.html#r-basics",
    "href": "posts/rbasics/index.html#r-basics",
    "title": "The basics of R programming",
    "section": "",
    "text": "To define a variable, we may use the assignment symbol, &lt;-.\n\n(1) type the variable name into the console and hit Return;\n\n\nuse the print() function by typing print(variable_name) and hitting Return.\n\n\nObjects are things that are stored in named containers in R. They can be variables, functions, etc.\nThe ls() function shows the names of the objects saved in your work space.\n\n\n\n\n\n# assigning values to variables\na &lt;- 1\nb &lt;- 1\nc &lt;- -1\n\n# solving the quadratic equation\n(-b + sqrt(b^2 - 4*a*c))/(2*a)\n(-b - sqrt(b^2 - 4*a*c))/(2*a)"
  },
  {
    "objectID": "posts/rbasics/index.html#function",
    "href": "posts/rbasics/index.html#function",
    "title": "The basics of R programming",
    "section": "",
    "text": "In general, to evaluate a function we need to use parentheses. If we type a function without parenthesis, R shows us the code for the function. Most functions also require an argument, that is, something to be written inside the parenthesis.\nTo access help files, we may use the help function, help(function_name), or write the question mark followed by the function name, ?function_name.\nThe help file shows you the arguments the function is expecting, some of which are required and some are optional. If an argument is optional, a default value is assigned with the equal sign. The args() function also shows the arguments a function needs.\nTo specify arguments, we use the equals sign. If no argument name is used, R assumes you’re entering arguments in the order shown in the help file.\nCreating and saving a script makes code much easier to execute.\nTo make your code more readable, use intuitive variable names and include comments (using the “#” symbol) to remind yourself why you wrote a particular line of code."
  },
  {
    "objectID": "posts/rbasics/index.html#data-types",
    "href": "posts/rbasics/index.html#data-types",
    "title": "The basics of R programming",
    "section": "",
    "text": "Note\n\n\n\nThe code data(\"dataset_name\") and data(dataset_name) do the same thing. The code will work regardless of whether the quotes are present. It is a bit faster to leave out the quotes (as we do in the Code at the bottom of this page), so that is usually what we recommend, but it is your choice.\n\n\n\n\n\nThe function class() helps us determine the type of an object.\nData frames can be thought of as tables with rows representing observations and columns representing different variables.\nTo access data from columns of a data frame, we use the dollar sign symbol, $, which is called the accessor.\nA vector is an object consisting of several entries and can be a numeric vector, a character vector, or a logical vector.\nWe use quotes to distinguish between variable names and character strings.\nFactors are useful for storing categorical data, and are more memory efficient than storing characters.\n\n\n\n\n\n\n\nKnowledge Extension\n\n\n\n\n\n\n\n\nflowchart LR\n  A{Data Type}---&gt; B[numeric]\n  A{Data Type}---&gt;C[integer]\n  A{Data Type} ---&gt;D[complex]\n  A{Data Type}---&gt;E[character]\n  A{Data Type}---&gt;F[logical]\n\n\n\n\n\n\n\n\nExplanation:Numeric: all real numbers with or without decimal values. e.g. 1, 2, 8, 1.1.Integer(整数): specifies real values without decimal points. we use the suffixL to specify integer data.Complex: specify purely imaginary values in R. We use the suffix i to specify the imaginary part. e.g. 3 + 2i.Character:specify character or string values in a variable. '' for character variables; \"\" for string variables.Logical: is known as boolean data type. It can only have two values: TRUE and FALSE\n\n\n\n\n# loading the dslabs package and the murders dataset\nlibrary(dslabs)\ndata(murders)\n\n# determining that the murders dataset is of the \"data frame\" class\nclass(murders)\n# finding out more about the structure of the object\nstr(murders)\n# showing the first 6 lines of the dataset\nhead(murders)\n\n# using the accessor operator to obtain the population column\nmurders$population\n# displaying the variable names in the murders dataset\nnames(murders)\n# determining how many entries are in a vector\npop &lt;- murders$population\nlength(pop)\n# vectors can be of class numeric and character\nclass(pop)\nclass(murders$state)\n\n# logical vectors are either TRUE or FALSE\nz &lt;- 3 == 2\nz\nclass(z)\n\n# factors are another type of class\nclass(murders$region)\n# obtaining the levels of a factor\nlevels(murders$region)"
  },
  {
    "objectID": "posts/rbasics/index.html#vectors",
    "href": "posts/rbasics/index.html#vectors",
    "title": "The basics of R programming",
    "section": "Vectors",
    "text": "Vectors\n\nKey points\n\nThe function c(), which stands for concatenate, is useful for creating vectors.\nAnother useful function for creating vectors is the seq() function, which generates sequences.\nSubsetting lets us access specific parts of a vector by using square brackets to access elements of a vector.\n\n\n\nCode\n\n# We may create vectors of class numeric or character with the concatenate function\ncodes &lt;- c(380, 124, 818)\ncountry &lt;- c(\"italy\", \"canada\", \"egypt\")\n\n# We can also name the elements of a numeric vector\n# Note that the two lines of code below have the same result\ncodes &lt;- c(italy = 380, canada = 124, egypt = 818)\ncodes &lt;- c(\"italy\" = 380, \"canada\" = 124, \"egypt\" = 818)\n\n# We can also name the elements of a numeric vector using the names() function\ncodes &lt;- c(380, 124, 818)\ncountry &lt;- c(\"italy\",\"canada\",\"egypt\")\nnames(codes) &lt;- country\n\n# Using square brackets is useful for subsetting to access specific elements of a vector\ncodes[2]\ncodes[c(1,3)]\ncodes[1:2]\n\n# If the entries of a vector are named, they may be accessed by referring to their name\ncodes[\"canada\"]\ncodes[c(\"egypt\",\"italy\")]"
  },
  {
    "objectID": "posts/rbasics/index.html#vector-coercion",
    "href": "posts/rbasics/index.html#vector-coercion",
    "title": "The basics of R programming",
    "section": "Vector Coercion",
    "text": "Vector Coercion\n\nKey Point\n\nIn general, coercion is an attempt by R to be flexible with data types by guessing what was meant when an entry does not match the expected. For example, when defining x as\n\n\n    x &lt;- c(1, \"canada\", 3)\n\nR coerced the data into characters. It guessed that because you put a character string in the vector, you meant the 1 and 3 to actually be character strings, “1” and “3”.\n\nThe function as.character() turns numbers into characters.\nThe function as.numeric() turns characters into numbers.\nIn R, missing data is assigned the value NA.\n\n\n\nQuestion\n\nclass(3L) is integer ?\n3L-3 equals 0 ?"
  },
  {
    "objectID": "posts/rbasics/index.html#sorting",
    "href": "posts/rbasics/index.html#sorting",
    "title": "The basics of R programming",
    "section": "Sorting",
    "text": "Sorting\n\n\n\n\n\n\n\n\n\nOriginal\nSort(按从小到大排列）\nOrder(Sort对应数字在原来数字排列中的顺序）\nRank(Original原来数字在Sort顺序中的排名）\n\n\n31\n4\n2\n3\n\n\n4\n15\n3\n1\n\n\n15\n31\n1\n2\n\n\n92\n65\n5\n5\n\n\n65\n92\n4\n4\n\n\n\n\nKey Points\n\nThe function sort() sorts a vector in increasing order.\nThe function order() produces the indices needed to obtain the sorted vector, e.g. a result of 2 3 1 5 4 means the sorted vector will be produced by listing the 2nd, 3rd, 1st, 5th, and then 4th item of the original vector.\nThe function rank() gives us the ranks of the items in the original vector.\nThe function max() returns the largest value, while which.max() returns the index of the largest value. The functions min() and which.min() work similarly for minimum values.\n\n\n\nCode\n\nlibrary(dslabs)\ndata(murders)\nsort(murders$total)\n\nx &lt;- c(31, 4, 15, 92, 65)\nx\nsort(x)    # puts elements in order\n\nindex &lt;- order(x)    # returns index that will put x in order\nx[index]    # rearranging by this index puts elements in order\norder(x)\n\nmurders$state[1:10]\nmurders$abb[1:10]\n\nindex &lt;- order(murders$total)\nmurders$abb[index]    # order abbreviations by total murders\n\nmax(murders$total)    # highest number of total murders\ni_max &lt;- which.max(murders$total)    # index with highest number of murders\nmurders$state[i_max]    # state name with highest number of total murders\n\nx &lt;- c(31, 4, 15, 92, 65)\nx\nrank(x)    # returns ranks (smallest to largest)"
  },
  {
    "objectID": "posts/rbasics/index.html#vector-arithmetic",
    "href": "posts/rbasics/index.html#vector-arithmetic",
    "title": "The basics of R programming",
    "section": "Vector Arithmetic",
    "text": "Vector Arithmetic\n\nKey Point\n\nIn R, arithmetic operation on vectors occur element-wise\n\n\n\nCode\n\n# The name of the state with the maximum population is found by doing the following\nmurders$state[which.max(murders$population)]\n\n# how to obtain the murder rate\nmurder_rate &lt;- murders$total / murders$population * 100000\n\n# ordering the states by murder rate, in decreasing order\nmurders$state[order(murder_rate, decreasing=TRUE)]"
  },
  {
    "objectID": "posts/rbasics/index.html#indexing",
    "href": "posts/rbasics/index.html#indexing",
    "title": "The basics of R programming",
    "section": "Indexing",
    "text": "Indexing\n\nKey Point\n\nWe can use logicals to index vectors.\nUsing the function sum()on a logical vector returns the number of entries that are true.\nThe logical operator “&” makes two logicals true only when they are both true.\n\n\n\nCode\n\n# defining murder rate as before\nmurder_rate &lt;- murders$total / murders$population * 100000\n# creating a logical vector that specifies if the murder rate in that state is less than or equal to 0.71\nindex &lt;- murder_rate &lt;= 0.71\n# determining which states have murder rates less than or equal to 0.71\nmurders$state[index]\n# calculating how many states have a murder rate less than or equal to 0.71\nsum(index)\n\n# creating the two logical vectors representing our conditions\nwest &lt;- murders$region == \"West\"\nsafe &lt;- murder_rate &lt;= 1\n# defining an index and identifying states with both conditions true\nindex &lt;- safe & west\nmurders$state[index]"
  },
  {
    "objectID": "posts/rbasics/index.html#indexing-functions",
    "href": "posts/rbasics/index.html#indexing-functions",
    "title": "The basics of R programming",
    "section": "Indexing Functions",
    "text": "Indexing Functions\n\nKey Points\n\nThe function which() gives us the entries of a logical vector that are true.\nThe function match() looks for entries in a vector and returns the index needed to access them.\nWe use the function %in% if we want to know whether or not each element of a first vector is in a second vector.\n\n\n\nCode\n\nx &lt;- c(FALSE, TRUE, FALSE, TRUE, TRUE, FALSE)\nwhich(x)    # returns indices that are TRUE\n\n# to determine the murder rate in Massachusetts we may do the following\nindex &lt;- which(murders$state == \"Massachusetts\")\nindex\nmurder_rate[index]\n\n# to obtain the indices and subsequent murder rates of New York, Florida, Texas, we do:\nindex &lt;- match(c(\"New York\", \"Florida\", \"Texas\"), murders$state)\nindex\nmurders$state[index]\nmurder_rate[index]\n\nx &lt;- c(\"a\", \"b\", \"c\", \"d\", \"e\")\ny &lt;- c(\"a\", \"d\", \"f\")\ny %in% x\n\n# to see if Boston, Dakota, and Washington are states\nc(\"Boston\", \"Dakota\", \"Washington\") %in% murders$state"
  },
  {
    "objectID": "posts/rbasics/index.html#basic-data-wrangling",
    "href": "posts/rbasics/index.html#basic-data-wrangling",
    "title": "The basics of R programming",
    "section": "Basic Data Wrangling",
    "text": "Basic Data Wrangling\n\nKey Points\n\nTo change a data table by adding a new column, or changing an existing one, we use the mutate() function.\nTo filter the data by subsetting rows, we use the function filter().\nTo subset the data by selecting specific columns, we use the select() function.\nWe can perform a series of operations by sending the results of one function to another function using the pipe operator, %&gt;%."
  },
  {
    "objectID": "posts/rbasics/index.html#creating-data-frames",
    "href": "posts/rbasics/index.html#creating-data-frames",
    "title": "The basics of R programming",
    "section": "Creating Data Frames",
    "text": "Creating Data Frames\n\nNote\nThe default settings in R have changed as of version 4.0, and it is no longer necessary to include the code stringsAsFactors = FALSE in order to keep strings as characters. Putting the entries in quotes, as in the example, is adequate to keep strings as characters. The stringsAsFactors = FALSE code is useful in certain other situations, but you do not need to include it when you create data frames in this manner.\n\n\nKey Points\n\nWe can use the data.frame() function to create data frames.\nFormerly, the data.frame() function turned characters into factors by default. To avoid this, we could utilize the stringsAsFactors argument and set it equal to false. As of R 4.0, it is no longer necessary to include the stringsAsFactors argument, because R no longer turns characters into factors by default.\n\n\n\nCode\n\n# creating a data frame with stringAsFactors = FALSE\ngrades &lt;- data.frame(names = c(\"John\", \"Juan\", \"Jean\", \"Yao\"), \n                     exam_1 = c(95, 80, 90, 85), \n                     exam_2 = c(90, 85, 85, 90),\n                     stringsAsFactors = FALSE)"
  },
  {
    "objectID": "posts/rbasics/index.html#basic-plots",
    "href": "posts/rbasics/index.html#basic-plots",
    "title": "The basics of R programming",
    "section": "Basic Plots",
    "text": "Basic Plots\n\nKey Points\n\nWe can create a simple scatterplot using the function plot().\nHistograms are graphical summaries that give you a general overview of the types of values you have. In R, they can be produced using the hist() function.\nBoxplots provide a more compact summary of a distribution than a histogram and are more useful for comparing distributions. They can be produced using the boxplot() function.\n\n\n\nCode\n\nlibrary(dplyr)\nlibrary(dslabs)\ndata(\"murders\")\n\n\n# a simple scatterplot of total murders versus population\nx &lt;- murders$population /10^6\ny &lt;- murders$total\nplot(x, y)\n\n\n\n\n\n\n\n\n\n# a histogram of murder rates\nmurders &lt;- mutate(murders, rate = total / population * 100000)\nhist(murders$rate)\n\n\n\n\n\n\n\n# boxplots of murder rates by region\nboxplot(rate~region, data = murders)"
  },
  {
    "objectID": "posts/rbasics/index.html#the-summarize-function",
    "href": "posts/rbasics/index.html#the-summarize-function",
    "title": "The basics of R programming",
    "section": "The summarize function",
    "text": "The summarize function\n\nKey Points\n\nSummarizing data is an important part of data analysis.\nSome summary ststistics are the mean, median, and standard deviation.\nThe summarize() function from dplyr provides an easy way to compute summary statics.\n\n\n\nCode\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(dslabs)\ndata(murders)\nmurders &lt;- mutate(murders, rate = total / population * 10^5)\n\n\n# minimum, median, and maximum murder rate for the states in the West region\ns &lt;- murders %&gt;% \n  filter(region == \"West\") %&gt;%\n  summarize(minimum = min(rate), \n            median = median(rate), \n            maximum = max(rate))\ns\n\n   minimum   median  maximum\n1 0.514592 1.292453 3.629527\n\n# accessing the components with the accessor $\ns$median\n\n[1] 1.292453\n\ns$maximum\n\n[1] 3.629527\n\n# average rate unadjusted by population size\nmean(murders$rate)\n\n[1] 2.779125\n\n# average rate adjusted by population size\nus_murder_rate &lt;- murders %&gt;% \n  summarize(rate = sum(total) / sum(population) * 10^5)\nus_murder_rate\n\n      rate\n1 3.034555"
  },
  {
    "objectID": "posts/rbasics/index.html#summarizing-with-more-than-one-value",
    "href": "posts/rbasics/index.html#summarizing-with-more-than-one-value",
    "title": "The basics of R programming",
    "section": "Summarizing with more than one value",
    "text": "Summarizing with more than one value\n\nKey Points\n\nThe quantile() function can be used to return the min, median, and max in a single line of code.\n\n\n\nCode\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(dslabs)\ndata(murders)\nmurders &lt;- mutate(murders, rate = total / population * 10^5)\n\n\n# minimum, median, and maximum murder rate for the states in the West region using quantile\n# note that this returns a vector\nmurders %&gt;% \n  filter(region == \"West\") %&gt;%\n  summarize(range = quantile(rate, c(0, 0.5, 1)))\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n     range\n1 0.514592\n2 1.292453\n3 3.629527\n\n# returning minimum, median, and maximum as a data frame\nmy_quantile &lt;- function(x){\n  r &lt;-  quantile(x, c(0, 0.5, 1))\n  data.frame(minimum = r[1], median = r[2], maximum = r[3]) \n}\nmurders %&gt;% \n  filter(region == \"West\") %&gt;%\n  summarize(my_quantile(rate))\n\n   minimum   median  maximum\n1 0.514592 1.292453 3.629527"
  },
  {
    "objectID": "posts/rbasics/index.html#pull-to-access-to-columns",
    "href": "posts/rbasics/index.html#pull-to-access-to-columns",
    "title": "The basics of R programming",
    "section": "Pull to access to columns",
    "text": "Pull to access to columns\n\nKey Points\n\nThe pull() function can be used to access values stored in data when using pipes: when a data object is piped that object and its columns can be accessed using the pull() function.\n\n\n\nCode\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(dslabs)\ndata(murders)\nmurders &lt;- mutate(murders, rate = total / population * 10^5)\n\n\n# average rate adjusted by population size\nus_murder_rate &lt;- murders %&gt;% \n  summarize(rate = sum(total) / sum(population) * 10^5)\nus_murder_rate\n\n      rate\n1 3.034555\n\n# us_murder_rate is stored as a data frame\nclass(us_murder_rate)\n\n[1] \"data.frame\"\n\n# the pull function can return it as a numeric value\nus_murder_rate %&gt;% pull(rate)\n\n[1] 3.034555\n\n# using pull to save the number directly\nus_murder_rate &lt;- murders %&gt;% \n  summarize(rate = sum(total) / sum(population) * 10^5) %&gt;%\n  pull(rate)\nus_murder_rate\n\n[1] 3.034555\n\n# us_murder_rate is now stored as a number\nclass(us_murder_rate)\n\n[1] \"numeric\""
  },
  {
    "objectID": "posts/rbasics/index.html#the-dot-placeholder",
    "href": "posts/rbasics/index.html#the-dot-placeholder",
    "title": "The basics of R programming",
    "section": "The dot placeholder",
    "text": "The dot placeholder\n\nKey Points\n\nThe dot (.) can be thought of as a placeholder for the data being passed through the pipe.\n\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(dslabs)\ndata(murders)\nmurders &lt;- mutate(murders, rate = total / population * 10^5)\n\n\n# average rate adjusted by population size\nus_murder_rate &lt;- murders %&gt;% \n  summarize(rate = sum(total) / sum(population) * 10^5)\nus_murder_rate\n\n      rate\n1 3.034555\n\n# using the dot to access the rate\nus_murder_rate &lt;- murders %&gt;% \n  summarize(rate = sum(total) / sum(population) * 10^5) %&gt;%\n  .$rate\nus_murder_rate\n\n[1] 3.034555\n\nclass(us_murder_rate)\n\n[1] \"numeric\""
  },
  {
    "objectID": "posts/rbasics/index.html#group-then-summarize",
    "href": "posts/rbasics/index.html#group-then-summarize",
    "title": "The basics of R programming",
    "section": "Group then summarize",
    "text": "Group then summarize\n\nKey Points\n\nSplitting data into groups and then computing summaries for each group is a common operation in data exploration.\nWe can use the dplyr group_by() function to create a special grouped data frame to facilitate such summaries.\n\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(dslabs)\ndata(murders)\nmurders &lt;- mutate(murders, rate = total / population * 10^5)\n\n\n# group by region\nmurders %&gt;% group_by(region)\n\n# A tibble: 51 × 6\n# Groups:   region [4]\n   state                abb   region    population total  rate\n   &lt;chr&gt;                &lt;chr&gt; &lt;fct&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Alabama              AL    South        4779736   135  2.82\n 2 Alaska               AK    West          710231    19  2.68\n 3 Arizona              AZ    West         6392017   232  3.63\n 4 Arkansas             AR    South        2915918    93  3.19\n 5 California           CA    West        37253956  1257  3.37\n 6 Colorado             CO    West         5029196    65  1.29\n 7 Connecticut          CT    Northeast    3574097    97  2.71\n 8 Delaware             DE    South         897934    38  4.23\n 9 District of Columbia DC    South         601723    99 16.5 \n10 Florida              FL    South       19687653   669  3.40\n# ℹ 41 more rows\n\n# summarize after grouping\nmurders %&gt;% \n  group_by(region) %&gt;%\n  summarize(median = median(rate))\n\n# A tibble: 4 × 2\n  region        median\n  &lt;fct&gt;          &lt;dbl&gt;\n1 Northeast       1.80\n2 South           3.40\n3 North Central   1.97\n4 West            1.29"
  },
  {
    "objectID": "posts/rbasics/index.html#sorting-data-tables",
    "href": "posts/rbasics/index.html#sorting-data-tables",
    "title": "The basics of R programming",
    "section": "Sorting data tables",
    "text": "Sorting data tables\n\nKey Points\n\nTo order an entire table, we can use the dplyr function arrange().\nWe can also use nested sorting to order by additional columns.\nThe function head() returns on the first few lines of a table.\nThe function top_n() returns the top n rows of a table.\n\n\n\nCode\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(dslabs)\ndata(murders)\nmurders &lt;- mutate(murders, rate = total / population * 10^5)\n\n\n# order the states by population size\nmurders %&gt;% arrange(population) %&gt;% head()\n\n                 state abb        region population total       rate\n1              Wyoming  WY          West     563626     5  0.8871131\n2 District of Columbia  DC         South     601723    99 16.4527532\n3              Vermont  VT     Northeast     625741     2  0.3196211\n4         North Dakota  ND North Central     672591     4  0.5947151\n5               Alaska  AK          West     710231    19  2.6751860\n6         South Dakota  SD North Central     814180     8  0.9825837\n\n# order the states by murder rate - the default is ascending order\nmurders %&gt;% arrange(rate) %&gt;% head()\n\n          state abb        region population total      rate\n1       Vermont  VT     Northeast     625741     2 0.3196211\n2 New Hampshire  NH     Northeast    1316470     5 0.3798036\n3        Hawaii  HI          West    1360301     7 0.5145920\n4  North Dakota  ND North Central     672591     4 0.5947151\n5          Iowa  IA North Central    3046355    21 0.6893484\n6         Idaho  ID          West    1567582    12 0.7655102\n\n# order the states by murder rate in descending order\nmurders %&gt;% arrange(desc(rate)) %&gt;% head()\n\n                 state abb        region population total      rate\n1 District of Columbia  DC         South     601723    99 16.452753\n2            Louisiana  LA         South    4533372   351  7.742581\n3             Missouri  MO North Central    5988927   321  5.359892\n4             Maryland  MD         South    5773552   293  5.074866\n5       South Carolina  SC         South    4625364   207  4.475323\n6             Delaware  DE         South     897934    38  4.231937\n\n# order the states by region and then by murder rate within region\nmurders %&gt;% arrange(region, rate) %&gt;% head()\n\n          state abb    region population total      rate\n1       Vermont  VT Northeast     625741     2 0.3196211\n2 New Hampshire  NH Northeast    1316470     5 0.3798036\n3         Maine  ME Northeast    1328361    11 0.8280881\n4  Rhode Island  RI Northeast    1052567    16 1.5200933\n5 Massachusetts  MA Northeast    6547629   118 1.8021791\n6      New York  NY Northeast   19378102   517 2.6679599\n\n# return the top 10 states by murder rate\nmurders %&gt;% top_n(10, rate)\n\n                  state abb        region population total      rate\n1               Arizona  AZ          West    6392017   232  3.629527\n2              Delaware  DE         South     897934    38  4.231937\n3  District of Columbia  DC         South     601723    99 16.452753\n4               Georgia  GA         South    9920000   376  3.790323\n5             Louisiana  LA         South    4533372   351  7.742581\n6              Maryland  MD         South    5773552   293  5.074866\n7              Michigan  MI North Central    9883640   413  4.178622\n8           Mississippi  MS         South    2967297   120  4.044085\n9              Missouri  MO North Central    5988927   321  5.359892\n10       South Carolina  SC         South    4625364   207  4.475323\n\n# return the top 10 states ranked by murder rate, sorted by murder rate\nmurders %&gt;% arrange(desc(rate)) %&gt;% top_n(10)\n\nSelecting by rate\n\n\n                  state abb        region population total      rate\n1  District of Columbia  DC         South     601723    99 16.452753\n2             Louisiana  LA         South    4533372   351  7.742581\n3              Missouri  MO North Central    5988927   321  5.359892\n4              Maryland  MD         South    5773552   293  5.074866\n5        South Carolina  SC         South    4625364   207  4.475323\n6              Delaware  DE         South     897934    38  4.231937\n7              Michigan  MI North Central    9883640   413  4.178622\n8           Mississippi  MS         South    2967297   120  4.044085\n9               Georgia  GA         South    9920000   376  3.790323\n10              Arizona  AZ          West    6392017   232  3.629527"
  },
  {
    "objectID": "posts/rbasics/index.html#introduction-to-data.table",
    "href": "posts/rbasics/index.html#introduction-to-data.table",
    "title": "The basics of R programming",
    "section": "Introduction to data.table",
    "text": "Introduction to data.table\n\nKey Points\n\nIn this course, we often use tidyverse packages to illustrate because these packages tend to have code that is very readable for beginners.\nThere are other approaches to wrangling and analyzing data in R that are faster and better at handling large objects, such as the data.table package.\nSelecting in data.table uses notation similar to that used with matrices.\nTo add a column in data.table, you can use the := function.\nBecause the data.table package is designed to avoid wasting memory, when you make a copy of a table, it does not create a new object. The := function changes by reference. If you want to make an actual copy, you need to use the copy() function.\nSide note: the R language has a new, built-in pipe operator as of version 4.1: |&gt;. This works similarly to the pipe %&gt;% you are already familiar with. You can read more about the |&gt; pipe here External link.\n\n\n\nCode\n\n# install the data.table package before you use it!\ninstall.packages(\"data.table\")\n\n# load data.table package\nlibrary(data.table)\n\n# load other packages and datasets\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(dslabs)\ndata(murders)\n\n# convert the data frame into a data.table object\nmurders &lt;- setDT(murders)\n\n# selecting in dplyr\nselect(murders, state, region)\n\n# selecting in data.table - 2 methods\nmurders[, c(\"state\", \"region\")] |&gt; head()\nmurders[, .(state, region)] |&gt; head()\n\n# adding or changing a column in dplyr\nmurders &lt;- mutate(murders, rate = total / population * 10^5)\n\n# adding or changing a column in data.table\nmurders[, rate := total / population * 100000]\nhead(murders)\nmurders[, \":=\"(rate = total / population * 100000, rank = rank(population))]\n\n# y is referring to x and := changes by reference\nx &lt;- data.table(a = 1)\ny &lt;- x\n\nx[,a := 2]\ny\n\ny[,a := 1]\nx\n\n# use copy to make an actual copy\nx &lt;- data.table(a = 1)\ny &lt;- copy(x)\nx[,a := 2]\ny"
  },
  {
    "objectID": "posts/rbasics/index.html#subsetting-with-data.table",
    "href": "posts/rbasics/index.html#subsetting-with-data.table",
    "title": "The basics of R programming",
    "section": "Subsetting with data.table",
    "text": "Subsetting with data.table\n\nKey Points\nSubsetting in data.table uses notation similar to that used with matrices.\n\n\nCode\n\n# load packages and prepare the data\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(dslabs)\ndata(murders)\nlibrary(data.table)\nmurders &lt;- setDT(murders)\nmurders &lt;- mutate(murders, rate = total / population * 10^5)\nmurders[, rate := total / population * 100000]\n\n# subsetting in dplyr\nfilter(murders, rate &lt;= 0.7)\n\n# subsetting in data.table\nmurders[rate &lt;= 0.7]\n\n# combining filter and select in data.table\nmurders[rate &lt;= 0.7, .(state, rate)]\n\n# combining filter and select in dplyr\nmurders %&gt;% filter(rate &lt;= 0.7) %&gt;% select(state, rate)"
  },
  {
    "objectID": "posts/rbasics/index.html#summarizing-with-data.table",
    "href": "posts/rbasics/index.html#summarizing-with-data.table",
    "title": "The basics of R programming",
    "section": "Summarizing with data.table",
    "text": "Summarizing with data.table\n\nKey Points\n\nIn data.table we can call functions inside .()and they will be applied to rows.\nThe group_by followed by summarize in dplyr is performed in one line in data.table using the by argument.\n\n\n\nCode\n\n# load packages and prepare the data - heights dataset\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(dslabs)\ndata(heights)\nheights &lt;- setDT(heights)\n\n# summarizing in dplyr\ns &lt;- heights %&gt;% \n  summarize(average = mean(height), standard_deviation = sd(height))\n  \n# summarizing in data.table\ns &lt;- heights[, .(average = mean(height), standard_deviation = sd(height))]\n\n# subsetting and then summarizing in dplyr\ns &lt;- heights %&gt;% \n  filter(sex == \"Female\") %&gt;%\n  summarize(average = mean(height), standard_deviation = sd(height))\n  \n# subsetting and then summarizing in data.table\ns &lt;- heights[sex == \"Female\", .(average = mean(height), standard_deviation = sd(height))]\n\n# previously defined function\nmedian_min_max &lt;- function(x){\n  qs &lt;- quantile(x, c(0.5, 0, 1))\n  data.frame(median = qs[1], minimum = qs[2], maximum = qs[3])\n}\n\n# multiple summaries in data.table\nheights[, .(median_min_max(height))]\n\n# grouping then summarizing in data.table\nheights[, .(average = mean(height), standard_deviation = sd(height)), by = sex]"
  },
  {
    "objectID": "posts/rbasics/index.html#sorting-data-frames",
    "href": "posts/rbasics/index.html#sorting-data-frames",
    "title": "The basics of R programming",
    "section": "Sorting data frames",
    "text": "Sorting data frames\n\nKey Points\n\nTo order rows in a data frame using data.table, we can use the same approach we used for filtering.\nThe default sort is an ascending order, but we can also sort tables in descending order.\nWe can also perform nested sorting by including multiple variables in the desired sort order.\n\n\n\nCode\n\n# load packages and datasets and prepare the data\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(data.table)\nlibrary(dslabs)\ndata(murders)\nmurders &lt;- setDT(murders)\nmurders[, rate := total / population * 100000]\n\n# order by population\nmurders[order(population)] |&gt; head()\n\n# order by population in descending order\nmurders[order(population, decreasing = TRUE)] \n\n# order by region and then murder rate\nmurders[order(region, rate)]"
  },
  {
    "objectID": "posts/rbasics/index.html#programming-basics-1",
    "href": "posts/rbasics/index.html#programming-basics-1",
    "title": "The basics of R programming",
    "section": "Programming Basics",
    "text": "Programming Basics\nIntroduction to Programming in R"
  },
  {
    "objectID": "posts/rbasics/index.html#basic-condationals",
    "href": "posts/rbasics/index.html#basic-condationals",
    "title": "The basics of R programming",
    "section": "Basic Condationals",
    "text": "Basic Condationals\n\nKey Points\n\nThe most common conditional expression in programming is an if-else statement, which has the form “if [condition], perform [expression], else perform [alternative expression]”.\nThe ifelse() function works similarly to an if-else statement, but it is particularly useful since it works on vectors by examining each element of the vector and returning a corresponding answer accordingly.\nThe any() function takes a vector of logicals and returns true if any of the entries are true.\nThe all() function takes a vector of logicals and returns true if all of the entries are true.\n\n\n\nCode\n\n# an example showing the general structure of an if-else statement\na &lt;- 0\nif(a!=0){\n  print(1/a)\n} else{\n  print(\"No reciprocal for 0.\")\n}\n\n# an example that tells us which states, if any, have a murder rate less than 0.5\nlibrary(dslabs)\ndata(murders)\nmurder_rate &lt;- murders$total / murders$population*100000\nind &lt;- which.min(murder_rate)\nif(murder_rate[ind] &lt; 0.5){\n  print(murders$state[ind]) \n} else{\n  print(\"No state has murder rate that low\")\n}\n\n# changing the condition to &lt; 0.25 changes the result\nif(murder_rate[ind] &lt; 0.25){\n  print(murders$state[ind]) \n} else{\n  print(\"No state has a murder rate that low.\")\n}\n\n# the ifelse() function works similarly to an if-else conditional\na &lt;- 0\nifelse(a &gt; 0, 1/a, NA)\n\n# the ifelse() function is particularly useful on vectors\na &lt;- c(0,1,2,-4,5)\nresult &lt;- ifelse(a &gt; 0, 1/a, NA)\n\n# the ifelse() function is also helpful for replacing missing values\ndata(na_example)\nno_nas &lt;- ifelse(is.na(na_example), 0, na_example) \nsum(is.na(no_nas))\n\n# the any() and all() functions evaluate logical vectors\nz &lt;- c(TRUE, TRUE, FALSE)\nany(z)\nall(z)"
  },
  {
    "objectID": "posts/rbasics/index.html#functions",
    "href": "posts/rbasics/index.html#functions",
    "title": "The basics of R programming",
    "section": "Functions",
    "text": "Functions\n\nKey Points\n\nThe R function called function() tells R you are about to define a new function.\nFunctions are objects, so must be assigned a variable name with the arrow operator.\nThe general way to define functions is:\n\n\ndecide the function name, which will be an object,\n\n\ntype function() with your function’s arguments in parentheses, - (3) write all the operations inside brackets.\n\n\nVariables defined inside a function are not saved in the workspace.\n\n\n\nCode\n\n# example of defining a function to compute the average of a vector x\navg &lt;- function(x){\n  s &lt;- sum(x)\n  n &lt;- length(x)\n  s/n\n}\n\n# we see that the above function and the pre-built R mean() function are identical\nx &lt;- 1:100\nidentical(mean(x), avg(x))\n\n# variables inside a function are not defined in the workspace\ns &lt;- 3\navg(1:10)\ns\n\n# the general form of a function\nmy_function &lt;- function(VARIABLE_NAME){\n  perform operations on VARIABLE_NAME and calculate VALUE\n  VALUE\n}\n\n# functions can have multiple arguments as well as default values\navg &lt;- function(x, arithmetic = TRUE){\n  n &lt;- length(x)\n  ifelse(arithmetic, sum(x)/n, prod(x)^(1/n))\n}"
  },
  {
    "objectID": "posts/rbasics/index.html#for-loops",
    "href": "posts/rbasics/index.html#for-loops",
    "title": "The basics of R programming",
    "section": "For Loops",
    "text": "For Loops\n\nKey Points\n\nFor-loops perform the same task over and over while changing the variable. They let us define the range that our variable takes, and then changes the value with each loop and evaluates the expression every time inside the loop.\nThe general form of a for-loop is: “For i in [some range], do operations”. This i changes across the range of values and the operations assume i is a value you’re interested in computing on.\nAt the end of the loop, the value of i is the last value of the range.\n\n\n\nCode\n\n# creating a function that computes the sum of integers 1 through n\ncompute_s_n &lt;- function(n){\n  x &lt;- 1:n\n  sum(x)\n}\n\n# a very simple for-loop\nfor(i in 1:5){\n  print(i)\n}\n\n# a for-loop for our summation\nm &lt;- 25\ns_n &lt;- vector(length = m) # create an empty vector\nfor(n in 1:m){\n  s_n[n] &lt;- compute_s_n(n)\n}\n\n# creating a plot for our summation function\nn &lt;- 1:m\nplot(n, s_n)\n\n# a table of values comparing our function to the summation formula\nhead(data.frame(s_n = s_n, formula = n*(n+1)/2))\n\n# overlaying our function with the summation formula\nplot(n, s_n)\nlines(n, n*(n+1)/2)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Datika!!!",
    "section": "",
    "text": "Welcome to Datika!!!\nWe’are glad you’re here at our website, datika! As an avid enthusiast of statistics and the R programming language, we’ve created this platform to share our experiences, research and projects with fellows\nDatika is more than just a showcase of our work – it’s a thriving community where you can engage with the content, expand your knowledge, and connect with others in the fascinating world of statistics and R. Meet the people that make datika up and running!\n\n\nPause for a moment and Join us\n\n\n\n\n\n\n\n\nMr. Masumbuko Semba\n\nRole: Mentor\nSemba works at the Nelson Mandela African Institution of Science and Technology. He use programming language like R and Python to manage and analyse data and report using Web Applications, Website or blogs. Semba also design graphics and automate plots, maps for static and interactive document.\n lugosemba@gmail.com\n +255 717 603 703\n\n\n\n\n\n\n\nMs. Nyamisi Peter\n\nRole: Mentor\nNyamisi works at University of Dar es Salaam. She is phytoplankton exeprt using earth observation data and automate data acquisation, process, analyse. She is excellent in using quarto that combine R codes and text to automate generation of technical documents in word, pdf or html formats.\n nyamisip@gmail.com\n +255 717 516 711\n\n\n\n\n\n\nMr. Kassim Said\n\nRole: Mentee\nKassim is an accountant and financial analyst. He works as an accountant at Letshego Faidika Bank and as a financial consultant at Asasi ya Uwezeshaji Tanzania under the USAID Heshimu Bahari Project.\n kassim.salum@asuta.or.tz\n +255 743 956 226\n\n\n\nMr. Kessy Revocatus\n\nRole: Mentor\nKessy Revocatus is a mathematician current teaches at Hannah Bennie Schools School. Recognizing the role of data in driving business, he is learning R programming to automate data analysis, model and reporting.\n kessyluhegaa@gmail.com\n +255 711 396 392\n\n\n\n\n\n\nMr. Barakael Matulu\n\nRole: Mentee\nBarakael works on the USAID Heshimu Bahari Project. He is an aquatic ecologist with interests in marine resources management and climate change issues. Recently, he has also developed an interest in automating data acquisition, processing, analysis, and reporting in R, as well as graphic design.\n bmatulu@gmail.com\n +255 716 349 126\n\n\n\nMr. Emmanuel Mpina\n\nRole: Facilitator\nAgnatquo digento tatqui officae rehentor reped quibero officae consenda que nobis ini tet libus, sanda debis sitatem pelignima doluptas eos sust parchitem dolor arume cumquia si coribus voluptio ent rem qui beatateseque nonsento modicia eprat.\n\n\n\n\n\n\n\n\nMr. Ailars MIMP\n\nRole: Mentee\nAgnatquo digento tatqui officae rehentor reped quibero officae consenda que nobis ini tet libus, sanda debis sitatem pelignima doluptas eos sust parchitem dolor arume cumquia si coribus voluptio ent rem qui beatateseque nonsento modicia eprat.\n\n\n\n\n\nMr. Alex Peter\n\nRole: Mentee\nAgnatquo digento tatqui officae rehentor reped quibero officae consenda que nobis ini tet libus, sanda debis sitatem pelignima doluptas eos sust parchitem dolor arume cumquia si coribus voluptio ent rem qui beatateseque nonsento modicia eprat.\n\n\n\n\n\n\n\n\nMs. Amina Kibola\n\nRole: Mentee\nAgnatquo digento tatqui officae rehentor reped quibero officae consenda que nobis ini tet libus, sanda debis sitatem pelignima doluptas eos sust parchitem dolor arume cumquia si coribus voluptio ent rem qui beatateseque nonsento modicia eprat.\n\n\n\n\n\nMr. Juma Charles\n\nRole: Mentee\nJuma Charles works at TANESCO as land surveyor. He uses tools like Excel, ArcGIS Pro, and AutoCAD to transform spatial data into information. Currently, he’s expanding his expertise by mastering R to enhance his career capabilities.\n charlesjuma85@gmail.com\n +255 783 293 841\n\n\n\n\n\n\nMr. Benjamini Mpinga\n\nRole: Mentee\nAgnatquo digento tatqui officae rehentor reped quibero officae consenda que nobis ini tet libus, sanda debis sitatem pelignima doluptas eos sust parchitem dolor arume cumquia si coribus voluptio ent rem qui beatateseque nonsento modicia eprat.\n\n\n\n\n\nMr. Bernaid MPRU\n\nRole: Facilitator\nAgnatquo digento tatqui officae rehentor reped quibero officae consenda que nobis ini tet libus, sanda debis sitatem pelignima doluptas eos sust parchitem dolor arume cumquia si coribus voluptio ent rem qui beatateseque nonsento modicia eprat.\n\n\n\n\n\n\n\n\nDr. Deo Shirima\n\nRole: Mentee\nAgnatquo digento tatqui officae rehentor reped quibero officae consenda que nobis ini tet libus, sanda debis sitatem pelignima doluptas eos sust parchitem dolor arume cumquia si coribus voluptio ent rem qui beatateseque nonsento modicia eprat.\n\n\n\n\n\nMs. Elika Kileo\n\nRole: Mentee\nElikananyi Kileo, a medical doctor at Kibada Health Centre, has an interest in paediatrics and child health. Recently, she has also developed an interest in automating data, processing, analysis, and reporting in R\n elikananyikileo1@gmail.com\n +255 756 048 083\n\n\n\n\n\n\nDr. Elikana\n\nRole: Mentee\nAgnatquo digento tatqui officae rehentor reped quibero officae consenda que nobis ini tet libus, sanda debis sitatem pelignima doluptas eos sust parchitem dolor arume cumquia si coribus voluptio ent rem qui beatateseque nonsento modicia eprat.\n\n\n\n\n\nMs. Glady K.\n\nRole: Mentee\nAgnatquo digento tatqui officae rehentor reped quibero officae consenda que nobis ini tet libus, sanda debis sitatem pelignima doluptas eos sust parchitem dolor arume cumquia si coribus voluptio ent rem qui beatateseque nonsento modicia eprat.\n\n\n\n\n\n\n\n\nMr. Hilary Mkai\n\nRole: Mentee\nAgnatquo digento tatqui officae rehentor reped quibero officae consenda que nobis ini tet libus, sanda debis sitatem pelignima doluptas eos sust parchitem dolor arume cumquia si coribus voluptio ent rem qui beatateseque nonsento modicia eprat.\n\n\n\n\n\nMr. Humphrey Mahundi\n\nRole: Mentee\nHumphrey Mahudi, a Senior Marine Conservation Warden with MPRU’s research and monitoring department in Tanga Coelacanth Marine Park, collaborates in biodiversity assessments using environmental DNA (eDNA). He seeks to enhance his R skills to further his work and research efforts.\n xxxxxxxxxx@gmail.com\n +255 716 196 131\n\n\n\n\n\n\nMr. Kelvin Ngelo\n\nRole: Mentee\nAgnatquo digento tatqui officae rehentor reped quibero officae consenda que nobis ini tet libus, sanda debis sitatem pelignima doluptas eos sust parchitem dolor arume cumquia si coribus voluptio ent rem qui beatateseque nonsento modicia eprat.\n\n\n\n\n\nMs. Kaijage\n\nRole: Facilitator\nAgnatquo digento tatqui officae rehentor reped quibero officae consenda que nobis ini tet libus, sanda debis sitatem pelignima doluptas eos sust parchitem dolor arume cumquia si coribus voluptio ent rem qui beatateseque nonsento modicia eprat.\n\n\n\n\n\n\n\n\nMs. Kulwa Mtaki\n\nRole: Mentee\nMs. Kulwa Mtaki serves as a Marine Conservation Warden at the Marine Parks and Reserves Unit in Tanzania. Her expertise lies in fisheries and aquaculture, marine ecosystem monitoring, and data management and analysis using R-program.\n mtakikulwa@yahoo.com\n\n\n\nMs. Maria Pentzel\n\nRole: Facilitator\nAgnatquo digento tatqui officae rehentor reped quibero officae consenda que nobis ini tet libus, sanda debis sitatem pelignima doluptas eos sust parchitem dolor arume cumquia si coribus voluptio ent rem qui beatateseque nonsento modicia eprat.\n\n\n\n\n\n\n\n\nMr. Paschal Mkongola\n\nRole: Mentee\nPaschal works at Marine parks and Reserves. I have also developed an interest in automatic data, Processing, analysis and Reporting in R, as well as graphic design, career capabilities.\n paschalmkongola@gmail.com\n +255 752 918 484\n\n\n\nMr. Sam Job\n\nRole: Facilitator\nAgnatquo digento tatqui officae rehentor reped quibero officae consenda que nobis ini tet libus, sanda debis sitatem pelignima doluptas eos sust parchitem dolor arume cumquia si coribus voluptio ent rem qui beatateseque nonsento modicia eprat.\n\n\n\n\n\n\n\n\nMr.Stephano Semba\n\nRole: Mentee\nStephano Semba is employed by the Tanzania Forestry Service (TFS) as a Forestry Conservator. Understanding the importance of programming, I have dedicated myself to acquiring knowledge in data science, report writing, and spatial analysis to address the challenges in conservation.\n +255 677 250 711\n\n\n\nMr. Edward Senkondo\n\nRole: Facilitator\nAgnatquo digento tatqui officae rehentor reped quibero officae consenda que nobis ini tet libus, sanda debis sitatem pelignima doluptas eos sust parchitem dolor arume cumquia si coribus voluptio ent rem qui beatateseque nonsento modicia eprat.\n\n\n\n\n\n\n\n\nMr.Shadrack Nyanda\n\nRole: Mentee\nAgnatquo digento tatqui officae rehentor reped quibero officae consenda que nobis ini tet libus, sanda debis sitatem pelignima doluptas eos sust parchitem dolor arume cumquia si coribus voluptio ent rem qui beatateseque nonsento modicia eprat.\n\n\n\n\n\nMr. January Wegoro\n\nRole: Mentee\nAgnatquo digento tatqui officae rehentor reped quibero officae consenda que nobis ini tet libus, sanda debis sitatem pelignima doluptas eos sust parchitem dolor arume cumquia si coribus voluptio ent rem qui beatateseque nonsento modicia eprat.\n\n\n\n\n\n\n\n\nMr. Zac Maritine\n\nRole: Facilitator\nZac manages the blue economy (oceans for business) in the Western Indian Ocean for The Nature Conservancy (TNC). Before that, he worked on using maps and data for conservation across Africa at WWF. He’s an expert in planning how we use the ocean sustainably, environmental protections, and land/sea surveying.\n\n +254 721 671642\n\n\n\nMs. Edina G\n\nRole: Mentee\nAgnatquo digento tatqui officae rehentor reped quibero officae consenda que nobis ini tet libus, sanda debis sitatem pelignima doluptas eos sust parchitem dolor arume cumquia si coribus voluptio ent rem qui beatateseque nonsento modicia eprat.\n\n\n\n\n\n\n\n\nMr. James Lusana\n\nRole: Mentee\nJames Lusana, an Assistant Lecturer at the University of Dar es Salaam, is researching catfish in Lake Tanganyika for his PhD. He uses advanced imaging techniques to study their physical traits and analyzes their diet and genetics to understand their ecological role and evolution. His work aims to uncover how these fish have adapted and diversified in their environment.\n\n +255 768 310 668\n\n\n\nMs. Kally\n\nRole: Mentee\nAgnatquo digento tatqui officae rehentor reped quibero officae consenda que nobis ini tet libus, sanda debis sitatem pelignima doluptas eos sust parchitem dolor arume cumquia si coribus voluptio ent rem qui beatateseque nonsento modicia eprat.\n\n\n\n\n\n\n\n\nMr. Gabriely J. Namate\n\nRole: Mentee\nNamate is employed at Pangani District Council as a Fisheries Officer and Aquaculture Specialist. Recognizing the power of R programming in data, he decided to learn and acquired necessary skills for statistical analysis, plotting, and sharing the finding.\n +255 654 989 271\n\n\n\nMr. Samwel\n\nRole: Mentee\nAgnatquo digento tatqui officae rehentor reped quibero officae consenda que nobis ini tet libus, sanda debis sitatem pelignima doluptas eos sust parchitem dolor arume cumquia si coribus voluptio ent rem qui beatateseque nonsento modicia eprat.\n\n\n\n\n\n\n\n\nMr. Julius Olumeh\n\nRole: Mentee\nAgnatquo digento tatqui officae rehentor reped quibero officae consenda que nobis ini tet libus, sanda debis sitatem pelignima doluptas eos sust parchitem dolor arume cumquia si coribus voluptio ent rem qui beatateseque nonsento modicia eprat.\n\n\n\n\n\nMr. Ekumbi Boniphace\n\nRole: Facilitator\nEkumbi Boniphace is currently working on the project, where he is responsible for promoting schools in Tanzania. He utilizes various tools such as ArcGIS, Kobo, and Adobe Photoshop to carry out his tasks. Recently, he is expanding his skill set by learning R and Python for data processing, analysis and reporting.\n ekumbiboni@gmail.com\n +255 716 494 125\n\n\n\n\n\n\nMs Swaumu Haruna\n\nRole: Mentee\nMiss Swaumu Haruna, a graduate of Sokoine University of Agriculture (SUA) with a bachelor’s degree in environmental science and management, is currently focused on mastering R programming to enhance her skills in data analysis, statistical computation, and data visualization. Her proactive pursuit of R programming reflects her commitment to continuous learning and professional growth. With a strong background in environmental sciences and growing expertise in R, she aims to contribute to the advancement of data-driven solutions for current environmental issues.\n\n +255 627 851 120\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: The video “CERN: The Journey of Discovery”"
  }
]