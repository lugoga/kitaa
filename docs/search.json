[
  {
    "objectID": "terms.html",
    "href": "terms.html",
    "title": "DATIKA",
    "section": "",
    "text": "66666",
    "crumbs": [
      "Posts",
      "Terms"
    ]
  },
  {
    "objectID": "posts/vectorDataframe/index.html",
    "href": "posts/vectorDataframe/index.html",
    "title": "Understanding vector and dataframe",
    "section": "",
    "text": "R language is a flexible language that allows to work with different kind of data format (R Core Team, 2023). This include integer, numeric, character, complex, dates and logical. The default data type or class in R is double precision– numeric. In a nutshell, R treats all kind of data into five categories but we deal with only four in this book. Datasets in R are often a combination of seven different data types are highlighted in Figure 1;\n\n\n\n\n\n\nFigure 1: Common data types often collected and stored for anaysis and modelling"
  },
  {
    "objectID": "posts/vectorDataframe/index.html#numeric",
    "href": "posts/vectorDataframe/index.html#numeric",
    "title": "Understanding vector and dataframe",
    "section": "Numeric",
    "text": "Numeric\nThe most common data type in R is numeric. The numeric class holds the set of real numbers — decimal place numbers. We create a numeric vector using a c() function but you can use any function that creates a sequence of numbers. For example, we can create a numeric vector of SST as follows;\n\nsst = c(25.4, 26, 28, 27.8, 29, 24.8, 22.3)\n\nWe can check whether the variable sst is numeric with is.numeric function\n\nis.numeric(sst)\n\n[1] TRUE"
  },
  {
    "objectID": "posts/vectorDataframe/index.html#integer",
    "href": "posts/vectorDataframe/index.html#integer",
    "title": "Understanding vector and dataframe",
    "section": "Integer",
    "text": "Integer\nInteger vector data type is actually a special case of numeric data. Unlike numeric, integer values do not have decimal places.They are commonly used for counting or indexing. Creating an integer vector is similar to numeric vector except that we need to instruct R to treat the data as integer and not numeric or double. To command R creating integer, we specify a suffix L to an element\n\ndepth = c(5L, 10L, 15L, 20L, 25L,30L)\nis.vector(depth);class(depth)\n\n[1] TRUE\n\n\n[1] \"integer\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nif your variable does not have decimals, R will automatically set the type as integers instead of numeric.\n\n\n\naa = c(20,68,78,50)\n\nYou can check if the data is integer with is.integer() and can convert numeric value to an integer with as.integer()\n\nis.integer(aa)\n\n[1] FALSE\n\n\nYou can query the class of the object with the class() to know the class of the object\n\nclass(aa)\n\n[1] \"numeric\"\n\n\nAlthough the object bb is integer as confirmed with as.integer() function, the class() ouput the answer as numeric. This is because the defaul type of number in r is numeric. However, you can use the function as.integer() to convert numeric value to integer\n\nclass(as.integer(aa))\n\n[1] \"integer\""
  },
  {
    "objectID": "posts/vectorDataframe/index.html#character",
    "href": "posts/vectorDataframe/index.html#character",
    "title": "Understanding vector and dataframe",
    "section": "Character",
    "text": "Character\nIn programming terms, we usually call text as string. This often are text data like names. A character vector may contain a single character , a word or a group of words. The elements must be enclosed with a single or double quotations mark.\n\nsites = c(\"Pemba Channel\", \"Zanzibar Channnel\", \"Pemba Channel\")\nis.vector(sites); class(sites)\n\n[1] TRUE\n\n\n[1] \"character\"\n\n\nWe can be sure whether the object is a string with is.character() or check the class of the object with class().\n\ncountries = c(\"Kenya\", \"Uganda\", \"Rwanda\", \"Tanzania\")\nclass(countries)\n\n[1] \"character\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nEverything inside \"\" will be considered as character, no matter if it looks like character or not"
  },
  {
    "objectID": "posts/vectorDataframe/index.html#factor",
    "href": "posts/vectorDataframe/index.html#factor",
    "title": "Understanding vector and dataframe",
    "section": "Factor",
    "text": "Factor\nFactor variables are a special case of character variables in the sense that it also contains text. However, factor variables are used when there are a limited number of unique character strings. It often represents a categorical variable. For instance, the gender will usually take on only two values, \"female\" or \"male\" (and will be considered as a factor variable) whereas the name will generally have lots of possibilities (and thus will be considered as a character variable). To create a factor variable use the factor() function:\n\n    maturity.stage &lt;- factor(c(\"I\", \"II\", \"III\", \"IV\", \"V\"))\n    maturity.stage\n\n[1] I   II  III IV  V  \nLevels: I II III IV V\n\n\nTo know the different levels of a factor variable, use levels():\n\n levels(maturity.stage)\n\n[1] \"I\"   \"II\"  \"III\" \"IV\"  \"V\"  \n\n\nBy default, the levels are sorted alphabetically. You can reorder the levels with the argument levels in the factor() function:\n\nmature &lt;- factor(maturity.stage, levels = c(\"V\", \"III\"))\n    levels(mature)\n\n[1] \"V\"   \"III\"\n\n\nCharacter strings can be converted to factors with as.factor():\n\n text &lt;- c(\"test1\", \"test2\", \"test1\", \"test1\") # create a character vector\n    class(text) # to know the class\n\n[1] \"character\"\n\n\n\n text_factor &lt;- as.factor(text) # transform to factor\n    class(text_factor) # recheck the class\n\n[1] \"factor\"\n\n\nThe character strings have been transformed to factors, as shown by its class of the type factor.\nOften we wish to take a continuous numerical vector and transform it into a factor. The function cut() takes a vector of numerical data and creates a factor based on your give cut-points. Let us make a fictional total length of 508 bigeye tuna with rnorm() function.\n\ntl.cm = rnorm(n = 508, mean = 40, sd = 18)\n\n# mosaic::plotDist(dist = \"norm\", mean = 40, sd = 18, under = F, kind = \"cdf\", add = TRUE)\n\ntl.cm |&gt;\n  tibble::as.tibble() |&gt;\n  ggstatsplot::gghistostats(x = value, binwidth = 10, test.value = 40.2, type = \"n\", normal.curve = T, centrality.type = \"p\", xlab = \"Total length (cm)\")\n\n\n\n\n\n\n\nFigure 2: Normal distribution of bigeye tuna’s tota length\n\n\n\n\n\nWe can now breaks the distribution into groups and make a simple plot as shown in ?@fig-lfq, where frequency of bigeye tuna color coded with the group size\n\ngroup = cut(tl.cm, breaks = c(0,30,60,110),\n            labels = c(\"Below 20\", \"30-60\", \"Above 60\"))\nis.factor(group)\n\n[1] TRUE\n\nlevels(group)\n\n[1] \"Below 20\" \"30-60\"    \"Above 60\"\n\n\n\nbarplot(table(group), las = 1, horiz = FALSE, col = c(\"blue\", \"green\", \"red\"), ylab = \"Frequency\", xlab = \"\")\n\n\n\n\n\n\n\nFigure 3: Length frequency of bigeye tuna"
  },
  {
    "objectID": "posts/vectorDataframe/index.html#logical",
    "href": "posts/vectorDataframe/index.html#logical",
    "title": "Understanding vector and dataframe",
    "section": "Logical",
    "text": "Logical\nLogical data (or simply logical ) represent the logical TRUE state and the logical FALSE state. Logical variables are the variables in which logical data are stored. Logical variables can assume only two states:\n\nFALSE, always represent by 0;\nTRUE, always represented by a nonzero object. Usually, the digit 1 is used for TRUE.\n\nWe can create logical variables indirectly, through logical operations, such as the result of a comparison between two numbers. These operations return logical values. For example, type the following statement at the R console:\n\n5 &gt; 3;\n\n[1] TRUE\n\n5 &lt; 3\n\n[1] FALSE\n\n\nSince 5 is indeed greater than 3, the result of the comparison is true, however, 5 is not less than 3, and hence the comparison is false. The sign &gt; and &lt; are relational operators, returning logical data types as a result.\n\n value1 &lt;- 7\n    value2 &lt;- 9\n\n\n    greater &lt;- value1 &gt; value2\n    greater\n\n[1] FALSE\n\n    class(greater)\n\n[1] \"logical\"\n\n\n\n    # is value1 less than or equal to value2?\n    less &lt;- value1 &lt;= value2\n    less\n\n[1] TRUE\n\n    class(less)\n\n[1] \"logical\"\n\n\nIt is also possible to transform logical data into numeric data. After the transformation from logical to numeric with the as.numeric() command, FALSE values equal to 0 and TRUE values equal to 1:\n\n greater_num &lt;- as.numeric(greater)\n    sum(greater)\n\n[1] 0\n\n\n\n   less_num &lt;- as.numeric(less)\n    sum(less)\n\n[1] 1\n\n\nConversely, numeric data can be converted to logical data, with FALSE for all values equal to 0 and TRUE for all other values.\n\n  x &lt;- 0\n  as.logical(x)\n\n[1] FALSE\n\n\n\n y &lt;- 5\nas.logical(y)\n\n[1] TRUE"
  },
  {
    "objectID": "posts/vectorDataframe/index.html#date-and-time",
    "href": "posts/vectorDataframe/index.html#date-and-time",
    "title": "Understanding vector and dataframe",
    "section": "Date and Time",
    "text": "Date and Time\nDate and time are also treated as vector in R\n\ndate.time = seq(lubridate::dmy(010121), \n                lubridate::dmy(250121), \n                length.out = 5)\ndate.time\n\n[1] \"2021-01-01\" \"2021-01-07\" \"2021-01-13\" \"2021-01-19\" \"2021-01-25\""
  },
  {
    "objectID": "posts/vectorDataframe/index.html#sequence-numbers",
    "href": "posts/vectorDataframe/index.html#sequence-numbers",
    "title": "Understanding vector and dataframe",
    "section": "Sequence Numbers",
    "text": "Sequence Numbers\nThere are few R operators that are designed for creating vecor of non-random numbers. These functions provide multiple ways for generating sequences of numbers\nThe colon : operator, explicitly generate regular sequence of numbers between the lower and upper boundary numbers specified. For example, generating number beween 0 and 10, we simply write;\n\nvector.seq = 0:10\nvector.seq\n\n [1]  0  1  2  3  4  5  6  7  8  9 10\n\n\nHowever, if you want to generate a vector of sequence number with specified interval, let say we want to generate number between 0 and 10 with interval of 2, then the seq() function is used\n\nregular.vector = seq(from = 0,to = 10, by = 2)\nregular.vector\n\n[1]  0  2  4  6  8 10\n\n\nunlike the seq() function and : operator that works with numbers, the rep() function generate sequence of repeated numbers or strings to create a vector\n\nid = rep(x = 3, each = 4)\nstation = rep(x = \"Station1\", each = 4)\nid;station\n\n[1] 3 3 3 3\n\n\n[1] \"Station1\" \"Station1\" \"Station1\" \"Station1\""
  },
  {
    "objectID": "posts/vectorDataframe/index.html#sequence-characters",
    "href": "posts/vectorDataframe/index.html#sequence-characters",
    "title": "Understanding vector and dataframe",
    "section": "Sequence characters",
    "text": "Sequence characters\nThe rep() function allows to parse each and times arguments. The each argument allows creation of vector that that repeat each element in a vector according to specified number.\n\nsampled.months = c(\"January\", \"March\", \"May\")\nrep(x = sampled.months, each = 3)\n\n[1] \"January\" \"January\" \"January\" \"March\"   \"March\"   \"March\"   \"May\"    \n[8] \"May\"     \"May\"    \n\n\nBut the times argument repeat the whole vector to specfied times\n\nrep(x = sampled.months, times = 3)\n\n[1] \"January\" \"March\"   \"May\"     \"January\" \"March\"   \"May\"     \"January\"\n[8] \"March\"   \"May\""
  },
  {
    "objectID": "posts/vectorDataframe/index.html#generating-normal-distribution",
    "href": "posts/vectorDataframe/index.html#generating-normal-distribution",
    "title": "Understanding vector and dataframe",
    "section": "Generating normal distribution",
    "text": "Generating normal distribution\nThe central limit theorem that ensure the data is normal distributed is well known to statistician. R has a rnorm() function which makes vector of normal distributed values. For example to generate a vector of 40 sea surface temperature values from a normal distribution with a mean of 25, and standard deviation of 1.58, we simply type this expression in console;\n\nsst = rnorm(n = 40, mean = 25,sd = 1.58)\nsst\n\n [1] 23.04693 24.99349 25.68869 23.84683 25.69666 24.93500 23.44773 26.62016\n [9] 27.67181 26.30010 22.03781 25.77229 23.92286 23.35629 27.66600 28.08170\n[17] 22.16890 24.93247 24.46477 25.94592 24.50469 28.61894 21.42219 26.88232\n[25] 26.96524 22.87907 26.34715 22.76567 24.19697 25.49118 29.21119 22.55112\n[33] 23.87877 25.75880 24.54350 23.59964 22.44975 25.43948 25.33276 23.46390"
  },
  {
    "objectID": "posts/vectorDataframe/index.html#rounding-off-numbers",
    "href": "posts/vectorDataframe/index.html#rounding-off-numbers",
    "title": "Understanding vector and dataframe",
    "section": "Rounding off numbers",
    "text": "Rounding off numbers\nThere are many ways of rounding off numerical number to the nearest integers or specify the number of decimal places. the code block below illustrate the common way to round off:\n\nchl = rnorm(n = 20, mean = .55, sd = .2)\nchl |&gt; round(digits = 2)\n\n [1] 0.32 0.74 0.58 0.31 0.59 0.85 0.76 0.53 0.32 0.72 0.63 0.38 0.47 0.52 0.98\n[16] 0.77 0.87 0.73 0.48 0.53"
  },
  {
    "objectID": "posts/vectorDataframe/index.html#number-of-elements-in-a-vector",
    "href": "posts/vectorDataframe/index.html#number-of-elements-in-a-vector",
    "title": "Understanding vector and dataframe",
    "section": "Number of elements in a vector",
    "text": "Number of elements in a vector\nSometimes you may have a long vector and want to know the numbers of elements in the object. R has length() function that allows you to query the vector and print the answer\n\nlength(chl)\n\n[1] 20"
  },
  {
    "objectID": "posts/tidying/index.html",
    "href": "posts/tidying/index.html",
    "title": "Tidying Data frame",
    "section": "",
    "text": "The post Data cleaning, merging, and appending demonstrates data manipulation with a dplyr package (Wickham et al., 2019) and Basic plots with ggplot2 introduce plotting data using a grammar of graphics package ggplot2 (Wickham, 2016). However, before you manipulate and present your result in plots, your dataframe must be arranged in format that R can read and understand it. This post introduce and discuss an important concept in data management widely refereed to tidy."
  },
  {
    "objectID": "posts/tidying/index.html#introduction",
    "href": "posts/tidying/index.html#introduction",
    "title": "Tidying Data frame",
    "section": "",
    "text": "The post Data cleaning, merging, and appending demonstrates data manipulation with a dplyr package (Wickham et al., 2019) and Basic plots with ggplot2 introduce plotting data using a grammar of graphics package ggplot2 (Wickham, 2016). However, before you manipulate and present your result in plots, your dataframe must be arranged in format that R can read and understand it. This post introduce and discuss an important concept in data management widely refereed to tidy."
  },
  {
    "objectID": "posts/tidying/index.html#tidy-data",
    "href": "posts/tidying/index.html#tidy-data",
    "title": "Tidying Data frame",
    "section": "2 Tidy data",
    "text": "2 Tidy data\nThe most common mistake made is treating spreadsheet programs like lab notebooks, that is, relying on context, notes in the margin, spatial layout of data and fields to convey information. As humans, we can (usually) interpret these things, but computers don’t view information the same way, and unless we explain to the computer what every single thing means (and that can be hard!), it will not be able to see how our data fits together. Using the power of computers, we can manage and analyze data in much more effective and faster ways, but to use that power, we have to organize our data for the computer to be able to understand it.\nThis is why it’s extremely important to structure well-formatted tables from the outset - before you even start entering data from your very first preliminary experiment. Data organization is the foundation of any research project. It can make it easier or harder to work with the data throughout your analysis, so it’s worth thinking about when you’re doing your data entry or setting up your experiment. You can organize and structure data in different ways in spreadsheets, but some of these choices can limit your ability to work with the data in other programs."
  },
  {
    "objectID": "posts/tidying/index.html#what-is-tidy-data",
    "href": "posts/tidying/index.html#what-is-tidy-data",
    "title": "Tidying Data frame",
    "section": "3 What is Tidy data？",
    "text": "3 What is Tidy data？\nTidy data is a specific way of organizing data into a consistent format where each variable is a column, each row an observation, and each cell contains a single value (Figure 1). The benefits of data being tidy are myriad which include;\n\nOrganizing datasets as tidy data makes data cleaning efforts easier\nBroad range of analytical tools are built upon the assumption to consume tidy data\nSharing tidy data increases re-use\n\n\n\n\n\n\n\nFigure 1: Long and wide data organization commonly used to tidy data frames"
  },
  {
    "objectID": "posts/tidying/index.html#objectives",
    "href": "posts/tidying/index.html#objectives",
    "title": "Tidying Data frame",
    "section": "4 Objectives",
    "text": "4 Objectives\nIn this post we’ll learn some tools to help make our data tidy and more coder-friendly. Those include:\n\nUse tidyr::pivot_wider() and tidyr::pivot_longer() to reshape data frames\njanitor::clean_names() to make column headers more manageable\ntidyr::unite() and tidyr::separate() to merge or separate information from different columns\nDetect or replace a string with stringr functions\n\nLets first load the package we are going to use in this session\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(readxl)\n\nThen we use read_csv function to load a file called fao_capture.csvv from our working directory and assign as fao.capture.wt.\n\nfao.capture.wt = read_csv(\"../data/tidy/fao_capture.csv\")\n\nWe’ve used read_csv() to import data from spreadsheets into R. Before we go further with the imported dataset in (fao-capture?), we need to check whether the fao.capture.wt dataset we have just loaded into our session is tidy. There are three assumptions that makes a dataset tidy, which are;\n\nEach variable is its its own column\nEach records is its own row, and\nEach value is in its own cell\n\n\n\n\n\nTable 1: Capture fisheries since 2012. Source FAO\n\n\n\nASFIS species20122013201420152016201720182019Marine fishes nei-Osteichthyes137 683112 095110 535118 078118 204124 895109 938415 031Gulf menhaden-Brevoortia patronus500 162440 709385 022539 198618 563461 189529 231336 221Round sardinella-Sardinella aurita42 34446 29950 69879 851123 377126 766126 400126 400American cupped oyster-Crassostrea virginica77 90053 86657 93055 04144 31378 96149 63147 723Ark clams nei-Arca spp12 68715 00017 38429 07329 07629 97029 97029 970Stromboid conchs nei-Strombus spp36 85536 61030 69134 38828 77435 27334 85628 308Blue crab-Callinectes sapidus43 99740 57044 36150 78153 61654 62856 68925 937Yellowfin tuna-Thunnus albacares19 08721 86525 83127 0363448729 31030 96624 503Caribbean spiny lobster-Panulirus argus31 10228 46828 29831 35832 41030 20428 32524 311Atlantic seabob-Xiphopenaeus kroyeri36 07334 24325 20325 32230 14731 31931 62522 497Mexican four-eyed octopus-Octopus maya12 6298 80615 40323 44125 72226 0202880920 119\n\n\n\n\n\nA careful check of fao.capture.wt dataset presented in Table 1 and adhering to tidy assumptions, a fair question would be, is this dataset tidy? The answer is BIG NO. This is not tidy data but rather a MESSY data. We are going to look why we have given that answer that our fao.capture.wt dataset is messy!"
  },
  {
    "objectID": "posts/tidying/index.html#reshape-from-wide-to-long-format",
    "href": "posts/tidying/index.html#reshape-from-wide-to-long-format",
    "title": "Tidying Data frame",
    "section": "5 Reshape from wide-to-long format",
    "text": "5 Reshape from wide-to-long format\nAlthough looking Table 1 our eyes can visually see the way the data is structured, but computers don’t view information the same way, we need to organize the data that computer can understand. If we look Table 1, we can see that the year variable is actually split over eight columns, so we’d say this is currently in wide format (?@fig-fide-long). There may be times when you want to have data in wide format, but often with code it is more efficient to convert to long format by gathering together observations for a variable that is currently split into multiple columns. Schematically, converting from wide to long format using pivot_longer() looks like this:\n{# fig-fide-long}\nWe’ll use pivot_longer() function from dityr package (Wickham and Henry (2018)) to gather data from all years in inverts (columns 2012 to 2019) into two columns: one called year, which contains the year, another called weight containing the weight of each fish species landed and assign a name of new dataset as fao.capture.long.\nThe new data frame will be stored as inverts_long:\n\nfao.capture.long = fao.capture.wt |&gt;\n  pivot_longer(\n    cols = '2012':'2019',\n    values_to = \"wt_mt\",\n    names_to = \"year\"\n    )\n\nThe outcome is the new long-format fao.capture.long data frame:\n\nfao.capture.long |&gt; \n  sample_n(size = 15) |&gt; \n  flextable::flextable() |&gt; \n  flextable::autofit()\n\nASFIS speciesyearwt_mtGulf menhaden-Brevoortia patronus2014385 022Marine fishes nei-Osteichthyes2017124 895Stromboid conchs nei-Strombus spp201534 388Gulf menhaden-Brevoortia patronus2017461 189Blue crab-Callinectes sapidus201856 689Atlantic seabob-Xiphopenaeus kroyeri201236 073American cupped oyster-Crassostrea virginica201457 930Mexican four-eyed octopus-Octopus maya201726 020American cupped oyster-Crassostrea virginica201555 041Round sardinella-Sardinella aurita201242 344Marine fishes nei-Osteichthyes2013112 095Gulf menhaden-Brevoortia patronus2012500 162Caribbean spiny lobster-Panulirus argus201428 298American cupped oyster-Crassostrea virginica201353 866Yellowfin tuna-Thunnus albacares201830 966\n\n\nlet’s use a glimpse function from dplyr package to look at the internal structure of the of the fao.capture.long format, we just created. One thing that isn’t obvious at first (but would become obvious if you continued working with this data) is that since those year numbers were initially column names (characters), when they are stacked into the year column, their class wasn’t auto-updated to numeric. Similar way, the weight are separated with a space and are treated as character instead of being numeric.\n\nfao.capture.long |&gt;\n  glimpse()\n\nRows: 88\nColumns: 3\n$ `ASFIS species` &lt;chr&gt; \"Marine fishes nei-Osteichthyes\", \"Marine fishes nei-O…\n$ year            &lt;chr&gt; \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\"…\n$ wt_mt           &lt;chr&gt; \"137 683\", \"112 095\", \"110 535\", \"118 078\", \"118 204\",…\n\n\nThat’s a good thing! We don’t want R to update classes of our data without our instruction. We’ll use dplyr::mutate() in a different way here: to create a new column (that’s how we’ve used mutate() in ?@sec-dplyr that has the same name of an existing column, in order to update and overwrite the existing column. In this case, we’ll mutate() to add a column called year, which contains an as.integer() version of the existing year variable:\n\nfao.capture.long = fao.capture.long |&gt;\n  mutate(year = as.integer(year))\n\nChecking the class again, we see that year has been updated to a numeric variable:\n\nfao.capture.long |&gt;\n  glimpse()\n\nRows: 88\nColumns: 3\n$ `ASFIS species` &lt;chr&gt; \"Marine fishes nei-Osteichthyes\", \"Marine fishes nei-O…\n$ year            &lt;int&gt; 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2012, …\n$ wt_mt           &lt;chr&gt; \"137 683\", \"112 095\", \"110 535\", \"118 078\", \"118 204\",…"
  },
  {
    "objectID": "posts/tidying/index.html#reshape-from-long-to-wide-format",
    "href": "posts/tidying/index.html#reshape-from-long-to-wide-format",
    "title": "Tidying Data frame",
    "section": "6 Reshape from long-to-wide format",
    "text": "6 Reshape from long-to-wide format\nIn the previous example, we had information spread over multiple columns that we wanted to gather. Sometimes, we’ll have data that we want to spread over multiple columns. For example, imagine that starting from inverts_long we want each species in the common_name column to exist as its own column. In that case, we would be converting from a longer to a wider format, and will use tidyr::pivot_wider(). Specifically for our data, we’ll use pivot_wider() to spread the common_name across multiple columns as follows:\n\nfao.capture.wide = fao.capture.long |&gt;\n  pivot_wider(\n    names_from = \"ASFIS species\",\n    values_from = \"wt_mt\"\n    )\n\n\nfao.capture.wide |&gt; \n  flextable::flextable() |&gt; \n  flextable::autofit()\n\nyearMarine fishes nei-OsteichthyesGulf menhaden-Brevoortia patronusRound sardinella-Sardinella auritaAmerican cupped oyster-Crassostrea virginicaArk clams nei-Arca sppStromboid conchs nei-Strombus sppBlue crab-Callinectes sapidusYellowfin tuna-Thunnus albacaresCaribbean spiny lobster-Panulirus argusAtlantic seabob-Xiphopenaeus kroyeriMexican four-eyed octopus-Octopus maya2,012137 683500 16242 34477 90012 68736 85543 99719 08731 10236 07312 6292,013112 095440 70946 29953 86615 00036 61040 57021 86528 46834 2438 8062,014110 535385 02250 69857 93017 38430 69144 36125 83128 29825 20315 4032,015118 078539 19879 85155 04129 07334 38850 78127 03631 35825 32223 4412,016118 204618 563123 37744 31329 07628 77453 6163448732 41030 14725 7222,017124 895461 189126 76678 96129 97035 27354 62829 31030 20431 31926 0202,018109 938529 231126 40049 63129 97034 85656 68930 96628 32531 625288092,019415 031336 221126 40047 72329 97028 30825 93724 50324 31122 49720 119\n\n\nWe can see that now each species has its own column (wider format). But also notice that those column headers (since they have spaces) might not be in the most coder-friendly format…"
  },
  {
    "objectID": "posts/tidying/index.html#clean-variable-column-names",
    "href": "posts/tidying/index.html#clean-variable-column-names",
    "title": "Tidying Data frame",
    "section": "7 Clean variable (column) names",
    "text": "7 Clean variable (column) names\nThe janitor package by Sam Firke is a great collection of functions for some quick data cleaning, like:\n\njanitor::clean_names(): update column headers to a case of your choosing\njanitor::get_dupes(): see all rows that are duplicates within variables you choose\njanitor::remove_empty(): remove empty rows and/or columns\njanitor::adorn_*(): jazz up tables\n\nHere, we’ll use janitor::clean_names() to convert all of our column headers to a more convenient case - the default is lower_snake_case, which means all spaces and symbols are replaced with an underscore (or a word describing the symbol), all characters are lowercase, and a few other nice adjustments. For example, janitor::clean_names() update ASFIS species variable name into much nicer asfis_species.\n\nfao.capture.long = fao.capture.long |&gt;\n  janitor::clean_names()\n\nfao.capture.long |&gt; \n  sample_n(size = 10) |&gt; \n  flextable::flextable() |&gt; \n  flextable::autofit()\n\nasfis_speciesyearwt_mtArk clams nei-Arca spp2,01529 073Stromboid conchs nei-Strombus spp2,01534 388Ark clams nei-Arca spp2,01212 687Round sardinella-Sardinella aurita2,01242 344Yellowfin tuna-Thunnus albacares2,01729 310Marine fishes nei-Osteichthyes2,019415 031Blue crab-Callinectes sapidus2,01653 616Atlantic seabob-Xiphopenaeus kroyeri2,01334 243Gulf menhaden-Brevoortia patronus2,015539 198Marine fishes nei-Osteichthyes2,012137 683"
  },
  {
    "objectID": "posts/tidying/index.html#separate-merged-variables",
    "href": "posts/tidying/index.html#separate-merged-variables",
    "title": "Tidying Data frame",
    "section": "8 Separate merged variables",
    "text": "8 Separate merged variables\nSometimes we’ll want to separate contents of a single column into multiple columns, or combine entries from different columns into a single column. For example, the fao.capture.long has a asfis_species variable, which combine an english and scientific name of the particular fish group. That variable should be separated into two variables, one variable we will call english_name and the other species. tidyr package has a function called separate that is dedicated to divide variables that are combined. For our case, a asfis_species variable can be separate with a hyphen, which was used to distinguish between an english and species names. The separate() function accepts arguments for the name of the variable to separate. You also need to specify the names of the variable to separate into, and an optional separator.\n\nfao.capture.long.sep = fao.capture.long %&gt;% \n  separate(\n    col = asfis_species, \n    into = c(\"english_name\", \"species\"),\n    sep = \"-\"\n    )\nfao.capture.long.sep|&gt; \n  sample_n(size = 10) |&gt; \n  flextable::flextable() |&gt; \n  flextable::autofit()\n\nenglish_namespeciesyearwt_mtGulf menhadenBrevoortia patronus2,015539 198Ark clams neiArca spp2,01829 970Mexican foureyed octopus2,0138 806American cupped oysterCrassostrea virginica2,01644 313Yellowfin tunaThunnus albacares2,01830 966Ark clams neiArca spp2,01315 000Ark clams neiArca spp2,01212 687Marine fishes neiOsteichthyes2,012137 683Yellowfin tunaThunnus albacares2,01321 865Yellowfin tunaThunnus albacares2,01219 087"
  },
  {
    "objectID": "posts/tidying/index.html#merge-separate-variables",
    "href": "posts/tidying/index.html#merge-separate-variables",
    "title": "Tidying Data frame",
    "section": "9 Merge separate variables",
    "text": "9 Merge separate variables\nThe unite() function is the exact opposite of separate() in that it combines multiple columns into a single column. While not used nearly as often as separate() , there may be times when you need the functionality provided by unite(). For example, we use tidyr::unite() to combine the variable english_name and english_name to form common_species, and separate them with a hyphen - symbol between the two variables and assign the output as fao.capture.long.merge.\n\nfao.capture.long.merge = fao.capture.long.sep |&gt;\n  unite(\n    col = \"common_species\", # What to name the new united column\n    c(english_name,  species), # The columns we'll unite (site, year)\n    sep = \"-\" # How to separate the things we're uniting\n    )\n\nfao.capture.long.merge|&gt; \n  sample_n(size = 10) |&gt; \n  flextable::flextable() |&gt; \n  flextable::autofit()\n\ncommon_speciesyearwt_mtGulf menhaden-Brevoortia patronus2,013440 709Atlantic seabob-Xiphopenaeus kroyeri2,01630 147Ark clams nei-Arca spp2,01417 384Blue crab-Callinectes sapidus2,01856 689Caribbean spiny lobster-Panulirus argus2,01730 204Marine fishes nei-Osteichthyes2,016118 204Gulf menhaden-Brevoortia patronus2,015539 198Stromboid conchs nei-Strombus spp2,01430 691Blue crab-Callinectes sapidus2,01653 616Yellowfin tuna-Thunnus albacares2,01729 310"
  },
  {
    "objectID": "posts/tidying/index.html#replace-a-pattern",
    "href": "posts/tidying/index.html#replace-a-pattern",
    "title": "Tidying Data frame",
    "section": "10 Replace a pattern",
    "text": "10 Replace a pattern\nOne thing we noticed in the dataset we imported is that the weight are not in numeric but rather as character. The reason for that is simple because for eye to read long numbers easily, we often separate a thousand with comma or space, which is the case for the data we loaded. though the eye can easily see and distinguish the numbers, that is hard computers can understand, as a key issue in tidying is that a cell should contain a single value. Therefore, since this weigh is in character format, there is nothing we can do to get any statistic values unless we convert them into numeric format. That’s simple and straight forward as R has a as.numeric function that convert character into numeric format values. Unfortunate, we do not get the numeric as we expected but instead we are given an NA values, which simply indicate that process isn’t fit for that value of the data type.\n\nfao.capture.long |&gt;\n  mutate(wt_mt = as.numeric(wt_mt))|&gt; \n  sample_n(size = 10) |&gt; \n  flextable::flextable() |&gt; \n  flextable::autofit()\n\nasfis_speciesyearwt_mtArk clams nei-Arca spp2,016Stromboid conchs nei-Strombus spp2,016Yellowfin tuna-Thunnus albacares2,015Marine fishes nei-Osteichthyes2,016Caribbean spiny lobster-Panulirus argus2,019Round sardinella-Sardinella aurita2,017Mexican four-eyed octopus-Octopus maya2,01828,809Marine fishes nei-Osteichthyes2,018Round sardinella-Sardinella aurita2,019Stromboid conchs nei-Strombus spp2,014\n\n\nWhen you face that situation, you must keen look at the internal structure of the dataset to understand what lies behind the problem you encounter. Using a glimpse function, we notice the wt_mt variable is character format with number values, but why does the values fail to convert to numeric the answer is simple–presence of space to separate the values into thousands. We must get rid of the space before we convert the character into numeric values.\n\nfao.capture.long |&gt;\n  glimpse()\n\nRows: 88\nColumns: 3\n$ asfis_species &lt;chr&gt; \"Marine fishes nei-Osteichthyes\", \"Marine fishes nei-Ost…\n$ year          &lt;int&gt; 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2012, 20…\n$ wt_mt         &lt;chr&gt; \"137 683\", \"112 095\", \"110 535\", \"118 078\", \"118 204\", \"…\n\n\nA stringr package has several function that works well with strings values in R. Among these function is str_remove(), which remove a specified string pattern. In our case we remove a space in wt_mt variable and assign the variable name as wt_mt_free\n\nfao.capture.long.free = fao.capture.long |&gt;\n  mutate(\n    wt_mt_free = str_remove(string = wt_mt, \n                             pattern = \" \")\n    )\n\nA glimpse of the fao.capture.long.free indicate that a wt_mt_free variable we just created has no spaces to separate values into thousands.\n\n\nRows: 88\nColumns: 4\n$ asfis_species &lt;chr&gt; \"Marine fishes nei-Osteichthyes\", \"Marine fishes nei-Ost…\n$ year          &lt;int&gt; 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2012, 20…\n$ wt_mt         &lt;chr&gt; \"137 683\", \"112 095\", \"110 535\", \"118 078\", \"118 204\", \"…\n$ wt_mt_free    &lt;chr&gt; \"137683\", \"112095\", \"110535\", \"118078\", \"118204\", \"12489…\n\n\nTherefore, once we have removed space in the values, the variable is in the format the R will understand that this is values and can easily convert from character into numeric with as.numeric function;\n\nfao.capture.long.free = fao.capture.long.free |&gt;\n  mutate(wt_mt = as.numeric(wt_mt_free)) |&gt;\n  select(-wt_mt_free)\n\nfao.capture.long.free|&gt; \n  sample_n(size = 10) |&gt; \n  flextable::flextable() |&gt; \n  flextable::autofit()\n\nasfis_speciesyearwt_mtMexican four-eyed octopus-Octopus maya2,01726,020Ark clams nei-Arca spp2,01212,687Atlantic seabob-Xiphopenaeus kroyeri2,01922,497American cupped oyster-Crassostrea virginica2,01849,631Atlantic seabob-Xiphopenaeus kroyeri2,01630,147Caribbean spiny lobster-Panulirus argus2,01428,298Caribbean spiny lobster-Panulirus argus2,01828,325Caribbean spiny lobster-Panulirus argus2,01730,204Yellowfin tuna-Thunnus albacares2,01924,503Marine fishes nei-Osteichthyes2,018109,938"
  },
  {
    "objectID": "posts/tidying/index.html#summary",
    "href": "posts/tidying/index.html#summary",
    "title": "Tidying Data frame",
    "section": "11 summary",
    "text": "11 summary\nThe ultimate of tidy dataset is in the analysis and plotting. When the dataset is in the right format, it makes advanced analysis easy and smooth while exploring the data and expose the inherited pattern is also much easier. This is because the tools in many programming languages and database require tidy structure. For example, Figure 2 is the result of tidy dataset that allows to compute the mean and standard deviation and then use the metric computed to make a plot. We will cover ploting in much detail in ?@sec-ggplot.\n\n##| echo: true\n#| \n\nfao.capture.long.free %&gt;% \n  separate(col = asfis_species, \n           into = c(\"name\", \"species\"), \n           sep = \"-\") %&gt;% \n  group_by(species) %&gt;% \n  summarise(\n    mean_wt = mean(wt_mt, na.rm = TRUE),\n    std = sd(wt_mt, na.rm = TRUE)\n    ) %&gt;% \n  arrange(mean_wt) %&gt;% \n  ggplot(aes(x = reorder(species, mean_wt), y = mean_wt))+\n  geom_point(size = 3)+\n  geom_errorbar(aes(ymin = mean_wt+std, \n                    ymax = mean_wt - std), \n                width = .5)+\n  scale_y_continuous(name = \"Landed Weight (MT)\", \n                     position = \"right\", \n                     breaks = scales::pretty_breaks(n = 5),\n                     labels = scales::label_number(big.mark = \",\"))+\n  scale_x_discrete(name = \"Species\")+\n  coord_flip() +\n  theme_bw(base_size = 12)+\n  theme(panel.grid = element_blank(), axis.text.y = element_text(face = \"italic\"))\n\n\n\n\n\n\n\nFigure 2: Mean weight of selected species from Capture fisheries"
  },
  {
    "objectID": "posts/stats0/index.html",
    "href": "posts/stats0/index.html",
    "title": "Understanding Descriptive Statistics",
    "section": "",
    "text": "In the world of statistics, there are three main types that are commonly used to analyze and interpret data: descriptive, inferential, and Bayesian. Each type serves a specific purpose and provides valuable insights into different aspects of the data.\nWhile descriptive statistics provides valuable insights into the basic features of a dataset (Figure 1), inferential statistics takes it a step further by making inferences or predictions about a population based on a sample. This type of statistics involves hypothesis testing, confidence intervals, and regression analysis, among other techniques.\nBayesian statistics, on the other hand, provides a framework for updating beliefs about parameters or hypotheses based on new evidence or data. It differs from traditional frequentist statistics in that it incorporates prior knowledge or beliefs about the parameters being estimated.\n\n\n\n\n\n\nFigure 1: The common metrics of descriptive statistics\n\n\n\nIn future sessions, we will delve deeper into inferential and Bayesian statistics to explore their applications and implications in data analysis. In this blog post, we will focus on descriptive statistics (Figure 1) and explore the central tendency theorem, as well as the measures of center and dispersion."
  },
  {
    "objectID": "posts/stats0/index.html#introduction",
    "href": "posts/stats0/index.html#introduction",
    "title": "Understanding Descriptive Statistics",
    "section": "",
    "text": "In the world of statistics, there are three main types that are commonly used to analyze and interpret data: descriptive, inferential, and Bayesian. Each type serves a specific purpose and provides valuable insights into different aspects of the data.\nWhile descriptive statistics provides valuable insights into the basic features of a dataset (Figure 1), inferential statistics takes it a step further by making inferences or predictions about a population based on a sample. This type of statistics involves hypothesis testing, confidence intervals, and regression analysis, among other techniques.\nBayesian statistics, on the other hand, provides a framework for updating beliefs about parameters or hypotheses based on new evidence or data. It differs from traditional frequentist statistics in that it incorporates prior knowledge or beliefs about the parameters being estimated.\n\n\n\n\n\n\nFigure 1: The common metrics of descriptive statistics\n\n\n\nIn future sessions, we will delve deeper into inferential and Bayesian statistics to explore their applications and implications in data analysis. In this blog post, we will focus on descriptive statistics (Figure 1) and explore the central tendency theorem, as well as the measures of center and dispersion."
  },
  {
    "objectID": "posts/stats0/index.html#descriptive-statistics",
    "href": "posts/stats0/index.html#descriptive-statistics",
    "title": "Understanding Descriptive Statistics",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nDescriptive statistics is a branch of statistics focused on summarizing, organizing, and presenting data in a clear and understandable way. Its primary aim is to define and analyze the fundamental characteristics of a dataset without making sweeping generalizations or assumptions about the entire data set.\nThe main purpose of descriptive statistics is to provide a straightforward and concise overview of the data, enabling researchers or analysts to gain insights and understand patterns, trends, and distributions within the dataset.\nDescriptive statistics typically involve measures of central tendency (such as mean, median, mode), dispersion (such as range, variance, standard deviation), and distribution shape (including skewness and kurtosis). Additionally, graphical representations like charts, graphs, and tables are commonly used to visualize and interpret the data.\nHistograms, bar charts, pie charts, scatter plots, and box plots are some examples of widely used graphical techniques in descriptive statistics."
  },
  {
    "objectID": "posts/stats0/index.html#types-of-descriptive-statistics",
    "href": "posts/stats0/index.html#types-of-descriptive-statistics",
    "title": "Understanding Descriptive Statistics",
    "section": "Types of Descriptive Statistics",
    "text": "Types of Descriptive Statistics\nThere are two types of descriptive statistics:\n\nMeasures of Central Tendency\nMeasures of Dispersion\n\n\nMeasures of Central Tendency\nThe central tendency is defined as a statistical measure that may be used to describe a complete distribution or dataset with a single value, known as a measure of central tendency. Any of the central tendency measures accurately describes the whole data distribution. In the following sections, we will look at the central tendency measures, their formulae, applications, and kinds in depth.\n\n\n\n\n\n\n\n\nFigure 2: The location of the mean value of the normal distribution\n\n\n\n\n\n\nMean\nMean is the sum of all the components in a group or collection divided by the number of items in that group or collection. Mean of a data collection is typically represented as x̄ (pronounced “x bar”). The formula for calculating the mean for ungrouped data to express it as the measure is given as follows:\nFor a series of observations:\n\\[\n\\bar x = \\sum \\frac{x}{n}\n\\tag{1}\\]\nWhere,\n\n\\(\\bar x\\) = Mean values of the vector\n\\(\\sum x\\) Sum of all terms\n\\(n\\) number of observations\n\n\nExample 1 Weights of 7 girls in kg are 54, 32, 45, 61, 20, 66 and 50. create a vector object from these values and compute the mean of the data.\n\n\nSolution 1. \n\nweights = c(54, 32, 45, 61, 20, 66, 50)\n\nweights |&gt; mean()\n\n[1] 46.85714\n\n\n\n\n\nMedian\nMedian of a data set is the value of the middle-most observation obtained after organizing the data in ascending order, which is one of the measures of central tendency. Median formula may be used to compute the median for many types of data, such as grouped and ungrouped data.\n\nExample 2 Weights of 7 girls in kg are 54, 32, 45, 61, 20, 66 and 50. crreate a vector object from these values and compute the median of the data.\n\n\nSolution 2. \n\nweights = c(54, 32, 45, 61, 20, 66, 50)\nweights |&gt; median()\n\n[1] 50\n\n\n\n\n\nMode\nMode is one of the measures of central tendency, defined as the value that appears the most frequently in the provided data, i.e. the observation with the highest frequency is known as the mode of data. The mode formulae provided below can be used to compute the mode for ungrouped data.\n\nExample 3 Weights of 7 girls in kg are 54, 32, 45, 61, 20, 66 and 50. create a vector object from these values and compute the mode of these data.\n\n\nSolution 3. \n\n#|eval: false\nweights = c(54, 32, 45, 61, 20, 55, 50)\n\n\nweights |&gt; mode()\n\n[1] \"numeric\"\n\n\n\n\n\n\nMeasure of Dispersion\nAnother important property of a distribution is the dispersion. Some of the parameters that can be used to quantify dispersion are illustrated in Figure 3.\n\n\n\n\n\n\n\n\nFigure 3: The location of the standard deviation values of the normal distribution\n\n\n\n\n\nIf the variability of data within an experiment must be established, absolute measures of variability should be employed. These metrics often reflect differences in a data collection in terms of the average deviations of the observations. The most prevalent absolute measurements of deviation are mentioned below. In the following sections, we will look at the variability measures, their formulae in depth.\n\nRange\nStandard Deviation\nVariance\n\n\n\nRange\nThe range represents the spread of your data from the lowest to the highest value in the distribution. It is the most straightforward measure of variability to compute. To get the range, subtract the data set’s lowest and highest values.\nRange = Highest Value – Lowest Value\n\nExample 4 Calculate the range of the following data series: 5, 13, 32, 42, 15, 84\n\n\nSolution 4. \n\n#|eval: false\nweights = c(5, 13, 32, 42, 15, 84)\n\n\nweights |&gt; range()\n\n[1]  5 84\n\n\n\n\n\nStandard Deviation\nStandard deviation (s or SD) represents the average level of variability in your dataset. It represents the average deviation of each score from the mean. The higher the standard deviation, the more varied the dataset is.\nTo calculate standard deviation, follow these six steps:\n\nMake a list of each score and calculate the mean.\nCalculate deviation from the mean, by subtracting the mean from each score.\nSquare each of these differences.\nSum up all squared variances.\nDivide the total of squared variances by N-1.\n\n1.Find the square root of the number that you discovered.\n\nExample 5 Calculate standard deviation of the following data series: 5, 13, 32, 42, 15, 84.\n\n\nSolution 5. \n\n#|eval: false\nweights = c(5, 13, 32, 42, 15, 84)\n\n\nweights |&gt; sd()\n\n[1] 28.92346\n\n\n\n\n\nVariance\nVariance is calculated as average of squared departures from the mean. Variance measures the degree of dispersion in a data collection. The more scattered the data, the larger the variance in relation to the mean. To calculate the variance, square the standard deviation.\n\nExample 6 Calculate standard deviation of the following data series: 5, 13, 32, 42, 15, 84.\n\n\nSolution 6. \n\n#|eval: false\nweights = c(5, 13, 32, 42, 15, 84)\n\n\nweights |&gt; sd()\n\n[1] 28.92346\n\n\n\n\n\nShape of the distribution\n\nSkewness\nSkewness is a measure of the asymmetry of the tails of a distribution. A negative skew indicates that the distribution is spread out more to the left of the mean value, assuming values increasing towards the right along the axis. Th e sample mean is in this case smaller than the mode. Distributions with positive skewness have large tails that extend towards the right. Th e skewness of the symmetric normal distribution is zero.\nThe most popular way to compute the asymmetry of a distribution is by Pearson’s mode skewness:\n\\[\nskewness = \\frac{mean - mode}{standar deviation}\n\\] Although Pearson’s measure is a useful one, a Fisher formula is often used, which is defined as;\n\\[\nskewness = \\sum_{i-1}^N \\frac{(x - \\bar x)}{s^3}^2\n\\tag{2}\\]\n\n\nKurtosis\nThe second important measure for the sape of the distribution is the kurtosis. The kurtosis is a measure of whether the data are peaked or flat relative to a normal distribution. A normal distribution has a kurtosis of three. A high kurtosis indicates that the distribution has a distinct peak near the mean, whereas a distribution characterized by a low kurtosis shows a flat top near the mean and broad tails. Higher peakedness in a distribution results from rare extreme deviations, whereas a low kurtosis is caused by frequent moderate deviations. Kurtosis formula is defined as :\n\\[\nkurtosis = \\sum \\frac{(x_i - \\bar x)^4}{s^4}\n\\tag{3}\\]\n\n\n\nExample of distribution\nR has several functions to numerically summarize variables. These include the capability of calculating the mean, standard deviation, variance, median, five number summary, interquartile range (IQR) as well as arbitrary quantiles. To improve the legibility of output, we will also set the default number of digits to display to a more reasonable level (see ?options() for more configuration possibilities).\n\noptions(digits = 3)\n\n\n\nSimulate the weight of tuna species in the WIO region\nAs an example, we can simulate the weight of yellow fin tuna in the tropical Indian ocean. According to , yellow fin tuna weight range from 0.5 to 7.8 kg and we can use runif function in R to generate a sample of 200 individual as the code highlight;\n\nweight = runif(n = 200, min = 0.5, max = 7.8)\n# weight = rnorm(n = 200, mean = 4.10, sd = .28)\n\nweight |&gt; hist()\n\n\n\n\n\n\n\n\nInbuilt function exists for summary statistics.\n\nweight |&gt;\n  mean()\n\n[1] 4.27\n\n\n\nweight |&gt;\n  median()\n\n[1] 4.31\n\n\n\nweight |&gt;\n  sd()\n\n[1] 2.2\n\n\n\nweight |&gt;\n  range()\n\n[1] 0.504 7.780\n\n\n\nweight |&gt;\n  mode()\n\n[1] \"numeric\"\n\n\nIt is also straightforward to calculate quantiles of the distribution.\n\nweight |&gt;\n  quantile()\n\n   0%   25%   50%   75%  100% \n0.504 2.250 4.309 6.453 7.780 \n\n\nRather than computing the metrics as individual, some packages have dedicated functions that produce the summary statistic. For example, the psych package has describe function;\n\nweight |&gt;\n  psych::describe()\n\n   vars   n mean  sd median trimmed  mad min  max range  skew kurtosis   se\nX1    1 200 4.27 2.2   4.31     4.3 3.13 0.5 7.78  7.28 -0.05     -1.3 0.16\n\n\n\nweight |&gt;\n  summarytools::descr()\n\nDescriptive Statistics  \nweight  \nN: 200  \n\n                    weight\n----------------- --------\n             Mean     4.27\n          Std.Dev     2.20\n              Min     0.50\n               Q1     2.24\n           Median     4.31\n               Q3     6.45\n              Max     7.78\n              MAD     3.13\n              IQR     4.20\n               CV     0.52\n         Skewness    -0.05\n      SE.Skewness     0.17\n         Kurtosis    -1.30\n          N.Valid   200.00\n        Pct.Valid   100.00\n\n\n\nweight |&gt;\n  EnvStats::summaryFull()\n\n                             weight  \nN                            200     \nMean                           4.27  \nMedian                         4.31  \n10% Trimmed Mean               4.3   \nGeometric Mean                 3.53  \nSkew                          -0.0482\nKurtosis                      -1.28  \nMin                            0.504 \nMax                            7.78  \nRange                          7.28  \n1st Quartile                   2.25  \n3rd Quartile                   6.45  \nStandard Deviation             2.2   \nGeometric Standard Deviation   2     \nInterquartile Range            4.2   \nMedian Absolute Deviation      3.13  \nCoefficient of Variation       0.516 \n\n\nFinally, the report function in the report package provides a concise summary of many useful statistics.\n\nweight |&gt;\n  report::report()\n\nx: n = 200, Mean = 4.27, SD = 2.20, Median = 4.31, MAD = 3.13, range: [0.50,\n7.78], Skewness = -0.05, Kurtosis = -1.28, 0% missing\n\n\nLikewise, the fav_stats function in mosaic package\n\nweight |&gt;\n  mosaic::fav_stats()\n\n   min   Q1 median   Q3  max mean  sd   n missing\n 0.504 2.25   4.31 6.45 7.78 4.27 2.2 200       0\n\n\n\n\nGraphical Summaries\n\nlength = rnorm(n = 200, mean = 60, sd = 8)\n\nTh e functions hist provide numerous ways of binning the data, of normalizing the data, and of displaying the histogram.\n\nlength |&gt;\n  hist()\n\n\n\n\n\n\n\n\nTh e functions boxplot provide five summary number (min, Q1, median, Q3, max) values that are displayed in boxplot\n\nlength |&gt;\n  boxplot()\n\n\n\n\n\n\n\n\n\nlength |&gt;\n  densityplot()"
  },
  {
    "objectID": "posts/stats0/index.html#summary",
    "href": "posts/stats0/index.html#summary",
    "title": "Understanding Descriptive Statistics",
    "section": "Summary",
    "text": "Summary\nUnderstanding both central tendency and dispersion is essential for gaining insights into the characteristics of a dataset. Together, these measures can help to identify patterns, trends, and potential outliers within the data."
  },
  {
    "objectID": "posts/stats0/index.html#references",
    "href": "posts/stats0/index.html#references",
    "title": "Understanding Descriptive Statistics",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "posts/manipulate/index.html",
    "href": "posts/manipulate/index.html",
    "title": "Data cleaning, merging, and appending",
    "section": "",
    "text": "In reality, data cleaning is not so much its own step in the data wrangling process as it is a constant activity that accompanies every other step, both because most data is not clean when we encounter it, and because how a data set (or part of it) needs to be “cleaned” is often revealed progressively as we work. At a high level, clean data might be summarized as being free from errors or typos – such as mismatched units, multiple spellings of the same word or term, and fields that are not well-separated – and missing or impossible values.\nWhile many of these are at least somewhat straightforward to recognize (though not always to correct), however, deeper data problems may still persist. Measurement changes, calculation errors and other oversights – especially in system-generated data – often don’t reveal themselves until some level of analysis has been done and the data has been “reality-checked” with folks who significant expertise and/or first-hand experience with the subject.\n\n\n\n\n\nflowchart LR\n  A[Main variable types] --&gt; B{Catrgorical}\n  A[Main variable types] --&gt; C{Numeric}\n  B{Catrgorical} --&gt; D[ordinal]\n  B{Catrgorical} --&gt; E[non-ordinal]\n  C{Numeric} --&gt; F[continuous]\n  C{Numeric} --&gt; G[discrete]\n\n\n\n\n\n\nThe iterative nature of data cleaning is an example of why data wrangling is a cycle rather than a linear series of steps: as your work reveals more about the data and your understanding of its relationship to the world deepens, you may find that you need to revisit earlier work that you’ve done and repeat or adjust certain aspects of the data\n\n\nAlthough creating data frames from existing data structures is extremely useful, by far the most common approach is to create a data frame by importing data from an external file. To do this, you’ll need to have your data correctly formatted and saved in a file format that R is able to recognize. Fortunately for us, R is able to recognize a wide variety of file formats, although in reality you’ll probably end up only using two or three regularly—comma separated, excel spreadsheet and tab-delimited.\nThe easiest method of creating a data file to import into R is to enter your data into a spreadsheet using either Microsoft Excel or LibreOffice Calc and save the spreadsheet as a comma separated (.csv), excel spreadsheet (.xls or .xlsx) and tab-delimited (.txt). We then read the file into our session, for this mode, we are going to use the LFQ_sample_1.xls, which is an Excel spreadsheet file. To load into the R session, we use a read_excel function from readxl package (Wickham and Bryan, 2018). Before we import the data, we need to load the packages that we will use their functions in this chapter\n\nrequire(tidyverse)\nrequire(magrittr)\n\n\nlfq = readxl::read_excel(path = \"../data/LFQ_sample_1.xls\",\n                         sheet = 1, \n                         skip = 0)\n\nThere are a few things to note about the above command. First, the file path and the filename (including the file extension) needs to be enclosed in either single or double quotes (i.e. the data/flower.txt bit) as the read_excel() function expects this to be a character string. If your working directory is already set to the directory which contains the file, you don’t need to include the entire file path just the filename.\nIn the example above, the file path is separated with a single forward slash /. This will work regardless of the operating system you are using. The argument sheet = 1 specify the function to pick the data from first sheet as spreadsheet store data in multiple sheet. The argument skip = 0 specify the function not to skip any row in the datase. This used to specify when the data has metadata information at the top.\nAfter importing our data into R it doesn’t appear that R has done much, at least nothing appears in the R Console! To see the contents of the data frame we need to tell R to do so using a print function.\n\nlfq %&gt;% print()\n\n# A tibble: 779 × 6\n   Date                Species `Size (cm)` `Size Class` `Gear type` Landing_site\n   &lt;dttm&gt;              &lt;chr&gt;         &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;       \n 1 2019-05-30 00:00:00 Siganu…        16.5           18 Speargun    Pwani       \n 2 2019-05-30 00:00:00 Siganu…        12             12 Speargun    Pwani       \n 3 2019-05-30 00:00:00 Siganu…        15             15 Speargun    Pwani       \n 4 2019-05-30 00:00:00 Siganu…        33             33 Speargun    Pwani       \n 5 2019-05-30 00:00:00 Siganu…        11.2           12 hook and l… Pwani       \n 6 2019-05-30 00:00:00 Siganu…        17             18 hook and l… Pwani       \n 7 2019-05-30 00:00:00 Siganu…        19             21 Gill net    Pwani       \n 8 2019-05-30 00:00:00 Lethri…        10             12 Traps       Pwani       \n 9 2019-05-30 00:00:00 Lethri…        11             12 Traps       Pwani       \n10 2019-05-30 00:00:00 Lethri…         9             12 Traps       Pwani       \n# ℹ 769 more rows\n\n\nThe data frame appears in the console and that prove to us that we successfully imported the data into R. But printing the imported data into console is only useful for small dataset and for large dataset that makes the console messy and there is nothing you can glean by simply printing the dataset in the console. The other common practice often used is looking on the internal structure of the dataset using str() function\n\nlfq %&gt;% \n  str()\n\ntibble [779 × 6] (S3: tbl_df/tbl/data.frame)\n $ Date        : POSIXct[1:779], format: \"2019-05-30\" \"2019-05-30\" ...\n $ Species     : chr [1:779] \"Siganus sutor\" \"Siganus sutor\" \"Siganus sutor\" \"Siganus sutor\" ...\n $ Size (cm)   : num [1:779] 16.5 12 15 33 11.2 17 19 10 11 9 ...\n $ Size Class  : num [1:779] 18 12 15 33 12 18 21 12 12 12 ...\n $ Gear type   : chr [1:779] \"Speargun\" \"Speargun\" \"Speargun\" \"Speargun\" ...\n $ Landing_site: chr [1:779] \"Pwani\" \"Pwani\" \"Pwani\" \"Pwani\" ...\n\n\nand a better solution is to use a tidyverse approach of glimpse function\n\nlfq %&gt;% \n  glimpse()\n\nRows: 779\nColumns: 6\n$ Date         &lt;dttm&gt; 2019-05-30, 2019-05-30, 2019-05-30, 2019-05-30, 2019-05-…\n$ Species      &lt;chr&gt; \"Siganus sutor\", \"Siganus sutor\", \"Siganus sutor\", \"Sigan…\n$ `Size (cm)`  &lt;dbl&gt; 16.5, 12.0, 15.0, 33.0, 11.2, 17.0, 19.0, 10.0, 11.0, 9.0…\n$ `Size Class` &lt;dbl&gt; 18, 12, 15, 33, 12, 18, 21, 12, 12, 12, 12, 12, 12, 12, 1…\n$ `Gear type`  &lt;chr&gt; \"Speargun\", \"Speargun\", \"Speargun\", \"Speargun\", \"hook and…\n$ Landing_site &lt;chr&gt; \"Pwani\", \"Pwani\", \"Pwani\", \"Pwani\", \"Pwani\", \"Pwani\", \"Pw…\n\n\nThe output in the console tell us that lfq dataset we just imported has six columns (variables) and 779 rows (records). Each of the variable are listed along with their data types and few records of the data The first row of the data contains the variable (column) names.\n\n\n\nIt’s quite common to get a bunch of really frustrating error messages when you first start importing data into R. Perhaps the most common is\nlfq = readxl::read_excel(path = \"assets/fao_paul_dataset/LFQ_sample_1\",\n                         sheet = 1, \n                         skip = 0)\nThis error message is telling you that R cannot find the file you are trying to import. Several reasons can lead to that information error.The first is that you’ve made a mistake in the spelling of either the filename or file path. Another common mistake is that you have forgotten to include the file extension in the filename (i.e. .txt). Lastly, the file is not where you say it is or you’ve used an incorrect file path. Using RStudio Projects and having a logical directory structure goes a long way to avoiding these types of errors.\n\n\n\n\nlfq %&gt;% \n  names()\n\n[1] \"Date\"         \"Species\"      \"Size (cm)\"    \"Size Class\"   \"Gear type\"   \n[6] \"Landing_site\"\n\n\n\nlfq.clean = lfq %&gt;% \n  janitor::clean_names() %&gt;% \n  janitor::remove_empty()\n\n\n\n\n\nlfq.clean %&gt;% \n  glimpse()\n\nRows: 779\nColumns: 6\n$ date         &lt;dttm&gt; 2019-05-30, 2019-05-30, 2019-05-30, 2019-05-30, 2019-05-…\n$ species      &lt;chr&gt; \"Siganus sutor\", \"Siganus sutor\", \"Siganus sutor\", \"Sigan…\n$ size_cm      &lt;dbl&gt; 16.5, 12.0, 15.0, 33.0, 11.2, 17.0, 19.0, 10.0, 11.0, 9.0…\n$ size_class   &lt;dbl&gt; 18, 12, 15, 33, 12, 18, 21, 12, 12, 12, 12, 12, 12, 12, 1…\n$ gear_type    &lt;chr&gt; \"Speargun\", \"Speargun\", \"Speargun\", \"Speargun\", \"hook and…\n$ landing_site &lt;chr&gt; \"Pwani\", \"Pwani\", \"Pwani\", \"Pwani\", \"Pwani\", \"Pwani\", \"Pw…\n\n\n\n\n\nYou notice that the date is datetime and date but the display is date. Thus, we need to create three variables that represent day, month, and year. We can extract these variables from date variables, We can use the day(), month(), and year() functions from lubridate package (Grolemund and Wickham, 2011);\n\nlfq.time = lfq.clean %&gt;% \n  mutate(\n    day = lubridate::day(date),\n    month = lubridate::month(date),\n    year = lubridate::year(date)\n    ) %&gt;% \n  relocate(c(landing_site, day, month, year), .before = date)\n\nlfq.time |&gt; \n  sample_n(30) |&gt; \n  select(-c(day, month, year)) |&gt; \n  flextable::flextable() |&gt; \n  flextable::autofit()\n\nlanding_sitedatespeciessize_cmsize_classgear_typePwani2019-07-12 00:00:00Anampses spp.14.015Gill netBahari2020-04-08 00:00:00Leptoscarus vaigiensis19.821SpeargunPwani2019-07-11 00:00:00Leptoscarus vaigiensis25.527SpeargunPwani2019-07-11 00:00:00Lutjanus fulviflamma27.530hook and lineBahari2020-07-25 00:00:00Siganus sutor13.815TrapsPwani2019-07-11 00:00:00Calotomus carolinus16.018SpeargunPwani2019-05-30 00:00:00Leptoscarus vaigiensis15.618SpeargunPwani2019-07-12 00:00:00Siganus sutor17.518TrapsPwani2019-06-25 00:00:00Siganus sutor18.421TrapsPwani2019-07-11 00:00:00Calotomus carolinus15.518Beach seinePwani2019-05-30 00:00:00Lethrinus mahsena11.112hook and linePwani2019-07-12 00:00:00Siganus sutor27.530Gill netBahari2020-07-25 00:00:00Siganus sutor15.518TrapsPwani2019-07-11 00:00:00Calotomus carolinus16.018Beach seineBahari2020-07-25 00:00:00Siganus sutor16.018SpeargunBahari2020-04-08 00:00:00Siganus sutor12.012TrapsBahari2020-07-25 00:00:00Siganus sutor14.215TrapsPwani2019-07-11 00:00:00Calotomus carolinus17.018Beach seinePwani2019-07-11 00:00:00Calotomus carolinus14.315SpeargunPwani2019-05-30 00:00:00Leptoscarus vaigiensis16.518SpeargunPwani2019-05-30 00:00:00Leptoscarus vaigiensis30.030SpeargunPwani2019-07-11 00:00:00Calotomus carolinus18.018Beach seinePwani2019-05-30 00:00:00Lethrinus mahsena12.012hook and lineBahari2020-07-25 00:00:00Siganus sutor14.315TrapsPwani2019-07-11 00:00:00Calotomus carolinus16.018Beach seinePwani2019-07-11 00:00:00Calotomus carolinus13.515Beach seinePwani2019-07-11 00:00:00Calotomus carolinus9.512Beach seinePwani2019-05-30 00:00:00Lethrinus mahsena14.015hook and linePwani2019-07-11 00:00:00Calotomus carolinus17.518Beach seineBahari2020-07-25 00:00:00Siganus sutor15.618Traps\n\n\n\n\n\n\nlfq.time %&gt;% \n  select(-date)\n\n# A tibble: 779 × 8\n   landing_site   day month  year species           size_cm size_class gear_type\n   &lt;chr&gt;        &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;    \n 1 Pwani           30     5  2019 Siganus sutor        16.5         18 Speargun \n 2 Pwani           30     5  2019 Siganus sutor        12           12 Speargun \n 3 Pwani           30     5  2019 Siganus sutor        15           15 Speargun \n 4 Pwani           30     5  2019 Siganus sutor        33           33 Speargun \n 5 Pwani           30     5  2019 Siganus sutor        11.2         12 hook and…\n 6 Pwani           30     5  2019 Siganus sutor        17           18 hook and…\n 7 Pwani           30     5  2019 Siganus sutor        19           21 Gill net \n 8 Pwani           30     5  2019 Lethrinus mahsena    10           12 Traps    \n 9 Pwani           30     5  2019 Lethrinus mahsena    11           12 Traps    \n10 Pwani           30     5  2019 Lethrinus mahsena     9           12 Traps    \n# ℹ 769 more rows\n\n\n\n\n\nR handles missing values differently than some other programs, including Stata. Missing values will appear as NA (whereas in Stata these will appear as large numeric values). Note, though, that NA is not a string, it is a symbol. If you try to conduct logical tests with NA, you are likely to get errors or NULL."
  },
  {
    "objectID": "posts/manipulate/index.html#introduction",
    "href": "posts/manipulate/index.html#introduction",
    "title": "Data cleaning, merging, and appending",
    "section": "",
    "text": "In reality, data cleaning is not so much its own step in the data wrangling process as it is a constant activity that accompanies every other step, both because most data is not clean when we encounter it, and because how a data set (or part of it) needs to be “cleaned” is often revealed progressively as we work. At a high level, clean data might be summarized as being free from errors or typos – such as mismatched units, multiple spellings of the same word or term, and fields that are not well-separated – and missing or impossible values.\nWhile many of these are at least somewhat straightforward to recognize (though not always to correct), however, deeper data problems may still persist. Measurement changes, calculation errors and other oversights – especially in system-generated data – often don’t reveal themselves until some level of analysis has been done and the data has been “reality-checked” with folks who significant expertise and/or first-hand experience with the subject.\n\n\n\n\n\nflowchart LR\n  A[Main variable types] --&gt; B{Catrgorical}\n  A[Main variable types] --&gt; C{Numeric}\n  B{Catrgorical} --&gt; D[ordinal]\n  B{Catrgorical} --&gt; E[non-ordinal]\n  C{Numeric} --&gt; F[continuous]\n  C{Numeric} --&gt; G[discrete]\n\n\n\n\n\n\nThe iterative nature of data cleaning is an example of why data wrangling is a cycle rather than a linear series of steps: as your work reveals more about the data and your understanding of its relationship to the world deepens, you may find that you need to revisit earlier work that you’ve done and repeat or adjust certain aspects of the data\n\n\nAlthough creating data frames from existing data structures is extremely useful, by far the most common approach is to create a data frame by importing data from an external file. To do this, you’ll need to have your data correctly formatted and saved in a file format that R is able to recognize. Fortunately for us, R is able to recognize a wide variety of file formats, although in reality you’ll probably end up only using two or three regularly—comma separated, excel spreadsheet and tab-delimited.\nThe easiest method of creating a data file to import into R is to enter your data into a spreadsheet using either Microsoft Excel or LibreOffice Calc and save the spreadsheet as a comma separated (.csv), excel spreadsheet (.xls or .xlsx) and tab-delimited (.txt). We then read the file into our session, for this mode, we are going to use the LFQ_sample_1.xls, which is an Excel spreadsheet file. To load into the R session, we use a read_excel function from readxl package (Wickham and Bryan, 2018). Before we import the data, we need to load the packages that we will use their functions in this chapter\n\nrequire(tidyverse)\nrequire(magrittr)\n\n\nlfq = readxl::read_excel(path = \"../data/LFQ_sample_1.xls\",\n                         sheet = 1, \n                         skip = 0)\n\nThere are a few things to note about the above command. First, the file path and the filename (including the file extension) needs to be enclosed in either single or double quotes (i.e. the data/flower.txt bit) as the read_excel() function expects this to be a character string. If your working directory is already set to the directory which contains the file, you don’t need to include the entire file path just the filename.\nIn the example above, the file path is separated with a single forward slash /. This will work regardless of the operating system you are using. The argument sheet = 1 specify the function to pick the data from first sheet as spreadsheet store data in multiple sheet. The argument skip = 0 specify the function not to skip any row in the datase. This used to specify when the data has metadata information at the top.\nAfter importing our data into R it doesn’t appear that R has done much, at least nothing appears in the R Console! To see the contents of the data frame we need to tell R to do so using a print function.\n\nlfq %&gt;% print()\n\n# A tibble: 779 × 6\n   Date                Species `Size (cm)` `Size Class` `Gear type` Landing_site\n   &lt;dttm&gt;              &lt;chr&gt;         &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;       \n 1 2019-05-30 00:00:00 Siganu…        16.5           18 Speargun    Pwani       \n 2 2019-05-30 00:00:00 Siganu…        12             12 Speargun    Pwani       \n 3 2019-05-30 00:00:00 Siganu…        15             15 Speargun    Pwani       \n 4 2019-05-30 00:00:00 Siganu…        33             33 Speargun    Pwani       \n 5 2019-05-30 00:00:00 Siganu…        11.2           12 hook and l… Pwani       \n 6 2019-05-30 00:00:00 Siganu…        17             18 hook and l… Pwani       \n 7 2019-05-30 00:00:00 Siganu…        19             21 Gill net    Pwani       \n 8 2019-05-30 00:00:00 Lethri…        10             12 Traps       Pwani       \n 9 2019-05-30 00:00:00 Lethri…        11             12 Traps       Pwani       \n10 2019-05-30 00:00:00 Lethri…         9             12 Traps       Pwani       \n# ℹ 769 more rows\n\n\nThe data frame appears in the console and that prove to us that we successfully imported the data into R. But printing the imported data into console is only useful for small dataset and for large dataset that makes the console messy and there is nothing you can glean by simply printing the dataset in the console. The other common practice often used is looking on the internal structure of the dataset using str() function\n\nlfq %&gt;% \n  str()\n\ntibble [779 × 6] (S3: tbl_df/tbl/data.frame)\n $ Date        : POSIXct[1:779], format: \"2019-05-30\" \"2019-05-30\" ...\n $ Species     : chr [1:779] \"Siganus sutor\" \"Siganus sutor\" \"Siganus sutor\" \"Siganus sutor\" ...\n $ Size (cm)   : num [1:779] 16.5 12 15 33 11.2 17 19 10 11 9 ...\n $ Size Class  : num [1:779] 18 12 15 33 12 18 21 12 12 12 ...\n $ Gear type   : chr [1:779] \"Speargun\" \"Speargun\" \"Speargun\" \"Speargun\" ...\n $ Landing_site: chr [1:779] \"Pwani\" \"Pwani\" \"Pwani\" \"Pwani\" ...\n\n\nand a better solution is to use a tidyverse approach of glimpse function\n\nlfq %&gt;% \n  glimpse()\n\nRows: 779\nColumns: 6\n$ Date         &lt;dttm&gt; 2019-05-30, 2019-05-30, 2019-05-30, 2019-05-30, 2019-05-…\n$ Species      &lt;chr&gt; \"Siganus sutor\", \"Siganus sutor\", \"Siganus sutor\", \"Sigan…\n$ `Size (cm)`  &lt;dbl&gt; 16.5, 12.0, 15.0, 33.0, 11.2, 17.0, 19.0, 10.0, 11.0, 9.0…\n$ `Size Class` &lt;dbl&gt; 18, 12, 15, 33, 12, 18, 21, 12, 12, 12, 12, 12, 12, 12, 1…\n$ `Gear type`  &lt;chr&gt; \"Speargun\", \"Speargun\", \"Speargun\", \"Speargun\", \"hook and…\n$ Landing_site &lt;chr&gt; \"Pwani\", \"Pwani\", \"Pwani\", \"Pwani\", \"Pwani\", \"Pwani\", \"Pw…\n\n\nThe output in the console tell us that lfq dataset we just imported has six columns (variables) and 779 rows (records). Each of the variable are listed along with their data types and few records of the data The first row of the data contains the variable (column) names.\n\n\n\nIt’s quite common to get a bunch of really frustrating error messages when you first start importing data into R. Perhaps the most common is\nlfq = readxl::read_excel(path = \"assets/fao_paul_dataset/LFQ_sample_1\",\n                         sheet = 1, \n                         skip = 0)\nThis error message is telling you that R cannot find the file you are trying to import. Several reasons can lead to that information error.The first is that you’ve made a mistake in the spelling of either the filename or file path. Another common mistake is that you have forgotten to include the file extension in the filename (i.e. .txt). Lastly, the file is not where you say it is or you’ve used an incorrect file path. Using RStudio Projects and having a logical directory structure goes a long way to avoiding these types of errors.\n\n\n\n\nlfq %&gt;% \n  names()\n\n[1] \"Date\"         \"Species\"      \"Size (cm)\"    \"Size Class\"   \"Gear type\"   \n[6] \"Landing_site\"\n\n\n\nlfq.clean = lfq %&gt;% \n  janitor::clean_names() %&gt;% \n  janitor::remove_empty()\n\n\n\n\n\nlfq.clean %&gt;% \n  glimpse()\n\nRows: 779\nColumns: 6\n$ date         &lt;dttm&gt; 2019-05-30, 2019-05-30, 2019-05-30, 2019-05-30, 2019-05-…\n$ species      &lt;chr&gt; \"Siganus sutor\", \"Siganus sutor\", \"Siganus sutor\", \"Sigan…\n$ size_cm      &lt;dbl&gt; 16.5, 12.0, 15.0, 33.0, 11.2, 17.0, 19.0, 10.0, 11.0, 9.0…\n$ size_class   &lt;dbl&gt; 18, 12, 15, 33, 12, 18, 21, 12, 12, 12, 12, 12, 12, 12, 1…\n$ gear_type    &lt;chr&gt; \"Speargun\", \"Speargun\", \"Speargun\", \"Speargun\", \"hook and…\n$ landing_site &lt;chr&gt; \"Pwani\", \"Pwani\", \"Pwani\", \"Pwani\", \"Pwani\", \"Pwani\", \"Pw…\n\n\n\n\n\nYou notice that the date is datetime and date but the display is date. Thus, we need to create three variables that represent day, month, and year. We can extract these variables from date variables, We can use the day(), month(), and year() functions from lubridate package (Grolemund and Wickham, 2011);\n\nlfq.time = lfq.clean %&gt;% \n  mutate(\n    day = lubridate::day(date),\n    month = lubridate::month(date),\n    year = lubridate::year(date)\n    ) %&gt;% \n  relocate(c(landing_site, day, month, year), .before = date)\n\nlfq.time |&gt; \n  sample_n(30) |&gt; \n  select(-c(day, month, year)) |&gt; \n  flextable::flextable() |&gt; \n  flextable::autofit()\n\nlanding_sitedatespeciessize_cmsize_classgear_typePwani2019-07-12 00:00:00Anampses spp.14.015Gill netBahari2020-04-08 00:00:00Leptoscarus vaigiensis19.821SpeargunPwani2019-07-11 00:00:00Leptoscarus vaigiensis25.527SpeargunPwani2019-07-11 00:00:00Lutjanus fulviflamma27.530hook and lineBahari2020-07-25 00:00:00Siganus sutor13.815TrapsPwani2019-07-11 00:00:00Calotomus carolinus16.018SpeargunPwani2019-05-30 00:00:00Leptoscarus vaigiensis15.618SpeargunPwani2019-07-12 00:00:00Siganus sutor17.518TrapsPwani2019-06-25 00:00:00Siganus sutor18.421TrapsPwani2019-07-11 00:00:00Calotomus carolinus15.518Beach seinePwani2019-05-30 00:00:00Lethrinus mahsena11.112hook and linePwani2019-07-12 00:00:00Siganus sutor27.530Gill netBahari2020-07-25 00:00:00Siganus sutor15.518TrapsPwani2019-07-11 00:00:00Calotomus carolinus16.018Beach seineBahari2020-07-25 00:00:00Siganus sutor16.018SpeargunBahari2020-04-08 00:00:00Siganus sutor12.012TrapsBahari2020-07-25 00:00:00Siganus sutor14.215TrapsPwani2019-07-11 00:00:00Calotomus carolinus17.018Beach seinePwani2019-07-11 00:00:00Calotomus carolinus14.315SpeargunPwani2019-05-30 00:00:00Leptoscarus vaigiensis16.518SpeargunPwani2019-05-30 00:00:00Leptoscarus vaigiensis30.030SpeargunPwani2019-07-11 00:00:00Calotomus carolinus18.018Beach seinePwani2019-05-30 00:00:00Lethrinus mahsena12.012hook and lineBahari2020-07-25 00:00:00Siganus sutor14.315TrapsPwani2019-07-11 00:00:00Calotomus carolinus16.018Beach seinePwani2019-07-11 00:00:00Calotomus carolinus13.515Beach seinePwani2019-07-11 00:00:00Calotomus carolinus9.512Beach seinePwani2019-05-30 00:00:00Lethrinus mahsena14.015hook and linePwani2019-07-11 00:00:00Calotomus carolinus17.518Beach seineBahari2020-07-25 00:00:00Siganus sutor15.618Traps\n\n\n\n\n\n\nlfq.time %&gt;% \n  select(-date)\n\n# A tibble: 779 × 8\n   landing_site   day month  year species           size_cm size_class gear_type\n   &lt;chr&gt;        &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;    \n 1 Pwani           30     5  2019 Siganus sutor        16.5         18 Speargun \n 2 Pwani           30     5  2019 Siganus sutor        12           12 Speargun \n 3 Pwani           30     5  2019 Siganus sutor        15           15 Speargun \n 4 Pwani           30     5  2019 Siganus sutor        33           33 Speargun \n 5 Pwani           30     5  2019 Siganus sutor        11.2         12 hook and…\n 6 Pwani           30     5  2019 Siganus sutor        17           18 hook and…\n 7 Pwani           30     5  2019 Siganus sutor        19           21 Gill net \n 8 Pwani           30     5  2019 Lethrinus mahsena    10           12 Traps    \n 9 Pwani           30     5  2019 Lethrinus mahsena    11           12 Traps    \n10 Pwani           30     5  2019 Lethrinus mahsena     9           12 Traps    \n# ℹ 769 more rows\n\n\n\n\n\nR handles missing values differently than some other programs, including Stata. Missing values will appear as NA (whereas in Stata these will appear as large numeric values). Note, though, that NA is not a string, it is a symbol. If you try to conduct logical tests with NA, you are likely to get errors or NULL."
  },
  {
    "objectID": "posts/manipulate/index.html#length-frequency",
    "href": "posts/manipulate/index.html#length-frequency",
    "title": "Data cleaning, merging, and appending",
    "section": "2 Length frequency",
    "text": "2 Length frequency\nLet’s first look at the length data from the catch, which gives an indication of the size structure and health of the population. Using several case studies, we will filter the dataset to choose certain species and then plot histograms of the length data, which reveal the number of individual of each size class were measured in a sample or catch data.\nYou have several options for dealing with NA values. - na.omit() or na.exclude() will row-wise delete missing values in your dataset - na.fail() will keep an object only if no missing values are present - na.action= is a common option in many functions, for example in a linear model where you might write model &lt;- lm(y~x, data = data, na.action = na.omit). - is.na allows you to logically test for NA values, for example when subsetting\n\n2.1 Merging and combining data\nYou may want to draw on data from multiple sources or at differing levels of aggregation. To do this, you first must know what your data look like and know what format you ultimately want them to take. Whatever you do, do not attempt to merge datasets by hand—this leads to mistakes and frustration.\nleft join bind_row bind_col\n\n\n2.2 Reshaping data\nReshaping data into different formats is a common task. With rectangular type data (data frames have the same number of rows in each column) there are two main data frame shapes that you will come across: the long format (sometimes called stacked) and the wide format (Wickham and Henry, 2018). An example of a long format data frame is given in Table 1. We can see that each row is a single observation from an individual species and each species can have multiple rows with different values of year and catch.\n\nset.seed(123)\n\naa = read_csv(\"../data/tidy/fao_capture.csv\")  %&gt;% \n  sample_n(size = 5) %&gt;% \n  separate(col = 'ASFIS species', into = c(\"english_name\", \"species\"), sep = \"-\") %&gt;% dplyr::select(1:2, 7:10) \n\n\n\n\n\nTable 1: Long format data frame data format\n\n\n\nenglish_namespeciesyearcatch_mtRound sardinellaSardinella aurita2016123 377Round sardinellaSardinella aurita2017126 766Round sardinellaSardinella aurita2018126 400Round sardinellaSardinella aurita2019126 400Mexican foureyed octopus201625 722Mexican foureyed octopus201726 020Mexican foureyed octopus201828809Mexican foureyed octopus201920 119Gulf menhadenBrevoortia patronus2016618 563Gulf menhadenBrevoortia patronus2017461 189Gulf menhadenBrevoortia patronus2018529 231Gulf menhadenBrevoortia patronus2019336 221Stromboid conchs neiStrombus spp201628 774Stromboid conchs neiStrombus spp201735 273Stromboid conchs neiStrombus spp201834 856Stromboid conchs neiStrombus spp201928 308Atlantic seabobXiphopenaeus kroyeri201630 147Atlantic seabobXiphopenaeus kroyeri201731 319Atlantic seabobXiphopenaeus kroyeri201831 625Atlantic seabobXiphopenaeus kroyeri201922 497\n\n\n\n\n\nWe can also format the same data in the wide format as in Table 2. In this format the year are treated as variables and catches are values within the years.\n\n\n\n\nTable 2: Wide format data frame data format\n\n\n\nenglish_namespecies2016201720182019Round sardinellaSardinella aurita123 377126 766126 400126 400Mexican foureyed octopus25 72226 0202880920 119Gulf menhadenBrevoortia patronus618 563461 189529 231336 221Stromboid conchs neiStrombus spp28 77435 27334 85628 308Atlantic seabobXiphopenaeus kroyeri30 14731 31931 62522 497\n\n\n\n\n\nWhilst there’s no inherent problem with either of these formats we will sometimes need to convert between the two because some functions will require a specific format for them to work. But for the modern tidyverse function, only the dataset that is long format accepted.\nThere are many ways to convert between these two formats but we’ll use the pivot_longer() and pivot_wider() functions from the tidyr package (Wickham and Henry, 2018). We will use fao_capture.csv dataset in this session, we can simply load it with read_csv function and specify the path our dataset reside;\n\ncapture.wide = read_csv(\"../data/tidy/fao_capture.csv\")\n\n\ncapture.wide  |&gt; \n  flextable::flextable() |&gt; \n  flextable::autofit()\n\nASFIS species20122013201420152016201720182019Marine fishes nei-Osteichthyes137 683112 095110 535118 078118 204124 895109 938415 031Gulf menhaden-Brevoortia patronus500 162440 709385 022539 198618 563461 189529 231336 221Round sardinella-Sardinella aurita42 34446 29950 69879 851123 377126 766126 400126 400American cupped oyster-Crassostrea virginica77 90053 86657 93055 04144 31378 96149 63147 723Ark clams nei-Arca spp12 68715 00017 38429 07329 07629 97029 97029 970Stromboid conchs nei-Strombus spp36 85536 61030 69134 38828 77435 27334 85628 308Blue crab-Callinectes sapidus43 99740 57044 36150 78153 61654 62856 68925 937Yellowfin tuna-Thunnus albacares19 08721 86525 83127 0363448729 31030 96624 503Caribbean spiny lobster-Panulirus argus31 10228 46828 29831 35832 41030 20428 32524 311Atlantic seabob-Xiphopenaeus kroyeri36 07334 24325 20325 32230 14731 31931 62522 497Mexican four-eyed octopus-Octopus maya12 6298 80615 40323 44125 72226 0202880920 119\n\n\nThe capture.wideobject we just created is the dataframe but is in wide format. To change from wide to long format data table, we use the pivot_longer function. The first argument specified in pivot_longer is cols = 2:9 are index value of the variable in column 2 to colum 9 of the dataset we want to stack, names_to = \"year\" argument specify that the stacked variables will all be under a new variable name called year and the values in the stacked variables will all be under one variable called catch\n\ncapture.long = capture.wide %&gt;% \n  pivot_longer(cols = 2:9, names_to = \"year\", values_to = \"catch\")\n\n\ncapture.long |&gt; \n  FSA::headtail(n = 4) |&gt; \n  flextable::flextable() |&gt; \n  flextable::autofit()\n\n\n\nTable 3: Data values arranged in tidy wide format\n\n\n\nASFIS speciesyearcatchMarine fishes nei-Osteichthyes2012137 683Marine fishes nei-Osteichthyes2013112 095Marine fishes nei-Osteichthyes2014110 535Marine fishes nei-Osteichthyes2015118 078Mexican four-eyed octopus-Octopus maya201625 722Mexican four-eyed octopus-Octopus maya201726 020Mexican four-eyed octopus-Octopus maya201828809Mexican four-eyed octopus-Octopus maya201920 119\n\n\n\n\n\nThe pivot_wider() function is used to convert from a long format data frame to a wide format data frame. The first argument is names_from = year is the variable of the data frame for which we want spread across and the second argument values_from = catch is the corresponding values will reside in each casted variable.\n\ncapture.wide = capture.long %&gt;% \n  pivot_wider(names_from = year, values_from = catch)\n\n\n\n\n\nTable 4: Data values arranged in tidy wide format\n\n\n\nASFIS species20122013201420152016201720182019Marine fishes nei-Osteichthyes137 683112 095110 535118 078118 204124 895109 938415 031Gulf menhaden-Brevoortia patronus500 162440 709385 022539 198618 563461 189529 231336 221Round sardinella-Sardinella aurita42 34446 29950 69879 851123 377126 766126 400126 400American cupped oyster-Crassostrea virginica77 90053 86657 93055 04144 31378 96149 63147 723Ark clams nei-Arca spp12 68715 00017 38429 07329 07629 97029 97029 970Stromboid conchs nei-Strombus spp36 85536 61030 69134 38828 77435 27334 85628 308Blue crab-Callinectes sapidus43 99740 57044 36150 78153 61654 62856 68925 937Yellowfin tuna-Thunnus albacares19 08721 86525 83127 0363448729 31030 96624 503Caribbean spiny lobster-Panulirus argus31 10228 46828 29831 35832 41030 20428 32524 311Atlantic seabob-Xiphopenaeus kroyeri36 07334 24325 20325 32230 14731 31931 62522 497Mexican four-eyed octopus-Octopus maya12 6298 80615 40323 44125 72226 0202880920 119\n\n\n\n\n\n\n\n2.3 Saving transformed data\nOnce you have transformed your data and edited variables, you may want to save your new dataframe as an external data file that your collaborators or other researchers can use. As with reading in data, for saving out data you will need the foreign package. Similarly, you can choose from a number of different data formats to export to as well. Most commonly you will want to save out data as a .csv or a tab-delited text file.\nmyfile %&gt;% write_csv(\"my.directory/file_name.csv\")"
  },
  {
    "objectID": "posts/manipulate/index.html#transforming-summarising-and-analysing-data",
    "href": "posts/manipulate/index.html#transforming-summarising-and-analysing-data",
    "title": "Data cleaning, merging, and appending",
    "section": "3 Transforming, summarising, and analysing data",
    "text": "3 Transforming, summarising, and analysing data\nThe R universe basically builds upon two (seemingly contradictive) approaches: base R and the tidyverse. While these two approaches are often seen as two different philosophies, they can form a symbiosis. We therefore recommend to pick whichever works best for you – or to combine the two.Whereas base R is already implemented in R, using the tidyverse requires users to load new packages. People often find base R unintuitive and hard to read. This is why Hadley Wickham developed and introduced the tidyverse – a more intuitive approach to managing and wrangling data (Wickham and Wickham, 2017).\nCode written before 2014 was usually written in base R whereas the tidyverse style is becoming increasingly widespread. Again, which approach you prefer is rather a matter of personal taste than a decision between “right or wrong”. We first familiarize ourselves with the basic logic of the tidyverse style using some examples. For this, we use LFQ_sample_1.xls. since is an Excel format, lets import it in our session;\n\nlfq.sample1 = readxl::read_excel(\"../data/LFQ_sample_1.xls\")\n\nLet’s print the assigned lfq.sample1\n\nlfq.sample1 |&gt; \n  FSA::headtail() |&gt; \n  flextable::flextable() |&gt; \n  flextable::autofit()\n\nDateSpeciesSize (cm)Size ClassGear typeLanding_site2019-05-30 00:00:00Siganus sutor16.518SpeargunPwani2019-05-30 00:00:00Siganus sutor12.012SpeargunPwani2019-05-30 00:00:00Siganus sutor15.015SpeargunPwani2020-07-25 00:00:00Lutjanus fulviflamma13.015SpeargunBahari2020-07-25 00:00:00Lutjanus fulviflamma17.018SpeargunBahari2020-07-25 00:00:00Lutjanus fulviflamma18.521SpeargunBahari\n\n\nAs you can see from the output generated in your console, the lfq.sample1 data frame object contains length information of several species. The fisheries data collected in this dataset ensured that each record on a separate row and each column represents a variable. The dataset contains six variables (columns) and 779 records (rows). These variables includes;\n\nDate a sample was collected. The date is in the YYYY-MM-DD, which is the format that accepted\nSpecies the scientific name of the species\nSize (cm) the length of the fish species measured in centimeter\nSize Class of the individual\nGear type used to catch the recorded fish, and\nLanding-site a landing station where the fish was recorded\n\nThis dataset is arranged in appropriate format. The data was entered in spreadsheet with a few basics. These correspond to: what, where, when it only miss who collect the data or the originator of this dataset. The way this dataset is organized is the classical example of how set out our data in a similar fashion. This makes manipulating the data more straightforward and also maintains the multi-purpose nature of the analysis work. Having your data organized is really important!\n\n\n\n\n\n\nWarning\n\n\n\nThough the size and unit of the dataaset is provide, but it does not tell us much because we do not know whether the measurement is total length (TL), standard length (SL) or fork length (FL). It is imperative to specify in the dataset variable names what was measured and the unit\n\n\n\n3.1 Renaming variables\nLooking lfq.sample1 dataset presented in ?@tbl-case1, we notice that variable names do not adhere to standard variable names. The tidyverse style guide recommends snake case (words separated by underscores tl_cm) for object and column names. Let’s look back at our column names for a minute. Using the names() function from the and pipe operator %&gt;% from the magrittr package simultaneously serves as the first pipe in a chain of commands and print names in the dataset\n\nlfq.sample1 %&gt;% \n  names()\n\n[1] \"Date\"         \"Species\"      \"Size (cm)\"    \"Size Class\"   \"Gear type\"   \n[6] \"Landing_site\"\n\n\nThere are all sorts of capital letters and spaces (e.g. “Gear type” ,” “Size Class” ) as well as symbols (“Size (cm)”). Therefore we mustl convert all of these to snake case for us. We can rename specific variables using the rename function from the dplyr package (Wickham et al., 2019). Since dplyr is part of the tidyverse package (Wickham and Wickham, 2017) that was loaded, we do not need to load it again but rather use its function to rename the variables. We’ll begin by specifying the name of our data frame object lfq.sample1, followed by the = operator so that we can overwrite the existing lfq.sample1 frame object with one that contains the renamed variables.\nNext type lfq.sample1 followed by pipe operator %&gt;% and then call arename function, where you parse the new names to their corresponding old names. As the first argument, let’s change the Date variable to date by typing the name of our new variable followed by = and, in quotation marks (” “), the name of the original variable date = \"Date\". As the second argument, let’s apply the same process as the second argument and change the Species variable to species by typing the name of our new variable followed by = and, in quotation marks (” “), the name of the original variable species = \"Species\". We follow the same naming procedure to the six variables as the chunk below highlight;\n\nlfq.sample1 = lfq.sample1 %&gt;% \n  rename(\n    date = \"Date\",\n    species = \"Species\",\n    size_cm = \"Size (cm)\",\n    size_class = \"Size Class\",\n    gear_type = \"Gear type\",\n    landing_site = \"Landing_site\"\n    )\n\nUsing the head function from base R, let’s verify that we renamed the two variables successfully.\n\nlfq.sample1\n\n# A tibble: 779 × 6\n   date                species         size_cm size_class gear_type landing_site\n   &lt;dttm&gt;              &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;       \n 1 2019-05-30 00:00:00 Siganus sutor      16.5         18 Speargun  Pwani       \n 2 2019-05-30 00:00:00 Siganus sutor      12           12 Speargun  Pwani       \n 3 2019-05-30 00:00:00 Siganus sutor      15           15 Speargun  Pwani       \n 4 2019-05-30 00:00:00 Siganus sutor      33           33 Speargun  Pwani       \n 5 2019-05-30 00:00:00 Siganus sutor      11.2         12 hook and… Pwani       \n 6 2019-05-30 00:00:00 Siganus sutor      17           18 hook and… Pwani       \n 7 2019-05-30 00:00:00 Siganus sutor      19           21 Gill net  Pwani       \n 8 2019-05-30 00:00:00 Lethrinus mahs…    10           12 Traps     Pwani       \n 9 2019-05-30 00:00:00 Lethrinus mahs…    11           12 Traps     Pwani       \n10 2019-05-30 00:00:00 Lethrinus mahs…     9           12 Traps     Pwani       \n# ℹ 769 more rows\n\n\nAs you can see, the new names have no spaces and all are written in small letters. Spaces in variables names were replaced with the underscore symbol.\nThough the rename function is great tool to rename variables in the dataset, however there are times when you a dataset has hundred of variables names to change and using rename function from dplyr package become tedious. Furtunate , a janitor package has a nifty clean_names() function that clean all the variable names with a single line of code. I highly recommend incorporating it into your workflow.\n\nlfq.sample1 = readxl::read_excel(\"../data/LFQ_sample_1.xls\") %&gt;% \n  janitor::clean_names()\n\nlfq.sample1\n\n# A tibble: 779 × 6\n   date                species         size_cm size_class gear_type landing_site\n   &lt;dttm&gt;              &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;       \n 1 2019-05-30 00:00:00 Siganus sutor      16.5         18 Speargun  Pwani       \n 2 2019-05-30 00:00:00 Siganus sutor      12           12 Speargun  Pwani       \n 3 2019-05-30 00:00:00 Siganus sutor      15           15 Speargun  Pwani       \n 4 2019-05-30 00:00:00 Siganus sutor      33           33 Speargun  Pwani       \n 5 2019-05-30 00:00:00 Siganus sutor      11.2         12 hook and… Pwani       \n 6 2019-05-30 00:00:00 Siganus sutor      17           18 hook and… Pwani       \n 7 2019-05-30 00:00:00 Siganus sutor      19           21 Gill net  Pwani       \n 8 2019-05-30 00:00:00 Lethrinus mahs…    10           12 Traps     Pwani       \n 9 2019-05-30 00:00:00 Lethrinus mahs…    11           12 Traps     Pwani       \n10 2019-05-30 00:00:00 Lethrinus mahs…     9           12 Traps     Pwani       \n# ℹ 769 more rows\n\n\nAs you can see above, the clean_names() function handled every kind of messy variable name that was present in our lfq.sample1 dataset. Everything now looks neat and tidy\n\n\n3.2 Filtering Data\nOur lfq.sample1 contains six variables – date, species, size_cm, size_class, gear_type and landing_site;\n\nlfq.sample1 %&gt;% \n  glimpse()\n\nRows: 779\nColumns: 6\n$ date         &lt;dttm&gt; 2019-05-30, 2019-05-30, 2019-05-30, 2019-05-30, 2019-05-…\n$ species      &lt;chr&gt; \"Siganus sutor\", \"Siganus sutor\", \"Siganus sutor\", \"Sigan…\n$ size_cm      &lt;dbl&gt; 16.5, 12.0, 15.0, 33.0, 11.2, 17.0, 19.0, 10.0, 11.0, 9.0…\n$ size_class   &lt;dbl&gt; 18, 12, 15, 33, 12, 18, 21, 12, 12, 12, 12, 12, 12, 12, 1…\n$ gear_type    &lt;chr&gt; \"Speargun\", \"Speargun\", \"Speargun\", \"Speargun\", \"hook and…\n$ landing_site &lt;chr&gt; \"Pwani\", \"Pwani\", \"Pwani\", \"Pwani\", \"Pwani\", \"Pwani\", \"Pw…\n\n\nSuppose we are only interested in Siganus sutor species only from the dataset. For this purpose, we can use the filter() function from dplyr package (Wickham et al., 2019)\n\nlfq.sample1 %&gt;% \n  filter(species == \"Siganus sutor\")\n\n# A tibble: 196 × 6\n   date                species       size_cm size_class gear_type   landing_site\n   &lt;dttm&gt;              &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;       \n 1 2019-05-30 00:00:00 Siganus sutor    16.5         18 Speargun    Pwani       \n 2 2019-05-30 00:00:00 Siganus sutor    12           12 Speargun    Pwani       \n 3 2019-05-30 00:00:00 Siganus sutor    15           15 Speargun    Pwani       \n 4 2019-05-30 00:00:00 Siganus sutor    33           33 Speargun    Pwani       \n 5 2019-05-30 00:00:00 Siganus sutor    11.2         12 hook and l… Pwani       \n 6 2019-05-30 00:00:00 Siganus sutor    17           18 hook and l… Pwani       \n 7 2019-05-30 00:00:00 Siganus sutor    19           21 Gill net    Pwani       \n 8 2019-06-25 00:00:00 Siganus sutor    36.5         39 Speargun    Pwani       \n 9 2019-06-25 00:00:00 Siganus sutor    11           12 hook and l… Pwani       \n10 2019-06-25 00:00:00 Siganus sutor    11           12 hook and l… Pwani       \n# ℹ 186 more rows\n\n\n\n\n3.3 Select\nSometimes, you want to select specific variables. For instance, We are interested in three variables – date, landing_site, species, size_cm but not so much in the other variables. A select() function dplyr package allows you to do exactly this.\n\nlfq.sample1 %&gt;% \n  select(date, landing_site, species, size_cm)\n\n# A tibble: 779 × 4\n   date                landing_site species           size_cm\n   &lt;dttm&gt;              &lt;chr&gt;        &lt;chr&gt;               &lt;dbl&gt;\n 1 2019-05-30 00:00:00 Pwani        Siganus sutor        16.5\n 2 2019-05-30 00:00:00 Pwani        Siganus sutor        12  \n 3 2019-05-30 00:00:00 Pwani        Siganus sutor        15  \n 4 2019-05-30 00:00:00 Pwani        Siganus sutor        33  \n 5 2019-05-30 00:00:00 Pwani        Siganus sutor        11.2\n 6 2019-05-30 00:00:00 Pwani        Siganus sutor        17  \n 7 2019-05-30 00:00:00 Pwani        Siganus sutor        19  \n 8 2019-05-30 00:00:00 Pwani        Lethrinus mahsena    10  \n 9 2019-05-30 00:00:00 Pwani        Lethrinus mahsena    11  \n10 2019-05-30 00:00:00 Pwani        Lethrinus mahsena     9  \n# ℹ 769 more rows\n\n\n\n\n3.4 Arranging variables\nLet’s say we want to know the species with larges and smallest size throughout the entire dataset. This can be done with the arrange() function.\n\nlfq.sample1 %&gt;% \n  arrange(size_cm)\n\n# A tibble: 779 × 6\n   date                species         size_cm size_class gear_type landing_site\n   &lt;dttm&gt;              &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;       \n 1 2019-07-12 00:00:00 Lethrinus harak     6            6 Traps     Pwani       \n 2 2020-04-08 00:00:00 Lethrinus mahs…     8.3         12 Traps     Bahari      \n 3 2020-04-08 00:00:00 Lethrinus mahs…     8.4         12 Traps     Bahari      \n 4 2019-05-30 00:00:00 Lethrinus mahs…     9           12 Traps     Pwani       \n 5 2019-05-30 00:00:00 Lethrinus mahs…     9           12 Traps     Pwani       \n 6 2020-06-15 00:00:00 Siganus sutor       9           12 Traps     Bahari      \n 7 2020-06-15 00:00:00 Lethrinus harak     9           12 Traps     Bahari      \n 8 2019-05-30 00:00:00 Lethrinus mahs…     9.3         12 hook and… Pwani       \n 9 2019-05-30 00:00:00 Lethrinus mahs…     9.3         12 hook and… Pwani       \n10 2020-04-08 00:00:00 Lethrinus mahs…     9.4         12 Traps     Bahari      \n# ℹ 769 more rows\n\n\nAs we can see, Lethrinus are the species with the smallest size in the datase. The smallest size in the dataset is 6cm. By default, the arrange() function sorts the data in ascending order. To display the data in descending order, we add desc().\n\nlfq.sample1 %&gt;% \n  arrange(desc(size_cm))\n\n# A tibble: 779 × 6\n   date                species         size_cm size_class gear_type landing_site\n   &lt;dttm&gt;              &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;       \n 1 2019-06-25 00:00:00 Siganus sutor      36.5         39 Speargun  Pwani       \n 2 2019-05-30 00:00:00 Siganus sutor      33           33 Speargun  Pwani       \n 3 2020-04-08 00:00:00 Calotomus caro…    30.4         33 Traps     Bahari      \n 4 2020-06-15 00:00:00 Lethrinus mahs…    30.2         33 hook and… Bahari      \n 5 2019-05-30 00:00:00 Leptoscarus va…    30           30 Speargun  Pwani       \n 6 2020-04-08 00:00:00 Anampses spp.      30           30 Speargun  Bahari      \n 7 2019-07-11 00:00:00 Lutjanus fulvi…    27.5         30 hook and… Pwani       \n 8 2019-07-12 00:00:00 Siganus sutor      27.5         30 Gill net  Pwani       \n 9 2019-07-11 00:00:00 Calotomus caro…    27           27 Beach se… Pwani       \n10 2019-07-12 00:00:00 Siganus sutor      27           27 Gill net  Pwani       \n# ℹ 769 more rows\n\n\nAs we can see, Siganus sutor are the species with the larges size in the dataset. The highest size in the dataset is 36.50cm.\n\n\n3.5 Extracting unique observations\nWhich unique species are included in the dataset? To get this information, we use the distinct() function in dplyr.\n\nlfq.sample1 %&gt;% \n  distinct(species)\n\n# A tibble: 9 × 1\n  species               \n  &lt;chr&gt;                 \n1 Siganus sutor         \n2 Lethrinus mahsena     \n3 Leptoscarus vaigiensis\n4 Calotomus carolinus   \n5 Lutjanus fulviflamma  \n6 Lethrinus harak       \n7 Anampses spp.         \n8 Acanthurus chronixis  \n9 Cheilinus chlorourus  \n\n\nAs we see, there are nine species in the dataset arranged in the species variable of the dataframe. However, We might need these species in vector instead of data frame. We use the function pull for that, which convert from dataframe to vector dataset.\n\nlfq.sample1 %&gt;% \n  distinct(species) %&gt;% \n  pull()\n\n[1] \"Siganus sutor\"          \"Lethrinus mahsena\"      \"Leptoscarus vaigiensis\"\n[4] \"Calotomus carolinus\"    \"Lutjanus fulviflamma\"   \"Lethrinus harak\"       \n[7] \"Anampses spp.\"          \"Acanthurus chronixis\"   \"Cheilinus chlorourus\"  \n\n\n\n\n3.6 Creating new variables\nThe mutate() command allows you to generate a new variable from existing. Let’s say you want to generate a day, month and year from date variables. The lubridate packaged (Grolemund and Wickham, 2011) has functions dedicated for dealing with dates, some of these functions include day(), month(), and year() that are used to extract values.\n\nlfq.sample1 %&gt;% \n  select(date, species, size_cm) %&gt;% \n  mutate(\n    year = lubridate::year(date),\n    month = lubridate::month(date),\n    day = lubridate::day(date)\n    )\n\n# A tibble: 779 × 6\n   date                species           size_cm  year month   day\n   &lt;dttm&gt;              &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n 1 2019-05-30 00:00:00 Siganus sutor        16.5  2019     5    30\n 2 2019-05-30 00:00:00 Siganus sutor        12    2019     5    30\n 3 2019-05-30 00:00:00 Siganus sutor        15    2019     5    30\n 4 2019-05-30 00:00:00 Siganus sutor        33    2019     5    30\n 5 2019-05-30 00:00:00 Siganus sutor        11.2  2019     5    30\n 6 2019-05-30 00:00:00 Siganus sutor        17    2019     5    30\n 7 2019-05-30 00:00:00 Siganus sutor        19    2019     5    30\n 8 2019-05-30 00:00:00 Lethrinus mahsena    10    2019     5    30\n 9 2019-05-30 00:00:00 Lethrinus mahsena    11    2019     5    30\n10 2019-05-30 00:00:00 Lethrinus mahsena     9    2019     5    30\n# ℹ 769 more rows\n\n\n\n\n3.7 Group-wise operations\nSuppose we are interested to know the sample size for each species in the dataset along with the mean, median, and standard deviation of the body size. That’s is not possible with single function in the tidyverse but rather a combination of functions. We use a combination of group_by() (to group our results by month), and parse the specific function we want to compute in the summarise() function.\n\nlfq.sample1 %&gt;% \n  group_by(species) %&gt;% \n  summarise(\n    n = n(),\n    mean_cm = mean(size_cm),\n    median_cm = median(size_cm),\n    std_cm = sd(size_cm)\n    ) \n\n# A tibble: 9 × 5\n  species                    n mean_cm median_cm std_cm\n  &lt;chr&gt;                  &lt;int&gt;   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 Acanthurus chronixis      21    13.7      12.5  2.65 \n2 Anampses spp.             17    17.5      16.5  4.43 \n3 Calotomus carolinus      176    16.4      16.5  3.05 \n4 Cheilinus chlorourus       4    13.9      14    0.810\n5 Leptoscarus vaigiensis   146    18.1      17.5  3.53 \n6 Lethrinus harak           37    12.6      12    2.72 \n7 Lethrinus mahsena        135    11.9      11.9  2.39 \n8 Lutjanus fulviflamma      47    16.7      16.9  2.67 \n9 Siganus sutor            196    18.2      17.5  4.66 \n\n\nThe computed variables are grouped after performing transformations. In this case, you can pipe another command with ungroup() to ungroup your result.\n\nlfq.sample1 %&gt;% \n  group_by(species) %&gt;% \n  summarise(\n    n = n(),\n    mean_cm = mean(size_cm),\n    median_cm = median(size_cm),\n    std_cm = sd(size_cm)\n    ) %&gt;% \n  ungroup()\n\n# A tibble: 9 × 5\n  species                    n mean_cm median_cm std_cm\n  &lt;chr&gt;                  &lt;int&gt;   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 Acanthurus chronixis      21    13.7      12.5  2.65 \n2 Anampses spp.             17    17.5      16.5  4.43 \n3 Calotomus carolinus      176    16.4      16.5  3.05 \n4 Cheilinus chlorourus       4    13.9      14    0.810\n5 Leptoscarus vaigiensis   146    18.1      17.5  3.53 \n6 Lethrinus harak           37    12.6      12    2.72 \n7 Lethrinus mahsena        135    11.9      11.9  2.39 \n8 Lutjanus fulviflamma      47    16.7      16.9  2.67 \n9 Siganus sutor            196    18.2      17.5  4.66 \n\n\nIn some cases, you might wish to arrange the sample size from lowest to highest sample size along with the computed values. You just add the arrange function\n\nlfq.sample1 %&gt;% \n  group_by(species) %&gt;% \n  summarise(\n    n = n(),\n    mean_cm = mean(size_cm),\n    median_cm = median(size_cm),\n    std_cm = sd(size_cm)\n    ) %&gt;% \n  arrange(n)\n\n# A tibble: 9 × 5\n  species                    n mean_cm median_cm std_cm\n  &lt;chr&gt;                  &lt;int&gt;   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 Cheilinus chlorourus       4    13.9      14    0.810\n2 Anampses spp.             17    17.5      16.5  4.43 \n3 Acanthurus chronixis      21    13.7      12.5  2.65 \n4 Lethrinus harak           37    12.6      12    2.72 \n5 Lutjanus fulviflamma      47    16.7      16.9  2.67 \n6 Lethrinus mahsena        135    11.9      11.9  2.39 \n7 Leptoscarus vaigiensis   146    18.1      17.5  3.53 \n8 Calotomus carolinus      176    16.4      16.5  3.05 \n9 Siganus sutor            196    18.2      17.5  4.66 \n\n\nYou notice that Cheilinus chlorourus has a only four observation where as Siganus sutor has 196 records in the dataset. You can also explore the summary statistics to see whether the mean and median values are close (indicator of normal distribution) or differ (indication of skewness)"
  },
  {
    "objectID": "posts/ggplot2/index.html",
    "href": "posts/ggplot2/index.html",
    "title": "Visualizing data with grammar of graphics",
    "section": "",
    "text": "The ability to create visualizations—graphical representations of data is an important step to convey results—information and findings to others. Visualizations make it much easier to spot aberrations in data and explain our findings to others. However, we should not reserve data visualizations exclusively for those we share the information with, but rather a practice that help to understand data quickly and particularly during the data exploratory analysis stage.\nR has many systems for visualization and creating plots, some of which are—base R graphics, lattice and ggplot2, but we focus on the use of ggplot2 (Wickham, 2016). ggplot2 is the most popular data visualization package in the R community. It was created by Hadley Wickham in 2005. It was implemented based on Leland Wilkinson’s Grammar of Graphics — a general scheme for data visualization which breaks up graphs into semantic components such as scales and layers (Wickham et al., 2023). While using ggplot2, you provide the data, call specific function, map your desired variables to aesthetics, define graphical arguments, rest it will take care!.\nggplot2 is designed to build graphs layer by layer, where each layer is a building block for your graph. Making graphs in layers is useful because we can think of building up our graphs in separate parts: the data comes first, then the x-axis and y-axis, and finally other components like text labels and graph shapes. When something goes wrong and your ggplot2 code returns an error, you can learn about what’s happening by removing one layer at a time and running it again until the code works properly. Once you know which line is causing the problem, you can focus on fixing it.\nWe’ll use the ggplot2 package, but the function we use to initialize a graph will be ggplot, which works best for data in tidy format (i.e., a column for every variable, and a row for every observation). Graphics with ggplot are built step-by-step, adding new elements as layers with a plus sign (+) between layers (note: this is different from the pipe operator, %&gt;%. Adding layers in this fashion allows for extensive flexibility and customization of plots. In this chapter, our objectives are;\n\nRead in external data (Excel files, CSVs) with readr and readxl\nInitial data exploration\nBuild several common types of graphs (scatterplot, column, line) in ggplot2\nCustomize gg-graph aesthetics (color, style, themes, etc.)\nUpdate axis labels and titles\nCombine compatible graph types (geoms)\nBuild multiseries graphs\nSplit up data into faceted graphs\nExport figures with ggsave()"
  },
  {
    "objectID": "posts/ggplot2/index.html#introduction",
    "href": "posts/ggplot2/index.html#introduction",
    "title": "Visualizing data with grammar of graphics",
    "section": "",
    "text": "The ability to create visualizations—graphical representations of data is an important step to convey results—information and findings to others. Visualizations make it much easier to spot aberrations in data and explain our findings to others. However, we should not reserve data visualizations exclusively for those we share the information with, but rather a practice that help to understand data quickly and particularly during the data exploratory analysis stage.\nR has many systems for visualization and creating plots, some of which are—base R graphics, lattice and ggplot2, but we focus on the use of ggplot2 (Wickham, 2016). ggplot2 is the most popular data visualization package in the R community. It was created by Hadley Wickham in 2005. It was implemented based on Leland Wilkinson’s Grammar of Graphics — a general scheme for data visualization which breaks up graphs into semantic components such as scales and layers (Wickham et al., 2023). While using ggplot2, you provide the data, call specific function, map your desired variables to aesthetics, define graphical arguments, rest it will take care!.\nggplot2 is designed to build graphs layer by layer, where each layer is a building block for your graph. Making graphs in layers is useful because we can think of building up our graphs in separate parts: the data comes first, then the x-axis and y-axis, and finally other components like text labels and graph shapes. When something goes wrong and your ggplot2 code returns an error, you can learn about what’s happening by removing one layer at a time and running it again until the code works properly. Once you know which line is causing the problem, you can focus on fixing it.\nWe’ll use the ggplot2 package, but the function we use to initialize a graph will be ggplot, which works best for data in tidy format (i.e., a column for every variable, and a row for every observation). Graphics with ggplot are built step-by-step, adding new elements as layers with a plus sign (+) between layers (note: this is different from the pipe operator, %&gt;%. Adding layers in this fashion allows for extensive flexibility and customization of plots. In this chapter, our objectives are;\n\nRead in external data (Excel files, CSVs) with readr and readxl\nInitial data exploration\nBuild several common types of graphs (scatterplot, column, line) in ggplot2\nCustomize gg-graph aesthetics (color, style, themes, etc.)\nUpdate axis labels and titles\nCombine compatible graph types (geoms)\nBuild multiseries graphs\nSplit up data into faceted graphs\nExport figures with ggsave()"
  },
  {
    "objectID": "posts/ggplot2/index.html#load-the-packages",
    "href": "posts/ggplot2/index.html#load-the-packages",
    "title": "Visualizing data with grammar of graphics",
    "section": "Load the packages",
    "text": "Load the packages\nThe ggplot2 package is part of the tidyverse (Wickham and Wickham, 2017). tidyverse encapsulates the ‘ggplot2’ along with other packages for data wrangling and data discoveries. so we don’t need to load it separately. Load the tidyverse and readxl packages in the top-most code chunk of your .Rmd file.\n\nlibrary(tidyverse)\nlibrary(readxl)\n\nIn this chapter we will use two datasets. The first dataset is the octopus.csv, a file containing length and weight of octopus fishery gathered between 2018 and 2020 in the coastal waters of Tanzania. The dataset was obtained from the Tanzania Fisheries Research Institute. The second dataset is the landings_wio.csv, which contain historical catch landings from 1950 to 2015 of ten countries in the Western Indian Ocean Region. The dataset was downloaded from rfisheries package and arranged in structure that make plotting easy (Ram et al., 2016). The following script imports the landings_wio_country.csv dataset and displays its first ten rows.\n\nlandings = read_csv(\"../data/tidy/landings_wio_country.csv\", skip = 4)\n\nlandings |&gt; FSA::headtail()\n\n          name year  catch epoch\n1        Kenya 1950  19154  1960\n2        Kenya 1951  21318  1960\n3        Kenya 1952  19126  1960\n658 Madagascar 2013 266953  2010\n659 Madagascar 2014 138478  2010\n660 Madagascar 2015 145629  2010\n\n\nWe can also check the internal structure of the dataset with the glimpse function;\n\nlandings %&gt;% glimpse()\n\nRows: 660\nColumns: 4\n$ name  &lt;chr&gt; \"Kenya\", \"Kenya\", \"Kenya\", \"Kenya\", \"Kenya\", \"Kenya\", \"Kenya\", \"…\n$ year  &lt;dbl&gt; 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960…\n$ catch &lt;dbl&gt; 19154, 21318, 19126, 20989, 17541, 19223, 23297, 28122, 28819, 2…\n$ epoch &lt;dbl&gt; 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960…\n\n\n\nThe ggplot2 basics\nWe will use several functions from the ggplot2 package. These functions work together to yield the desired graphs. Every ggplot2 graph starts with the function ggplot() . It basically creates the coordinate system. Over this the graphical layers are added. The first argument of this function is the input data for the intended graph. Then comes the geom functions which add the layers of plotting on the coordinate system according to its geom i.e. geom_point, geom_line, geom_bar etc.\nEvery geom function needs a mapping argument. This defines how the variables in the dataset are mapped to visual properties. The aesthetic function aes() is assigned to the mapping argument. The main arguments of the aes() function are axes augments-x, y and differentiating arguments like color, size, fill, alpha. The differentiating arguments become common featured arguments when they are put outside of the aes() function. ggtitle(), xlab(), ylab(), theme() these functions are used for labelling and thematic attributes.\n\n\n\n\n\n\nNote\n\n\n\nYou can find detail of these functions in the help tab by executing the command — ?function_name like ?geom_point\n\n\nTo create a bare-bones ggplot graph, we need to tell R three basic things:\n\nWe’re using ggplot2::ggplot()\nData we’re using & variables we’re plotting (i.e., what is x and/or y?)\nWhat type of graph we’re making (the type of geom)\n\nGenerally, that structure will look like this:\nggplot(data = df_name, \naes(x = x_var_name, y = y_var_name)) +\n  geom_type()\nBreaking that down:\n\nFirst, tell R you’re using ggplot()\nThen, tell it the object name where variables exist (data = df_name)\nNext, tell it the aesthetics aes() to specify which variables you want to plot\nThen add a layer for the type of geom (graph type) with geom_*() - for example, geom_point() is a scatterplot, geom_line() is a line graph, geom_col() is a column graph, etc.\n\nLet’s illustrate these concept by creating a line graph of fish catches in the WIO region since 1950s. Since the landings dataset that we just imported has records for ten countries, we need to summarize and get the total landing in the region by years. We can achieve this by a combination of group_by and summarize function as the chunk below highlight\n\nlanding.wio = landings %&gt;% \n  group_by(year) %&gt;% \n  summarise(catch = sum(catch, na.rm = TRUE), .groups = \"drop\") %&gt;% \n  mutate(region = \"WIO\")\n\nTable 1 display the total catch of landing in the WIO region for early 1950s and late 2010s.\n\n\n\n\nTable 1: Annual landings in the WIO region\n\n\n\nyearcatchregion1,950572,743WIO1,951643,877WIO1,952623,655WIO2,0131,780,935WIO2,0141,809,121WIO2,0151,810,705WIO\n\n\n\n\n\nA sample annual landing presented in Table 1 can visually presented in Figure 1, which shows the annual catch landed in the ten countries of the Western Indian Ocean Region.\n\nggplot(data = landing.wio, \n       aes(x = year, y = catch)) +\n  geom_line()+\n  coord_cartesian()+\n  scale_x_continuous(name = \"Years\", breaks = seq(1955,2016,10))+\n  scale_y_continuous(name = \"Landed catch (MT)\", labels = scales::label_number(big.mark = \",\"))+\n  labs(x = \"Years\", \n       y = \"Landed Catches (MT)\", \n       title = \"Capture fisheries trend in the WIO Region\", \n       subtitle = \"The Landed catch today is far lower than 1970s\", \n       caption = \"Coursety: FAO@2022\")+\n  theme(axis.title.x = element_blank())\n\n\n\n\n\n\n\nFigure 1: Annual landings in the WIO region\n\n\n\n\n\nLet’s explore in details the key elements used to make Figure 1:\n\ndata: The data that is in tidy form is core for plotting with ggplot2. It must be a data frame (Table 1) for ggplot2 to read and understand.\naesthetics: is used to map the x and y axis for 2–dimensional plot and add the z value for 3–dimensionla plots. It is also used to define visual properties like color, size, shapes or height etc, and. For instance in the figure Figure 1), the position along the y-axis is mapped to catch and the x - axis is mapped to year values. Other aesthetics—like size, color, shape, and transparency have been left at their default settings.\ngeometry; a layer which define the type of plot you want to make, whether is histogram, boxplot, barplot, scatterplot, lineplot etc.\ncoordinate system: used to set a limit for the plot. The cartesian coordinate is the most familiar and common system that is widely used to zoom the plot and does not change the underlying data.\nscales: scales allows to customize the plot. For instance in Figure 1) both x and y-axis used continuous data and hence the scale_x_continuous() and scale_y_continuous() were used to modiy the values of the axis. For color, I simply stick on scale_colour_discrete() and customize the legend name.\nlabels: The plot is well labelled and easy to understand. It has title, subtitle, axes and caption for the courtesy of the data.\ntheme: the plot stick on the default theme_gray theme, which has a gray background color and white gridlines, a sans serif font family, and a base font size of 11. We can customize all the propoerties in the theme to suit our standard.\n\n\n\nBuilding a plot\nSince you now have a clue of the different layers added to create a plot, its time to work around to create a plot with the ggplot2 package. We use the same landing.wio dataset that used to plot Figure 1. To create a data visualization using ggplot2 package, we will add layers for each of the plot elements described in Section 2.1. I will take you through step by step of the key lines needed to make such a plot. First make sure the ggplot2 or tidyverse packages are loaded in your R’s session. You can load the package with this code;\nThen, to initialize plotting with ggplot2, we must call ggplot() function, which tell R that we are going to make plots, not with plot of base R but with ggplot2\n\nggplot()\n\n\n\n\n\n\n\n\nThe plot above is black with grey background. This is because we have not specified the data and aesthetic arguments inside the ggplot() function. Let’s specify the data, which in our case is the landing.wio and also specify the x-axis with year and y-axis with catch.\n\nggplot(data = landing.wio, \n       aes(x = year, y = catch)) \n\n\n\n\n\n\n\n\nNow the plot has gridlines and axis with values and labels—x-axis show the value of years and the y-axis show the value of catches. However, there is no graphics. This is because we have not added any geom yet. Therefore, since we have already specified the data and the aesthetic values, now we can add the geom where we map the aesthetics to columns in the dataset. Let’s add the geom_line().\n\nggplot(data = landing.wio, \n       aes(x = year, y = catch)) +\n  geom_line()\n\n\n\n\n\n\n\n\nWe’re going to be doing a lot of plot variations with those same variables. Let’s store the first line as object gg.landing so that we don’t need to retype it each time:\n\ngg.landing = ggplot(data = landing.wio, \n       aes(x = year, y = catch)) \n\ngg.landing\n\n\n\n\n\n\n\n\nWe notice that a line plot showing the trend of catch over time. But suppose we are not interested with the line but rather an area plot, then you simply change the geom from geom_line to geom_area\n\ngg.landing +\n  geom_area()\n\n\n\n\n\n\n\n\nOr, we could add the point just by updating the geom_*:\n\ngg.landing+\n  geom_line()+\n  geom_point()\n\n\n\n\n\n\n\n\nWe could even do that for a column graph:\n\ngg.landing +\n  geom_col()\n\n\n\n\n\n\n\n\nWe can see that updating to different geom_* types is quick, so long as the types of graphs we’re switching between are compatible. The data are there, now let’s customize the visualization and make these plots appealing.\n\n\nIntro to customizing ggplot graphs\nFirst, we’ll customize some aesthetics (e.g. colors, styles, axis labels, etc.) of our graphs based on non-variable values.\n\n\n\n\n\n\nNote\n\n\n\nWe can change the aesthetics of elements in a ggplot graph by adding arguments within the layer where that element is created.\n\n\nSome common arguments we’ll use first are:\n\ncolor = or colour =: update point or line colors\nfill =: update fill color for objects with areas\nlinetype =: update the line type (dashed, long dash, etc.)\npch =: update the point style\nsize =: update the element size (e.g. of points or line thickness)\nalpha =: update element opacity (1 = opaque, 0 = transparent)\n\nBuilding on our first line graph, let’s update the line color to “purple” and make the line type “dashed”:\n\ngg.landing +\n  geom_line(\n    color = \"purple\",\n    linetype = \"dashed\"\n  )\n\n\n\n\n\n\n\n\nHow do we know which color names ggplot will recognize? If you google “R colors ggplot2” you’ll find a lot of good resources. Here’s one: SAPE ggplot2 colors quick reference guide\nNow let’s update the point, style and size of points on our previous scatterplot graph using color =, size =, and pch = (see ?pch for the different point styles, which can be further customized).\n\ngg.landing + \n  geom_line() +\n  geom_point(color = \"purple\",\n             pch = 17,\n             size = 4,\n             alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nMapping variables onto aesthetics\nIn the examples above, we have customized aesthetics based on constants that we input as arguments (e.g., the color / style / size isn’t changing based on a variable characteristic or value). Sometimes, however, we do want the aesthetics of a graph to depend on a variable. To do that, we’ll map variables onto graph aesthetics, meaning we’ll change how an element on the graph looks based on a variable characteristic (usually, character or value).\n\n\n\n\n\n\nNote\n\n\n\nWhen we want to customize a graph element based on a variable’s characteristic or value, add the argument within aes() in the appropriate geom_*() layer\n\n\nIn short, if updating aesthetics based on a variable, make sure to put that argument inside of aes().\nExample: Create a ggplot scatterplot graph where the size and color of the points change based on the catch, and make all points the same level of opacity (alpha = 0.5). Notice the aes() around the size = and color = arguments.\nAlso: this is overmapped and unnecessary. Avoid excessive / overcomplicated aesthetic mapping in data visualization.\n\ngg.landing + \n  geom_line() +\n  geom_point(\n    aes(size = catch,\n        color = catch),\n    alpha = 0.5\n  )\n\n\n\n\n\n\n\n\nIn the example above, notice that the two arguments that do depend on variables are within aes(), but since alpha = 0.5 doesn’t depend on a variable then it is outside the aes() but still within the geom_point() layer.\n\n\nActivity: map variables onto graph aesthetics\nCreate a column plot of Channel Islands National Park visitation over time, where the fill color (argument: fill =) changes based on the number of catch.\n\ngg.landing + \n  geom_col(aes(fill = catch))\n\n\n\n\n\n\n\n\n\n\nThemes in ggplot2\nWhile every element of a ggplot graph is manually customizable, there are also built-in themes (theme_*()) that you can add to your ggplot code to make some major headway before making smaller tweaks manually.\nHere are a few to try today (but also notice all the options that appear as we start typing theme_ into our ggplot graph code!):\n\ntheme_light()\ntheme_minimal()\ntheme_bw()\n\nHere, let’s update our previous graph with theme_minimal():\n\ngg.landing +\n  geom_line() +\n  geom_point(\n    aes(size = catch,\n        color = catch),\n    alpha = 0.5\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nAxis labels and titles\nUse labs() to update axis labels, and add a title and/or subtitle to your ggplot graph.\n\ngg.landing +\n  geom_line(linetype = \"dotted\") +\n  theme_bw() +\n  labs(\n    x = \"Year\",\n    y = \"Annual Catch (MT)\",\n    title = \"Total Fisheries Annual Catches\",\n    subtitle = \"In the Western Indian Ocean Region\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you want to update the formatting of axis values (for example, to convert to comma format instead of scientific format above), you can use the scales package options (see more from the R Cookbook).\n\n\n\n\nCombining geoms\nAs long as the geoms are compatible, we can layer them on top of one another to further customize a graph. For example, adding points to a line graph:\n\ngg.landing +\n  geom_line(color = \"purple\") +\n  geom_point(color = \"orange\",\n             aes(size = year),\n             alpha = 0.5)\n\n\n\n\n\n\n\n\nOr, combine a column and line graph (not sure why you’d want to do this, but you can):\n\ngg.landing +\n  geom_col(fill = \"orange\",\n           color = \"purple\") +\n  geom_line(color = \"green\")\n\n\n\n\n\n\n\n\n\n\nMulti-series ggplot graphs\nIn the examples above, we only had a single series - total annual catches in the WIO region. However, we often succum a situaton where we want to visualize multiple series. For example, rather than looking the catch at the region, we may wish to visualize catch over time for individual countries.\nTo do that, we need to add an aesthetic that lets ggplot know how things are going to be grouped. A demonstration of why that’s important - what happens if we don’t let ggplot know how to group things? Unfortunate, the dataset we have loaded is unable to to do that. we need to load into the session a dataset that have catch values for individual countries in the WIO region.\n\nlanding.countries = read_csv(\"../data/tidy/landings_wio_country.csv\", skip = 4)\n\nlanding.countries %&gt;% head()\n\n# A tibble: 6 × 4\n  name   year catch epoch\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Kenya  1950 19154  1960\n2 Kenya  1951 21318  1960\n3 Kenya  1952 19126  1960\n4 Kenya  1953 20989  1960\n5 Kenya  1954 17541  1960\n6 Kenya  1955 19223  1960\n\n\nNotice that the dataset has a name variable that we can use for this case\n\nggplot(data = landing.countries, \n       aes(x = year, y = catch)) +\n  geom_line()\n\n\n\n\n\n\n\n\nWell that’s definitely a mess, and it’s because ggplot has no idea that these should be different series based on the different countries that appear in the name’ column of the dataset. We can make sure R does know by adding an explicit grouping argument (group =), or by updating an aesthetic based on name:\n\nggplot(data = landing.countries, \n       aes(x = year, y = catch, group = name)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou could also add an aesthetic (color = name) in the geom_line() layer to create groupings, instead of in the topmost ggplot() layer.\n\n\nLet’s store that topmost line so that we can use it aesthetic based on name:\n\ngg_np = ggplot(data = landing.countries, \n       aes(x = year, y = catch, group = name)) +\n  geom_line()\n\n\n\nFaceting ggplot graphs\nWhen we facet graphs, we split them up into multiple plotting panels, where each panel contains a subset of the data. In our case, we’ll split the graph above into different panels, each containing annual catches over time for each country in the WIO.\n\ngg_np +\n  geom_line(show.legend = FALSE) +\n  theme_light() + \n  labs(x = \"year\", y = \"Annual Catches (MT)\") +\n  facet_wrap(~ name)\n\n\n\n\n\n\n\n\nUnfortunate, a South Africa plot is the only visible clearly with changes of catches over time, plots for other countries are almost flat because of the catches difference between these countries and South Africa. We can use the scale= \"free\" arguments in geom_facet function to control this behaviour;\n\ngg_np +\n  geom_line(show.legend = FALSE) +\n  theme_light() + \n  labs(x = \"year\", y = \"Annual Catches (MT)\") +\n  facet_wrap(~ name, scales = \"free\", nrow = 3)\n\n\n\n\n\n\n\n\n\n\nExporting a ggplot plots\nIf we want our graph to appear in a knitted html, then we don’t need to do anything else. But often we’ll need a saved image file, of specific size and resolution, to share or for publication. ggsave() will export the most recently run ggplot graph by default (plot = last_plot()), unless you give it the name of a different saved ggplot object. Some common arguments for ggsave():\n\nwidth =: set exported image width (default inches)\nheight =: set exported image height (default height)\ndpi =: set dpi (dots per inch)\n\nSo to export the faceted graph above at 300 dpi, width a width of 8” and a height of 7”, we can use:\n\nggsave(here(\"plots\",\"catch.jpg\"), dpi = 180, width = 8, height = 7)\n\n\nggsave(here(\"plots\", \"catch.png\"), dpi = 180, width = 8, height = 7)\n\n\n\n\n\n\n\nTip\n\n\n\nNotice that a .jpg image of that name and size is now stored in the plots\\ folder within your working directory. You can change the type of exported image, too (e.g. pdf, tiff, eps, png, mmp, svg)."
  },
  {
    "objectID": "posts/dataTypes/index.html",
    "href": "posts/dataTypes/index.html",
    "title": "Understanding and using Data types in R",
    "section": "",
    "text": "In the realm of data science and statistical analysis, mastering data types is akin to understanding the building blocks of a language. In R, a powerful statistical computing language, data types form the foundation upon which all analyses are conducted. From integers to dates, each data type serves a unique purpose and understanding their nuances is critical for efficient and accurate data manipulation.\nUnderstanding and effectively utilizing these core data types in R is essential for performing data analysis, visualization, and modeling tasks. Mastery of data types empowers data scientists to manipulate data efficiently and extract valuable insights from complex datasets. Whether performing arithmetic operations, manipulating text, or handling temporal information, the versatility of R’s data types makes it a powerful tool for data analysis and statistical computing.\nIn this guide, we will delve into the core data types in R, exploring their characteristics and providing illustrative examples. Before we dive in, let pause for a moment and watch video in Figure 1\n\n\n\n\n\n\nFigure 1: Primary data types in R\n\n\n\n\n\nIntegers are whole numbers without any decimal or fractional component. In R, integers are represented by the integer class. They are commonly used for indexing and counting operations.\n\nExample 1  \n# Creating an integer variable\nx &lt;- 5L\nclass(x) # Output: \"integer\"\n\n# Arithmetic operations with integers\ny &lt;- x + 3\n\n\n\n\nNumeric data type, also known as double in other programming languages, represents numbers with decimal points. Numeric data types are used for most mathematical calculations and statistical operations in R.\n\nExample 2  \n# Creating a numeric variable\nheight &lt;- 175.5\nclass(height) # Output: \"numeric\"\n\n# Arithmetic operations with numeric variables\nbmi &lt;- weight / (height^2)\n\n\n\n\nCharacter data type represents textual data such as strings of letters, words, or sentences. In R, character values are enclosed in either single or double quotes.\n\nExample 3  \n# Creating a character variable\nname &lt;- \"John Doe\"\nclass(name) # Output: \"character\"\n\n# Concatenating character strings\ngreeting &lt;- paste(\"Hello\", name)\n\n\n\n\nLogical data type, often referred to as Boolean, represents binary values: TRUE or FALSE. Logical values are fundamental in controlling program flow and making decisions based on conditions.\n\nExample 4  \n# Creating logical variables\nis_adult &lt;- TRUE\nclass(is_adult) # Output: \"logical\"\n\n# Conditional statements with logical variables\nif (is_adult) {\n  print(\"You are an adult.\")\n} else {\n  print(\"You are not an adult.\")\n}\n\n\n\n\nFactor data type is used to represent categorical data in R. Factors are stored as integers with associated labels, making them efficient for statistical modeling and analysis.\n\nExample 5  \n# Creating a factor variable\ngender &lt;- factor(c(\"Male\", \"Female\", \"Female\", \"Male\"))\nclass(gender) # Output: \"factor\"\n\n# Summary statistics with factors\ntable(gender)\n\n\n\n\nDate and time data types are crucial for handling temporal information in R. R provides specialized classes for dates (Date) and date-time values (POSIXct, POSIXlt).\n\nExample 6  \n# Creating a date variable\ntoday &lt;- as.Date(\"2024-04-25\")\nclass(today) # Output: \"Date\"\n\n# Date arithmetic\nnext_week &lt;- today + 7\n\n# Creating a POSIXct variable (date-time)\ncurrent_time &lt;- Sys.time()\nclass(current_time) # Output: \"POSIXct\"\n\nIn this post we learned about different R data types and what kind of data do they hold. Data type is very important concept in programming and can not be ignored. We have explained about each data type with example in this article."
  },
  {
    "objectID": "posts/dataTypes/index.html#introduction",
    "href": "posts/dataTypes/index.html#introduction",
    "title": "Understanding and using Data types in R",
    "section": "",
    "text": "In the realm of data science and statistical analysis, mastering data types is akin to understanding the building blocks of a language. In R, a powerful statistical computing language, data types form the foundation upon which all analyses are conducted. From integers to dates, each data type serves a unique purpose and understanding their nuances is critical for efficient and accurate data manipulation.\nUnderstanding and effectively utilizing these core data types in R is essential for performing data analysis, visualization, and modeling tasks. Mastery of data types empowers data scientists to manipulate data efficiently and extract valuable insights from complex datasets. Whether performing arithmetic operations, manipulating text, or handling temporal information, the versatility of R’s data types makes it a powerful tool for data analysis and statistical computing.\nIn this guide, we will delve into the core data types in R, exploring their characteristics and providing illustrative examples. Before we dive in, let pause for a moment and watch video in Figure 1\n\n\n\n\n\n\nFigure 1: Primary data types in R\n\n\n\n\n\nIntegers are whole numbers without any decimal or fractional component. In R, integers are represented by the integer class. They are commonly used for indexing and counting operations.\n\nExample 1  \n# Creating an integer variable\nx &lt;- 5L\nclass(x) # Output: \"integer\"\n\n# Arithmetic operations with integers\ny &lt;- x + 3\n\n\n\n\nNumeric data type, also known as double in other programming languages, represents numbers with decimal points. Numeric data types are used for most mathematical calculations and statistical operations in R.\n\nExample 2  \n# Creating a numeric variable\nheight &lt;- 175.5\nclass(height) # Output: \"numeric\"\n\n# Arithmetic operations with numeric variables\nbmi &lt;- weight / (height^2)\n\n\n\n\nCharacter data type represents textual data such as strings of letters, words, or sentences. In R, character values are enclosed in either single or double quotes.\n\nExample 3  \n# Creating a character variable\nname &lt;- \"John Doe\"\nclass(name) # Output: \"character\"\n\n# Concatenating character strings\ngreeting &lt;- paste(\"Hello\", name)\n\n\n\n\nLogical data type, often referred to as Boolean, represents binary values: TRUE or FALSE. Logical values are fundamental in controlling program flow and making decisions based on conditions.\n\nExample 4  \n# Creating logical variables\nis_adult &lt;- TRUE\nclass(is_adult) # Output: \"logical\"\n\n# Conditional statements with logical variables\nif (is_adult) {\n  print(\"You are an adult.\")\n} else {\n  print(\"You are not an adult.\")\n}\n\n\n\n\nFactor data type is used to represent categorical data in R. Factors are stored as integers with associated labels, making them efficient for statistical modeling and analysis.\n\nExample 5  \n# Creating a factor variable\ngender &lt;- factor(c(\"Male\", \"Female\", \"Female\", \"Male\"))\nclass(gender) # Output: \"factor\"\n\n# Summary statistics with factors\ntable(gender)\n\n\n\n\nDate and time data types are crucial for handling temporal information in R. R provides specialized classes for dates (Date) and date-time values (POSIXct, POSIXlt).\n\nExample 6  \n# Creating a date variable\ntoday &lt;- as.Date(\"2024-04-25\")\nclass(today) # Output: \"Date\"\n\n# Date arithmetic\nnext_week &lt;- today + 7\n\n# Creating a POSIXct variable (date-time)\ncurrent_time &lt;- Sys.time()\nclass(current_time) # Output: \"POSIXct\"\n\nIn this post we learned about different R data types and what kind of data do they hold. Data type is very important concept in programming and can not be ignored. We have explained about each data type with example in this article."
  },
  {
    "objectID": "posts/dataTypes/index.html#references",
    "href": "posts/dataTypes/index.html#references",
    "title": "Understanding and using Data types in R",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "posts/basicplots/index.html",
    "href": "posts/basicplots/index.html",
    "title": "Basic plots with ggplot2",
    "section": "",
    "text": "The ggplot2 package provides a set of functions that mirror the Grammar of Graphics (Wickham, 2016), enabling you to efficaciously specify what you want a plot to look like. To have a glimpse of ggplot2, we start with five basic types of plots that are familiar to most people. These include: scatterplot, linegraphs, boxplots, histograms, and barplots. The first four graphs works with quantitative data and barplots are appropriate for categorical data.\nThus, understanding the type of data is inevitable before you throw the variable into ggplot2 to make plot for you. In this post, we will cover the most common plot types, such as line plots, histograms, pie charts, scatter plots, and bar plots, along with several other plot types that build upon these.\n\n\nScatterplots are also called bivariate, allows you to visualize the association between two numerical variables. They are among the widely used plot in fisheries science particularly when looking for association between length and weight of a particular fish. Probably you might have come across a scatterplot like the one in Figure 1 that base R was used, but probably you have not made one based on the fundamental theorem of grammar of graphics.\n\n\n\n\n\n\n\n\nFigure 1: Length and weight relationship of Chinook Salmon sampled in Atantic Ocean\n\n\n\n\n\nWe are going to visualize the relationship between length and weight of fish measured in the coastal waters of Kenya. We use the tidy_LFQ_sample_4.csv file. Let’s import the dataset in the session using read_csv function.\n\nlfq4 = read_csv(\"../data/tidy/tidy_LFQ_sample_4.csv\")\n\nThis file contain length and weight measurements along with sex sampled in Mombasa and Voi from March 2016 to September 2020 (Table 1).\n\n\n\n\nTable 1: Sample length and weight of sampled fish\n\n\n\nsitedatetl_mmfl_mmwt_gmsexMombasa2019-04-0518416959.50MMombasa2019-04-0518516954.71MMombasa2019-04-0514513424.15MVoi2020-09-1118917465.88FVoi2020-09-1116214736.35FVoi2020-09-1116815346.13F\n\n\n\n\n\nLet’s now dive into the code of using the *grammar of graphics to create the scatterplot. We use the ggplot() function from ggplot2** package. The code highlighted in the chunk below was used to plot Figure 2\n\nggplot(data = lfq4, aes(x = tl_mm, y = wt_gm))+\n  geom_point()+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")\n\n\n\n\n\n\n\nFigure 2: Length and weight relationship\n\n\n\n\n\nLet’s explore the code above piece-by-piece\n\nThe plotting in ggplot2 begin with ggplot() function, where the two components of grammar of graphics are required. in the data component we specified the dataset by setting data = lfq4. Then the second argument aesthetic that map the plot with coordinate was set by aes(x = tl_mm, y = wt_gm)). In a nutshell, the aes() define the variable – axis specifications.\nWe then added a layer to the ggplot() function call using the + sign. The added layer specify the third part of the *grammar—the geometric component. Because we want to plot scatterplot, the appropriate geom for this case is the geom_point().\nadded a layer labs that allows us to label axis with meaningful axis titles\n\nadding regression line you can simply add the regression line by adding a geom_smooth() layer. However, Figure 2 is non-linear and hence we need to specify the modal that fits the data, the loess model is mostly used for non-linear data. Therefore, we parse the argumentmethod = \"loess\" to draw a non-linear regression line but also parse an argument se = FALSE to prevent plotting confidence error.\n\n  ggplot(data = lfq4, aes(x = tl_mm, y = wt_gm))+\n  geom_point()+\n  geom_smooth(method = \"loess\", se = FALSE)+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")\n\n\n\n\n\n\n\nFigure 3: Length and weight relationship with non-linear regression line\n\n\n\n\n\nIf we want to add a linear regression line i the scatter plot instead of the non linear shown in (ig-scatter2?), we simply replace method = \"loess\" with method = \"lm\"\n\n  ggplot(data = lfq4, aes(x = tl_mm, y = wt_gm))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")\n\n\n\n\n\n\n\nFigure 4: Length and weight relationship with linear regression line\n\n\n\n\n\nThe linear regression line we added in Figure 4 does not fit the data points. That’s is nature of the length and weight measurements of most fishes as their growth is allometric and not isometric. To make use of the linear model in such data points, we often log-transform the data points first and replot. But in ggplot framework, you do need to do that but simply add the scale_x_log10 and scale_y_log10 layer\n\n  ggplot(data = lfq4, aes(x = tl_mm, y = wt_gm))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")+\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\n\n\nFigure 5: Log-transformed length and weight relationship with linear regression line\n\n\n\n\n\nKnowing whether the relationship is positive or negative and whether is linear or non linear is one thing, but people would like to know the strength of the relationship that you have simply presented in Figure 5. Luckily, Pedro Aphalo developed a ggpmisc package (Aphalo, 2016), which extend the statistical function of ggplot2. By simply adding a layer ggpmisc::stat_correlation() in Figure 5, the function generates labels for correlation coefficients and p-value, coefficient of determination (R^2) for method “pearson” and number of observations and add them into the plot (Figure 6).\n\n  ggplot(data = lfq4, aes(x = tl_mm, y = wt_gm))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")+\n  scale_x_log10() +\n  scale_y_log10()+\n  ggpmisc::stat_correlation()\n\n\n\n\n\n\n\nFigure 6: Log-transformed length and weight relationship with linear regression line with correlation coefficient\n\n\n\n\n\nWe might be interested to distinguish the data points and the regression line based on the site. We can do that by adding the color argument in the aesthetic, which change from aes(x = tl_mm, y = wt_gm) to aes(x = tl_mm, y = wt_gm, color = site). The argument color = site will force the data points and the regression line to adhere to colors based on the site but the points and line are plotted on the same plot.\n\n  ggplot(data = lfq4, aes(x = tl_mm, y = wt_gm, color = site))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")+\n  scale_x_log10() +\n  scale_y_log10()+\n  ggpmisc::stat_correlation()\n\n\n\n\n\n\n\nFigure 7: Log-transformed length and weight relationship with linear regression line by site\n\n\n\n\n\nLooking on Figure 7, it is clear that sample from Mombasa station has relatively bigger and heavier fish than those sampled from Voi. But the problem with Figure 7 is that most of the Mombasa data points are masked by Voi data points, which are overlaid on Mombasa data points. We can overcome the issue of point cluttering by simply adding a transparency level in point with alpha = .2.\n\n  ggplot(data = lfq4, aes(x = tl_mm, y = wt_gm, color = site))+\n  geom_point(alpha = .2)+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")+\n  ggpmisc::stat_correlation()+\n  scale_x_log10() +\n  scale_y_log10()+\n  ggpmisc::stat_correlation()\n\n\n\n\n\n\n\nFigure 8: Log-transformed length and weight relationship with linear regression line by site. Points density is highlighted with transparency\n\n\n\n\n\nSometimes you may wish to plot Figure 8 as separate plot shown in Figure 9. That’s is achieved with facet_wrap function, which facet plots based on the levels that are in the variable that is specified. For instance, in our case, the variable chosen is site and there are two sites–Voi and Mombasa. Therefore by simply adding a facet_wrap(~site) layer will force ggplot to make two plots\n\n  ggplot(data = lfq4, aes(x = tl_mm, y = wt_gm))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")+\n  scale_x_log10() +\n  scale_y_log10()+\n  ggpmisc::stat_correlation()+\n  facet_wrap(~site, nrow = 1)\n\n\n\n\n\n\n\nFigure 9: Faceted Log-transformed length and weight relationship with linear regression line\n\n\n\n\n\n\n\n\nThe next basic graph of ggplot2 is the linegraph. Line graphs is similar to drawing points, except that it connects the points with line. often times you don’t show the points. Let’s illustrate how to create linegraphs using catch data in the region. We first load the dataset in the session\n\nlanding.countries = read_csv(\"../data/tidy/landings_wio_country.csv\", skip = 4)\n\nThe landing.countries dataset contain 660 records of landed fisheries catch information recorded between 1950 and 2015 from Somalia, Kenya, Mozambique, South Africa, Madagascar, Mauritius, Seychelles, Mayotte, Tanzania and Zanzibar.\n\nlanding.countries %&gt;% \n  FSA::headtail() |&gt; \n  flextable::flextable() |&gt; \n  flextable::autofit()\n\nnameyearcatchepochKenya1,95019,1541,960Kenya1,95121,3181,960Kenya1,95219,1261,960Madagascar2,013266,9532,010Madagascar2,014138,4782,010Madagascar2,015145,6292,010\n\n\nLinegraphs are used to show time series data. Its inappropriate to use the linegraphs for data that has no clear sequential ordering and should be continuous and not discrete data type. The internal structure of the catch dataset we just loaded indicate that with exception of country’s name, year, catch and epoch are numeric values.\n\nlanding.countries %&gt;% \n  glimpse() \n\nRows: 660\nColumns: 4\n$ name  &lt;chr&gt; \"Kenya\", \"Kenya\", \"Kenya\", \"Kenya\", \"Kenya\", \"Kenya\", \"Kenya\", \"…\n$ year  &lt;dbl&gt; 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960…\n$ catch &lt;dbl&gt; 19154, 21318, 19126, 20989, 17541, 19223, 23297, 28122, 28819, 2…\n$ epoch &lt;dbl&gt; 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960…\n\n\nLet’s us plot the annual landings of fish over the period with ggplot. Like the scatterplot we made earlier, where supply the data frame in data argument and specified the aesthetic mapping with x and y coordinates, but instead of using geom_point(), we use the geom_line(). The code to make the line graph of annual landing in the WIO region shown in Figure 10 is written as;\n\nggplot(data = landing.countries,\n       aes(x = year, y = catch)) +\n  geom_line()+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 10: Annual alnding in the WIO region\n\n\n\n\n\nAlthough we added a geom_line, but we notice that Figure 10 display a plot which we did not expect. The problem is that line for the ten countries are all lumped together and result in the chaotic situation. For illustration purpose, I will use the catch data from Mauritius. Let’s filter Mauritius’ catch information from the landing.countries dataset and display its rows and variables;\n\nmauritius.landings = landing.countries %&gt;% \n  filter(name == \"Mauritius\")\n\n\nmauritius.landings %&gt;% \n  FSA::headtail() |&gt; \n  flextable::flextable() |&gt; \n  flextable::autofit()\n\nnameyearcatchepochMauritius1,950183,0821,960Mauritius1,951216,1511,960Mauritius1,952181,8221,960Mauritius2,01315,7972,010Mauritius2,01413,8792,010Mauritius2,01516,3732,010\n\n\nThere are only 66 rows in Mauritius which are equivalent to 66 records each per year from 1950 to 2015. Let’s use the mauritius.landings dataset to plot\n\nggplot(data = mauritius.landings,\n       aes(x = year, y = catch)) +\n  geom_line()+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 11: Mauritius’ annual landing\n\n\n\n\n\nOften times you find that linegraphs has points. You can also do that in ggplot environment by adding a geom_point layer\n\nggplot(data = mauritius.landings,\n       aes(x = year, y = catch)) +\n  geom_line()+\n  geom_point()+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 12: Mauritius’ annual landing\n\n\n\n\n\nYou can also customize the appearance of the line and point by parsing the color argument in the geom_point and geom_line layers\n\nggplot(data = mauritius.landings,\n       aes(x = year, y = catch)) +\n  geom_line(color = \"black\")+\n  geom_point(color = \"red\")+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 13: Mauritius’ annual landing\n\n\n\n\n\nThe problem we faced in Figure 10 is that catch data for all ten countries were pooled together, and the plot was not informative. But what is we want to compare the trend of catch among the countries. That is achieved by simply distinguishing the color layer for each country. That is done by adding an argument color=name in aes function as the code below highlight\n\nggplot(data = landing.countries,\n       aes(x = year, y = catch, color = name)) +\n  geom_line()+\n  # geom_point(color = \"red\")+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 14: Annual landing by countries in the WIO region\n\n\n\n\n\nThe landings from South Africa is far higher than the rest of the WIO’s countries, which overshadow the appearance of other countries (Figure 14). There several approaches to resolve this issues where some countries have low catch values while others have relatively very high catches. For our case, we have decided to remove South Africa from the plot. We can do that by negating the selection with filter function from dplyr package. By parsing filter(!name == \"South Africa\"), note the exclamation mark before name tell to reverse selection and therefore select all countries except South Africa.\n\nother.countries = landing.countries %&gt;% \n  filter(!name == \"South Africa\")\n\nWe then plot and parse the argument data = other.countries instead of data = landing.countries to make Figure 15.\n\nggplot(data = other.countries,\n       aes(x = year, y = catch, color = name)) +\n  geom_line()+\n  # geom_point(color = \"red\")+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 15: Annual landing by countries in the WIO region with South Africa ommited\n\n\n\n\n\nWe notice that Tanzania and Zanzibar are presented as separate entity. Although the two states report to the FAO separate, but would be interested to know the landing of the combined Tanzania and Zanzibar catches. But before we combine these two states, lets see how their catches vary over the period. First we need to select only records for Tanzania and Zanzibar using a filter function as illustrated below;\n\ntanzania.zanzibar = landing.countries %&gt;% \n  filter(name %in% c(\"Tanzania\", \"Zanzibar\")) \n\nOnce we have created a tanzania.zanzibar object, we can use it to make plots that compare catch trend of Tanzania and Zanzibar over the last 66 years. The code in this chunk is used to make Figure 16\n\n  ggplot(data = tanzania.zanzibar,\n       aes(x = year, y = catch, color = name)) +\n  geom_line()+\n  # geom_point(color = \"red\")+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 16: Annual landing for mainland Tanzania and Zanzibar\n\n\n\n\n\n\nlanding.countries %&gt;% \n  mutate(name = str_replace(string = name, \n                            pattern = \"Zanzibar\", \n                            replacement = \"Tanzania\")) %&gt;% \n  filter(!name == \"South Africa\") %&gt;% \n  group_by(name, year) %&gt;% \n  summarise(catch_new = sum(catch, na.rm = TRUE)) %&gt;% \n  ggplot(\n       aes(x = year, y = catch_new, color = name)) +\n  geom_line()+\n  # geom_point(color = \"red\")+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 17: Annual landing for WIO where mainland Tanzania and Zanzibar are combined\n\n\n\n\n\n\n\n\nThe geom_area method is used to create an area plot. It can be used as a component in the ggplot method. The alpha parameter in the geom_area method is used to depict the opacity of a genome, the value ranges from zero to one integral values. In case, we choose a lower value, this means that a more transparent color version will be chosen to depict the plot and its smoothness. We have used the value for the alpha parameter to be one by two means it is somewhat translucent in nature.\n\n  ggplot(data = tanzania.zanzibar,\n       aes(x = year, y = catch, fill = name)) +\n  geom_area(alpha = 0.6, position=\"identity\")+\n  # geom_point(color = \"red\")+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 18: Area plot showing Annual landing for mainland Tanzania and Zanzibar\n\n\n\n\n\n\n\n\nA histogram is a plot that can be used to examine the shape and spread of continuous data. It looks very similar to a bar graph and organized in intervals or classes. It divides the range of the data into bin equal intervals (also called bins or classes), count the number of observations in each bin, and display the frequency distribution of observations as a bar plot. Such histogram plots provide valuable information on the characteristics of the data, such as the central tendency, the dispersion and the general shape of the distribution. With lfq4 dataset, we can plot the histogram of tl_mm. Since histogram works for single variable that contains quantitative values, you can not bother looking for relationship as we have seen in previous plots, but histogram offers an opportunity to answer question like\n\nWhat are the smallest and largest values of tl_mm?\nWhat is the center value? 3 How does these values spread out?\n\nWe can make a histogram shown in Figure 19 by simply setting aes(x = tl_mm) and add geom_histogram(). Within the geom_histogram(), we simply specify the number of bins bins = 30, fill color for the colum and also the color separating each columns of the histogram with col == \"red\" and fill = \"red\". However, a word of caution regarding histograms—bin size matters. The reproducible code to plot Figure 19 is written as;\n\n  ggplot(data = lfq4,\n       aes(x = tl_mm)) +\n  geom_histogram(bins = 30, fill = \"red\", color = \"red\", alpha = 0.4)+\n  labs(x = \"Total length (mm)\", y = \"Frequency\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 19: Histogram of total length\n\n\n\n\n\nThe resulting histogram gives us an idea of the range of total length of fish we can expect from the sample. You may be interested to compare histogram of the data values sampled from two or sites. For example, in our case, we are interested to compare the distribution of total length using samples collected from Mombasa and Voi sites. We simply add the fill = site argument in the aes function\n\n  ggplot(data = lfq4,\n       aes(x = tl_mm, fill = site)) +\n  geom_histogram(bins = 50, alpha = 0.6)+\n  labs(x = \"Total length (mm)\", y = \"Frequency\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 20: Histogram of total length by sites\n\n\n\n\n\nThe histogram of Mombasa and Voi is plotted as shown in Figure 20, however, despite the transparency level of the bins is set to 0.6 (alpha = .6), yet the bins from Mombasa are masked with bins from Voi. The voi bins are plotted over the Mombasa ones and prevent us to visualize the underneath Mombasa bins. To correct for this issue, we need to parse position = \"identity\"in the geom_bin, which create an different color where the Mombasa and Voi bins are intersected.\n\n  ggplot(data = lfq4,\n       aes(x = tl_mm, fill = site)) +\n  geom_histogram(bins = 50, alpha = 0.6, position = \"identity\")+\n  labs(x = \"Total length (mm)\", y = \"Frequency\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 21: Histogram of total length by sites\n\n\n\n\n\n\n\n\nIt is often useful to visualise the distribution of a numerical variable. Comparing the distributions of different groups can lead to important insights. Visualising distributions is also essential when checking assumptions used for various statistical tests (sometimes called initial data analysis). In this section we will illustrate how this can be done using the diamonds data from the ggplot2 package, which you started to explore in Chapter 2.\nAn advantage with frequency polygons is that they can be used to compare groups, e.g. diamonds with different cuts, without facetting:\n\n  ggplot(data = lfq4,\n       aes(x = tl_mm, color = site)) +\n  geom_freqpoly(alpha = 0.6, position = \"identity\")+\n  labs(x = \"Total length (mm)\", y = \"Frequency\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 22: Frequency polygon of total length by sites\n\n\n\n\n\nIt is clear from this figure that the total length of fish from Voi is larger in size than those from Mombasa. The polygons have roughly the same shape, except the shape of Mombasa a long right tail indicating the presence of outlier points.\n\n\n\nIn some cases, we are more interested in the shape of the distribution than in the actual counts in the different bins. Density plots are similar to frequency polygons but show an estimate of the density function of the underlying random variable. These estimates are smooth curves that are scaled so that the area below them is 1 (i.e. scaled to be proper density functions):\n\n#|\n\n  ggplot(data = lfq4,\n       aes(x = tl_mm, fill = site)) +\n  geom_density(alpha = 0.4, position = \"identity\")+\n  labs(x = \"Total length (mm)\", y = \"Frequency\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 23: Density plot of total length by sites\n\n\n\n\n\nFrom Figure 23, it’s clear that small size fish tend to have better total length, which wasn’t obvious from the frequency polygons. However, the plot does not provide any information about how common different total length are.\n\n\n\nThe boxplot is a standardized way of displaying the distribution of data based on the five number summary: minimum, first quantile, median, third quantile, and maximum. Boxplots are useful for detecting outliers and for comparing distributions. These five number summary also called the 25th percentile, median, and 75th percentile of the quantitative data. The whisker (vertical lines) capture roungly 99% of a distribution, and observation outside this range are plotted as points representing outliers as shown in Figure 24.\n\n\n\n\n\n\n\n\nFigure 24: Conceputal boxplot diagram\n\n\n\n\n\nBoxplots is one of statistical plot that present continuous variable and in ggplot a geom_boxplot() function is dedicated for that. The aes function always have at least two arguments. The first argument should be a categrial variable and the second one is numeric.\n\n  ggplot(data = lfq4,\n       aes(x = site, y = tl_mm)) +\n  geom_boxplot(alpha = 0.6, position = \"identity\")+\n  labs(x = \"Sites\", y = \"Total length (mm)\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 25: Boxplot of total length by sites\n\n\n\n\n\nthe geom_boxplot() has outlier_ arguments that allows to highlight and modify the color, shape, size, alpha … etc of outliers —extreme observation. For instance, you can highlight the outlier with;\n\n  ggplot(data = lfq4,\n       aes(x = site, y = tl_mm, fill = site)) +\n  geom_boxplot(alpha = 0.6, position = \"identity\", outlier.colour = \"red\", outlier.color = )+\n  labs(x = \"Sites\", y = \"Total length (mm)\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 26: Boxplot of total length by sites\n\n\n\n\n\nWe can also map the fill and color to variable in to distinguish boxplot. for example, we can specify the fill = site argument in the aes() to fill the boxplot based on site.\n\n  ggplot(data = lfq4,\n       aes(x = site, y = tl_mm, fill = site)) +\n  geom_boxplot(alpha = 0.6, position = \"identity\", outlier.colour = \"red\", outlier.color = )+\n  labs(x = \"Sites\", y = \"Total length (mm)\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 27: Boxplot of total length and color to distinguish sites\n\n\n\n\n\nWe can add the points on top of the boxplot with the geom_jitter(). It also allows for specifying other arguments like colors and width of the points.\n\n  ggplot(data = lfq4,\n       aes(x = site, y = tl_mm, fill = site)) +\n  geom_boxplot(alpha = 0.6, position = \"identity\", \n               outlier.colour = \"red\", outlier.color = )+\n  geom_jitter(width = .1, height = .5, alpha = 0.1)+\n  labs(x = \"Sites\", y = \"Total length (mm)\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 28: Boxplot with points of total length by sites\n\n\n\n\n\n\n\n\nInstead of using a boxplot, we can use a violin plot. Each group is represented by a “violin”, given by a rotated and duplicated density plot:\n\n  ggplot(data = lfq4,\n       aes(x = site, y = tl_mm, fill = site)) +\n  geom_violin(alpha = 0.6, position = \"identity\")+\n  labs(x = \"Sites\", y = \"Total length (mm)\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 29: Violin of total length by sites\n\n\n\n\n\n\n\n\nBar graphs are perhaps the widely used plot. They are typically used to display count values on the y-axis for different groups on the x-axis. There is an important distinction you should be aware of when making bar graphs. The height of a bar in barplot may represent either the counts or percentage of elements in the dataset. Let’s begin with the former—count. We use the shrimps_cleaned.csv dataset, which contains weight and length of four shrimp species. To access the variable and values of this file we need to load the file using a read_csv function as the code in the chunk below highlight;\n\nshrimp = read_csv(\"../data/tidy/shrimps_cleaned.csv\")\n\nThe sample dataset of shrimp is shown in Table 2. It contain six variables year, season, tide, species, weight (total_wt_kg) and length (tl_mm).\n\n\n\n\nTable 2: Shrimp dataset\n\n\n\n\n\n\n\n\n\n\nyear\nseason\ntide\nspecies\ntotal_wt_kg\ntl_mm\n\n\n\n\n2008\nWET\nSTF\nMetapenaeus monoceros\n2.0\n21\n\n\n2008\nWET\nSTF\nMetapenaeus monoceros\n2.0\n20\n\n\n2008\nWET\nSTF\nMetapenaeus monoceros\n2.0\n19\n\n\n2012\nDRY\nSTN\nPenaeus monodon\n1.7\n12\n\n\n2012\nDRY\nSTN\nFenneropenaeus indicus\n1.7\n14\n\n\n2012\nDRY\nSTN\nPenaeus monodon\n1.7\n11\n\n\n\n\n\n\n\n\n\n\n\nWe realize that the scientific names are too long and may not fit into the plotting area. Therefore, we use a case_when function from dplyr package to change species name and assign it as a new variable called species.short\n\nshrimp = shrimp %&gt;% \n  mutate(species.short = case_when(\n    species == \"Metapenaeus monoceros\"~ \"M.monoceros\",\n    species == \"Penaeus monodon\"~ \"P.monodon\",\n    species == \"Fenneropenaeus indicus\"~ \"F.indicus\",\n    species == \"Penaeus semisulcatus\"~ \"P.semisulcatus\")\n    ) %&gt;% \n  relocate(species.short, .after = species)\n\n\n\nTo make the bar graph that show the number of shrimp per species over the sampling period you you simply specify the the variable species in the x coordinates in the aesthetic and add the geom_bar()\n\nggplot(data = shrimp, aes(x = species.short))+\n  geom_bar()+\n  labs(x = \"Species\", y= \"Frequency\")\n\n\n\n\n\n\n\nFigure 30: Frequency of shrimp species\n\n\n\n\n\nThen to stack the bar based on the sampling season, we add the argument fill = season in aes() part\n\nggplot(data = shrimp, aes(x = species.short, fill = season))+\n  geom_bar()+\n  labs(x = \"Species\", y =\"Frequency\")\n\n\n\n\n\n\n\nFigure 31: Frequency of shrimp species by season\n\n\n\n\n\nYou can flip the order of bar with position = position_stack(reverse = TRUE)\n\nggplot(data = shrimp, aes(x = species.short, fill = season))+\n  geom_bar(position = position_stack(reverse = TRUE))+\n  labs(x = \"Species\", y =\"Frequency\")\n\n\n\n\n\n\n\nFigure 32: Frequency of shrimp species by season\n\n\n\n\n\nInstead of stacking, you can dodge the bar with position = position_dodge() argument\n\nggplot(data = shrimp, aes(x = species.short, fill = season))+\n  geom_bar(position = position_dodge())+\n  labs(x = \"Species\", y =\"Frequency\")\n\n\n\n\n\n\n\nFigure 33: Frequency of shrimp species by season with span\n\n\n\n\n\nWe notice that the species that only appear one season, the count for that species is span across and make the bar wideer than those species occur in both seasons. We can fix that by parsing position = position_dodge(preserve = \"single\") in the geom_bar function\n\nggplot(data = shrimp, aes(x = species.short, fill = season))+\n  geom_bar(position = position_dodge(preserve = \"single\"))+\n  labs(x = \"Species\", y =\"Frequency\")\n\n\n\n\n\n\n\nFigure 34: Frequency of shrimp species by season without span\n\n\n\n\n\nTo add a black stroke color of the bar, add the argument col = \"black\" inside the geom_bar()\n\nggplot(data = shrimp, aes(x = species.short, fill = season))+\n  geom_bar(position = position_dodge(preserve = \"single\"), color = \"black\")+\n  labs(x = \"Species\", y =\"Frequency\")\n\n\n\n\n\n\n\nFigure 35: Frequency of shrimp species by season without span with black bar color\n\n\n\n\n\nAnd to specify the width of the bar you specify a value in width=.75 argument in geom_bar()\n\nggplot(data = shrimp, aes(x = species.short, fill = season))+\n  geom_bar(position = position_dodge(preserve = \"single\"), color = \"black\", width = .75)+\n  labs(x = \"Species\", y =\"Frequency\")\n\n\n\n\n\n\n\nFigure 36: Frequency of shrimp species by season without span with black bar color\n\n\n\n\n\n\n\n\nWe have seen how to make barplot that show the count with geom_bar(). You can also use the barplot to show the values with the geom_col() function and specify what variables you want on the x and y axis. For instance, we want to show percentage of shrimp species by season. Because the geom_col() requires summarized statistics, we need to compute the percentage for each season as the chunk below highlight.\n\nshrimp.pct = shrimp %&gt;% \n  group_by(species.short, season) %&gt;% \n  summarise(n = n()) %&gt;% \n  mutate(pct = n/sum(n), \n         pct = (pct * 100) %&gt;% round(2))\n\nshrimp.pct\n\n# A tibble: 6 × 4\n# Groups:   species.short [4]\n  species.short  season     n   pct\n  &lt;chr&gt;          &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;\n1 F.indicus      DRY      404 100  \n2 M.monoceros    DRY      539  86.9\n3 M.monoceros    WET       81  13.1\n4 P.monodon      DRY      738  52.4\n5 P.monodon      WET      671  47.6\n6 P.semisulcatus DRY      227 100  \n\n\nOnce we have computed the statistics, we can use them to make barplot. Note that unlike the geom_bar(), which need only the x variable, geom_col() requires x and y variables specified. For illustration, we specified the x = species, and y = pct in the aes() to make a barplot that show the percentage of shrimp by season (Figure 37).\n\nggplot(data = shrimp.pct, aes(x = species.short, y = pct, fill = season))+\n  geom_col(position = position_dodge(preserve = \"single\"), color = \"black\", width = .75)+\n  labs(x = \"Species\", y =\"Percentage\")\n\n\n\n\n\n\n\nFigure 37: Percentage of shrimp species by season without span with black bar color\n\n\n\n\n\n\n\n\n\nA pie chart is a disk divided into pie-shaped pieces proportional to the relative frequencies of the classes. To obtain angle for any class, we multiply the relative frequencies by 360 degree, which corresponds to the complete circle. Either variables or attributes can be portrayed in this manner, but a pie chart is especially useful for attributes. A pie diagram for contribution of different fish groups/species to the total fish landings at a landing site of a river is shown in Figure 38.\n\nshrimp %&gt;% \n  group_by(species) %&gt;% \n  summarise(n = n()) %&gt;% \n  mutate(pct = round(n/sum(n)*100), 2) %&gt;% \n  mutate(species = str_replace(string = species, pattern = \" \", replacement = \"\\n\")) %&gt;% \n  mutate(label = paste0(\"(\",pct,\"%\",\")\")) %&gt;% \n  ggpubr::ggpie(x = \"pct\", label = \"label\", fill = \"species\", lab.pos = \"in\", palette = \"jama\", color = \"ivory\", ggtheme = theme_void(), )\n\n\n\n\n\n\n\nFigure 38: Percentage composition of prawn species\n\n\n\n\n\nAn extended pie chart is donut shown in Figure 39\n\nshrimp %&gt;% \n  group_by(species) %&gt;% \n  summarise(n = n()) %&gt;% \n  mutate(pct = round(n/sum(n)*100), 2) %&gt;% \n  mutate(species = str_replace(string = species, pattern = \" \", replacement = \"\\n\")) %&gt;% \n  mutate(label = paste0(\"(\",pct,\"%\",\")\")) %&gt;% \n  ggpubr::ggdonutchart(x = \"pct\", label = \"label\", fill = \"species\", lab.pos = \"in\", palette = \"jama\", color = \"ivory\", ggtheme = theme_void())\n\n\n\n\n\n\n\nFigure 39: Percentage composition of prawn species\n\n\n\n\n\n\n\n\n\nshrimp %&gt;% \n  group_by(species.short) %&gt;% \n  count() %&gt;% \n  arrange(-n) %&gt;% \n  ggplot(aes(x = reorder(species.short,n), y = n, \n             fill = species.short), stat = \"identity\")+\n  geom_col() +\n  coord_polar(theta = \"y\")+\n  theme_bw() +\n  theme(axis.title = element_blank(), legend.position = \"right\", axis.text.y = element_blank(), axis.ticks = element_blank())+\n  scale_fill_brewer(palette = \"Set2\", name = \"Species\")\n\n\n\n\n\n\n\nFigure 40: Barplot with polar transformation\n\n\n\n\n\n\nshrimp %&gt;% \n  ggplot() +\n  geom_bar(aes(x = tide, fill = species.short),\n           color = \"ivory\") +\n  labs(x = \"Tide\", y = \"Count\") +\n  coord_polar()+\n  theme_bw() +\n  theme(axis.title = element_blank(), \n        legend.position = \"right\")+\n  scale_fill_brewer(palette = \"Set2\", name = \"Species\")\n\n\n\n\n\n\n\nFigure 41: Stacked barplot with polar transformation\n\n\n\n\n\n\n\n\nAlthough the ggridges package provides geom_ridgeline and geom_density_ridges, we focus on the latter because it has ability to estimates data densities and then draws those using ridgelines.The geom geom_density_ridges calculates density estimates from the provided data and then plots those, using the ridgeline visualization.\n\nlfq4 %&gt;% \n  mutate(months = lubridate::month(date, label = TRUE)) %&gt;%\n  ggplot()+\n  ggridges::geom_density_ridges(aes(x = tl_mm, y = months, fill = site), alpha = .7)+\n  scale_fill_brewer(palette = \"Set2\", name = \"Sampling\\nsite\")+\n  theme_minimal()+\n  theme(legend.position = c(.85,.2), legend.background = element_rect())+\n  labs(y = \"Months\", x = \"Total length (mm.)\")"
  },
  {
    "objectID": "posts/basicplots/index.html#introduction",
    "href": "posts/basicplots/index.html#introduction",
    "title": "Basic plots with ggplot2",
    "section": "",
    "text": "The ggplot2 package provides a set of functions that mirror the Grammar of Graphics (Wickham, 2016), enabling you to efficaciously specify what you want a plot to look like. To have a glimpse of ggplot2, we start with five basic types of plots that are familiar to most people. These include: scatterplot, linegraphs, boxplots, histograms, and barplots. The first four graphs works with quantitative data and barplots are appropriate for categorical data.\nThus, understanding the type of data is inevitable before you throw the variable into ggplot2 to make plot for you. In this post, we will cover the most common plot types, such as line plots, histograms, pie charts, scatter plots, and bar plots, along with several other plot types that build upon these.\n\n\nScatterplots are also called bivariate, allows you to visualize the association between two numerical variables. They are among the widely used plot in fisheries science particularly when looking for association between length and weight of a particular fish. Probably you might have come across a scatterplot like the one in Figure 1 that base R was used, but probably you have not made one based on the fundamental theorem of grammar of graphics.\n\n\n\n\n\n\n\n\nFigure 1: Length and weight relationship of Chinook Salmon sampled in Atantic Ocean\n\n\n\n\n\nWe are going to visualize the relationship between length and weight of fish measured in the coastal waters of Kenya. We use the tidy_LFQ_sample_4.csv file. Let’s import the dataset in the session using read_csv function.\n\nlfq4 = read_csv(\"../data/tidy/tidy_LFQ_sample_4.csv\")\n\nThis file contain length and weight measurements along with sex sampled in Mombasa and Voi from March 2016 to September 2020 (Table 1).\n\n\n\n\nTable 1: Sample length and weight of sampled fish\n\n\n\nsitedatetl_mmfl_mmwt_gmsexMombasa2019-04-0518416959.50MMombasa2019-04-0518516954.71MMombasa2019-04-0514513424.15MVoi2020-09-1118917465.88FVoi2020-09-1116214736.35FVoi2020-09-1116815346.13F\n\n\n\n\n\nLet’s now dive into the code of using the *grammar of graphics to create the scatterplot. We use the ggplot() function from ggplot2** package. The code highlighted in the chunk below was used to plot Figure 2\n\nggplot(data = lfq4, aes(x = tl_mm, y = wt_gm))+\n  geom_point()+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")\n\n\n\n\n\n\n\nFigure 2: Length and weight relationship\n\n\n\n\n\nLet’s explore the code above piece-by-piece\n\nThe plotting in ggplot2 begin with ggplot() function, where the two components of grammar of graphics are required. in the data component we specified the dataset by setting data = lfq4. Then the second argument aesthetic that map the plot with coordinate was set by aes(x = tl_mm, y = wt_gm)). In a nutshell, the aes() define the variable – axis specifications.\nWe then added a layer to the ggplot() function call using the + sign. The added layer specify the third part of the *grammar—the geometric component. Because we want to plot scatterplot, the appropriate geom for this case is the geom_point().\nadded a layer labs that allows us to label axis with meaningful axis titles\n\nadding regression line you can simply add the regression line by adding a geom_smooth() layer. However, Figure 2 is non-linear and hence we need to specify the modal that fits the data, the loess model is mostly used for non-linear data. Therefore, we parse the argumentmethod = \"loess\" to draw a non-linear regression line but also parse an argument se = FALSE to prevent plotting confidence error.\n\n  ggplot(data = lfq4, aes(x = tl_mm, y = wt_gm))+\n  geom_point()+\n  geom_smooth(method = \"loess\", se = FALSE)+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")\n\n\n\n\n\n\n\nFigure 3: Length and weight relationship with non-linear regression line\n\n\n\n\n\nIf we want to add a linear regression line i the scatter plot instead of the non linear shown in (ig-scatter2?), we simply replace method = \"loess\" with method = \"lm\"\n\n  ggplot(data = lfq4, aes(x = tl_mm, y = wt_gm))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")\n\n\n\n\n\n\n\nFigure 4: Length and weight relationship with linear regression line\n\n\n\n\n\nThe linear regression line we added in Figure 4 does not fit the data points. That’s is nature of the length and weight measurements of most fishes as their growth is allometric and not isometric. To make use of the linear model in such data points, we often log-transform the data points first and replot. But in ggplot framework, you do need to do that but simply add the scale_x_log10 and scale_y_log10 layer\n\n  ggplot(data = lfq4, aes(x = tl_mm, y = wt_gm))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")+\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\n\n\nFigure 5: Log-transformed length and weight relationship with linear regression line\n\n\n\n\n\nKnowing whether the relationship is positive or negative and whether is linear or non linear is one thing, but people would like to know the strength of the relationship that you have simply presented in Figure 5. Luckily, Pedro Aphalo developed a ggpmisc package (Aphalo, 2016), which extend the statistical function of ggplot2. By simply adding a layer ggpmisc::stat_correlation() in Figure 5, the function generates labels for correlation coefficients and p-value, coefficient of determination (R^2) for method “pearson” and number of observations and add them into the plot (Figure 6).\n\n  ggplot(data = lfq4, aes(x = tl_mm, y = wt_gm))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")+\n  scale_x_log10() +\n  scale_y_log10()+\n  ggpmisc::stat_correlation()\n\n\n\n\n\n\n\nFigure 6: Log-transformed length and weight relationship with linear regression line with correlation coefficient\n\n\n\n\n\nWe might be interested to distinguish the data points and the regression line based on the site. We can do that by adding the color argument in the aesthetic, which change from aes(x = tl_mm, y = wt_gm) to aes(x = tl_mm, y = wt_gm, color = site). The argument color = site will force the data points and the regression line to adhere to colors based on the site but the points and line are plotted on the same plot.\n\n  ggplot(data = lfq4, aes(x = tl_mm, y = wt_gm, color = site))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")+\n  scale_x_log10() +\n  scale_y_log10()+\n  ggpmisc::stat_correlation()\n\n\n\n\n\n\n\nFigure 7: Log-transformed length and weight relationship with linear regression line by site\n\n\n\n\n\nLooking on Figure 7, it is clear that sample from Mombasa station has relatively bigger and heavier fish than those sampled from Voi. But the problem with Figure 7 is that most of the Mombasa data points are masked by Voi data points, which are overlaid on Mombasa data points. We can overcome the issue of point cluttering by simply adding a transparency level in point with alpha = .2.\n\n  ggplot(data = lfq4, aes(x = tl_mm, y = wt_gm, color = site))+\n  geom_point(alpha = .2)+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")+\n  ggpmisc::stat_correlation()+\n  scale_x_log10() +\n  scale_y_log10()+\n  ggpmisc::stat_correlation()\n\n\n\n\n\n\n\nFigure 8: Log-transformed length and weight relationship with linear regression line by site. Points density is highlighted with transparency\n\n\n\n\n\nSometimes you may wish to plot Figure 8 as separate plot shown in Figure 9. That’s is achieved with facet_wrap function, which facet plots based on the levels that are in the variable that is specified. For instance, in our case, the variable chosen is site and there are two sites–Voi and Mombasa. Therefore by simply adding a facet_wrap(~site) layer will force ggplot to make two plots\n\n  ggplot(data = lfq4, aes(x = tl_mm, y = wt_gm))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  labs(x = \"Total length (mm)\", y = \"Weight (gm)\")+\n  scale_x_log10() +\n  scale_y_log10()+\n  ggpmisc::stat_correlation()+\n  facet_wrap(~site, nrow = 1)\n\n\n\n\n\n\n\nFigure 9: Faceted Log-transformed length and weight relationship with linear regression line\n\n\n\n\n\n\n\n\nThe next basic graph of ggplot2 is the linegraph. Line graphs is similar to drawing points, except that it connects the points with line. often times you don’t show the points. Let’s illustrate how to create linegraphs using catch data in the region. We first load the dataset in the session\n\nlanding.countries = read_csv(\"../data/tidy/landings_wio_country.csv\", skip = 4)\n\nThe landing.countries dataset contain 660 records of landed fisheries catch information recorded between 1950 and 2015 from Somalia, Kenya, Mozambique, South Africa, Madagascar, Mauritius, Seychelles, Mayotte, Tanzania and Zanzibar.\n\nlanding.countries %&gt;% \n  FSA::headtail() |&gt; \n  flextable::flextable() |&gt; \n  flextable::autofit()\n\nnameyearcatchepochKenya1,95019,1541,960Kenya1,95121,3181,960Kenya1,95219,1261,960Madagascar2,013266,9532,010Madagascar2,014138,4782,010Madagascar2,015145,6292,010\n\n\nLinegraphs are used to show time series data. Its inappropriate to use the linegraphs for data that has no clear sequential ordering and should be continuous and not discrete data type. The internal structure of the catch dataset we just loaded indicate that with exception of country’s name, year, catch and epoch are numeric values.\n\nlanding.countries %&gt;% \n  glimpse() \n\nRows: 660\nColumns: 4\n$ name  &lt;chr&gt; \"Kenya\", \"Kenya\", \"Kenya\", \"Kenya\", \"Kenya\", \"Kenya\", \"Kenya\", \"…\n$ year  &lt;dbl&gt; 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960…\n$ catch &lt;dbl&gt; 19154, 21318, 19126, 20989, 17541, 19223, 23297, 28122, 28819, 2…\n$ epoch &lt;dbl&gt; 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960…\n\n\nLet’s us plot the annual landings of fish over the period with ggplot. Like the scatterplot we made earlier, where supply the data frame in data argument and specified the aesthetic mapping with x and y coordinates, but instead of using geom_point(), we use the geom_line(). The code to make the line graph of annual landing in the WIO region shown in Figure 10 is written as;\n\nggplot(data = landing.countries,\n       aes(x = year, y = catch)) +\n  geom_line()+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 10: Annual alnding in the WIO region\n\n\n\n\n\nAlthough we added a geom_line, but we notice that Figure 10 display a plot which we did not expect. The problem is that line for the ten countries are all lumped together and result in the chaotic situation. For illustration purpose, I will use the catch data from Mauritius. Let’s filter Mauritius’ catch information from the landing.countries dataset and display its rows and variables;\n\nmauritius.landings = landing.countries %&gt;% \n  filter(name == \"Mauritius\")\n\n\nmauritius.landings %&gt;% \n  FSA::headtail() |&gt; \n  flextable::flextable() |&gt; \n  flextable::autofit()\n\nnameyearcatchepochMauritius1,950183,0821,960Mauritius1,951216,1511,960Mauritius1,952181,8221,960Mauritius2,01315,7972,010Mauritius2,01413,8792,010Mauritius2,01516,3732,010\n\n\nThere are only 66 rows in Mauritius which are equivalent to 66 records each per year from 1950 to 2015. Let’s use the mauritius.landings dataset to plot\n\nggplot(data = mauritius.landings,\n       aes(x = year, y = catch)) +\n  geom_line()+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 11: Mauritius’ annual landing\n\n\n\n\n\nOften times you find that linegraphs has points. You can also do that in ggplot environment by adding a geom_point layer\n\nggplot(data = mauritius.landings,\n       aes(x = year, y = catch)) +\n  geom_line()+\n  geom_point()+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 12: Mauritius’ annual landing\n\n\n\n\n\nYou can also customize the appearance of the line and point by parsing the color argument in the geom_point and geom_line layers\n\nggplot(data = mauritius.landings,\n       aes(x = year, y = catch)) +\n  geom_line(color = \"black\")+\n  geom_point(color = \"red\")+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 13: Mauritius’ annual landing\n\n\n\n\n\nThe problem we faced in Figure 10 is that catch data for all ten countries were pooled together, and the plot was not informative. But what is we want to compare the trend of catch among the countries. That is achieved by simply distinguishing the color layer for each country. That is done by adding an argument color=name in aes function as the code below highlight\n\nggplot(data = landing.countries,\n       aes(x = year, y = catch, color = name)) +\n  geom_line()+\n  # geom_point(color = \"red\")+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 14: Annual landing by countries in the WIO region\n\n\n\n\n\nThe landings from South Africa is far higher than the rest of the WIO’s countries, which overshadow the appearance of other countries (Figure 14). There several approaches to resolve this issues where some countries have low catch values while others have relatively very high catches. For our case, we have decided to remove South Africa from the plot. We can do that by negating the selection with filter function from dplyr package. By parsing filter(!name == \"South Africa\"), note the exclamation mark before name tell to reverse selection and therefore select all countries except South Africa.\n\nother.countries = landing.countries %&gt;% \n  filter(!name == \"South Africa\")\n\nWe then plot and parse the argument data = other.countries instead of data = landing.countries to make Figure 15.\n\nggplot(data = other.countries,\n       aes(x = year, y = catch, color = name)) +\n  geom_line()+\n  # geom_point(color = \"red\")+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 15: Annual landing by countries in the WIO region with South Africa ommited\n\n\n\n\n\nWe notice that Tanzania and Zanzibar are presented as separate entity. Although the two states report to the FAO separate, but would be interested to know the landing of the combined Tanzania and Zanzibar catches. But before we combine these two states, lets see how their catches vary over the period. First we need to select only records for Tanzania and Zanzibar using a filter function as illustrated below;\n\ntanzania.zanzibar = landing.countries %&gt;% \n  filter(name %in% c(\"Tanzania\", \"Zanzibar\")) \n\nOnce we have created a tanzania.zanzibar object, we can use it to make plots that compare catch trend of Tanzania and Zanzibar over the last 66 years. The code in this chunk is used to make Figure 16\n\n  ggplot(data = tanzania.zanzibar,\n       aes(x = year, y = catch, color = name)) +\n  geom_line()+\n  # geom_point(color = \"red\")+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 16: Annual landing for mainland Tanzania and Zanzibar\n\n\n\n\n\n\nlanding.countries %&gt;% \n  mutate(name = str_replace(string = name, \n                            pattern = \"Zanzibar\", \n                            replacement = \"Tanzania\")) %&gt;% \n  filter(!name == \"South Africa\") %&gt;% \n  group_by(name, year) %&gt;% \n  summarise(catch_new = sum(catch, na.rm = TRUE)) %&gt;% \n  ggplot(\n       aes(x = year, y = catch_new, color = name)) +\n  geom_line()+\n  # geom_point(color = \"red\")+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 17: Annual landing for WIO where mainland Tanzania and Zanzibar are combined\n\n\n\n\n\n\n\n\nThe geom_area method is used to create an area plot. It can be used as a component in the ggplot method. The alpha parameter in the geom_area method is used to depict the opacity of a genome, the value ranges from zero to one integral values. In case, we choose a lower value, this means that a more transparent color version will be chosen to depict the plot and its smoothness. We have used the value for the alpha parameter to be one by two means it is somewhat translucent in nature.\n\n  ggplot(data = tanzania.zanzibar,\n       aes(x = year, y = catch, fill = name)) +\n  geom_area(alpha = 0.6, position=\"identity\")+\n  # geom_point(color = \"red\")+\n  labs(x = \"Year\", y = \"Annual catch (MT)\")\n\n\n\n\n\n\n\nFigure 18: Area plot showing Annual landing for mainland Tanzania and Zanzibar\n\n\n\n\n\n\n\n\nA histogram is a plot that can be used to examine the shape and spread of continuous data. It looks very similar to a bar graph and organized in intervals or classes. It divides the range of the data into bin equal intervals (also called bins or classes), count the number of observations in each bin, and display the frequency distribution of observations as a bar plot. Such histogram plots provide valuable information on the characteristics of the data, such as the central tendency, the dispersion and the general shape of the distribution. With lfq4 dataset, we can plot the histogram of tl_mm. Since histogram works for single variable that contains quantitative values, you can not bother looking for relationship as we have seen in previous plots, but histogram offers an opportunity to answer question like\n\nWhat are the smallest and largest values of tl_mm?\nWhat is the center value? 3 How does these values spread out?\n\nWe can make a histogram shown in Figure 19 by simply setting aes(x = tl_mm) and add geom_histogram(). Within the geom_histogram(), we simply specify the number of bins bins = 30, fill color for the colum and also the color separating each columns of the histogram with col == \"red\" and fill = \"red\". However, a word of caution regarding histograms—bin size matters. The reproducible code to plot Figure 19 is written as;\n\n  ggplot(data = lfq4,\n       aes(x = tl_mm)) +\n  geom_histogram(bins = 30, fill = \"red\", color = \"red\", alpha = 0.4)+\n  labs(x = \"Total length (mm)\", y = \"Frequency\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 19: Histogram of total length\n\n\n\n\n\nThe resulting histogram gives us an idea of the range of total length of fish we can expect from the sample. You may be interested to compare histogram of the data values sampled from two or sites. For example, in our case, we are interested to compare the distribution of total length using samples collected from Mombasa and Voi sites. We simply add the fill = site argument in the aes function\n\n  ggplot(data = lfq4,\n       aes(x = tl_mm, fill = site)) +\n  geom_histogram(bins = 50, alpha = 0.6)+\n  labs(x = \"Total length (mm)\", y = \"Frequency\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 20: Histogram of total length by sites\n\n\n\n\n\nThe histogram of Mombasa and Voi is plotted as shown in Figure 20, however, despite the transparency level of the bins is set to 0.6 (alpha = .6), yet the bins from Mombasa are masked with bins from Voi. The voi bins are plotted over the Mombasa ones and prevent us to visualize the underneath Mombasa bins. To correct for this issue, we need to parse position = \"identity\"in the geom_bin, which create an different color where the Mombasa and Voi bins are intersected.\n\n  ggplot(data = lfq4,\n       aes(x = tl_mm, fill = site)) +\n  geom_histogram(bins = 50, alpha = 0.6, position = \"identity\")+\n  labs(x = \"Total length (mm)\", y = \"Frequency\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 21: Histogram of total length by sites\n\n\n\n\n\n\n\n\nIt is often useful to visualise the distribution of a numerical variable. Comparing the distributions of different groups can lead to important insights. Visualising distributions is also essential when checking assumptions used for various statistical tests (sometimes called initial data analysis). In this section we will illustrate how this can be done using the diamonds data from the ggplot2 package, which you started to explore in Chapter 2.\nAn advantage with frequency polygons is that they can be used to compare groups, e.g. diamonds with different cuts, without facetting:\n\n  ggplot(data = lfq4,\n       aes(x = tl_mm, color = site)) +\n  geom_freqpoly(alpha = 0.6, position = \"identity\")+\n  labs(x = \"Total length (mm)\", y = \"Frequency\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 22: Frequency polygon of total length by sites\n\n\n\n\n\nIt is clear from this figure that the total length of fish from Voi is larger in size than those from Mombasa. The polygons have roughly the same shape, except the shape of Mombasa a long right tail indicating the presence of outlier points.\n\n\n\nIn some cases, we are more interested in the shape of the distribution than in the actual counts in the different bins. Density plots are similar to frequency polygons but show an estimate of the density function of the underlying random variable. These estimates are smooth curves that are scaled so that the area below them is 1 (i.e. scaled to be proper density functions):\n\n#|\n\n  ggplot(data = lfq4,\n       aes(x = tl_mm, fill = site)) +\n  geom_density(alpha = 0.4, position = \"identity\")+\n  labs(x = \"Total length (mm)\", y = \"Frequency\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 23: Density plot of total length by sites\n\n\n\n\n\nFrom Figure 23, it’s clear that small size fish tend to have better total length, which wasn’t obvious from the frequency polygons. However, the plot does not provide any information about how common different total length are.\n\n\n\nThe boxplot is a standardized way of displaying the distribution of data based on the five number summary: minimum, first quantile, median, third quantile, and maximum. Boxplots are useful for detecting outliers and for comparing distributions. These five number summary also called the 25th percentile, median, and 75th percentile of the quantitative data. The whisker (vertical lines) capture roungly 99% of a distribution, and observation outside this range are plotted as points representing outliers as shown in Figure 24.\n\n\n\n\n\n\n\n\nFigure 24: Conceputal boxplot diagram\n\n\n\n\n\nBoxplots is one of statistical plot that present continuous variable and in ggplot a geom_boxplot() function is dedicated for that. The aes function always have at least two arguments. The first argument should be a categrial variable and the second one is numeric.\n\n  ggplot(data = lfq4,\n       aes(x = site, y = tl_mm)) +\n  geom_boxplot(alpha = 0.6, position = \"identity\")+\n  labs(x = \"Sites\", y = \"Total length (mm)\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 25: Boxplot of total length by sites\n\n\n\n\n\nthe geom_boxplot() has outlier_ arguments that allows to highlight and modify the color, shape, size, alpha … etc of outliers —extreme observation. For instance, you can highlight the outlier with;\n\n  ggplot(data = lfq4,\n       aes(x = site, y = tl_mm, fill = site)) +\n  geom_boxplot(alpha = 0.6, position = \"identity\", outlier.colour = \"red\", outlier.color = )+\n  labs(x = \"Sites\", y = \"Total length (mm)\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 26: Boxplot of total length by sites\n\n\n\n\n\nWe can also map the fill and color to variable in to distinguish boxplot. for example, we can specify the fill = site argument in the aes() to fill the boxplot based on site.\n\n  ggplot(data = lfq4,\n       aes(x = site, y = tl_mm, fill = site)) +\n  geom_boxplot(alpha = 0.6, position = \"identity\", outlier.colour = \"red\", outlier.color = )+\n  labs(x = \"Sites\", y = \"Total length (mm)\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 27: Boxplot of total length and color to distinguish sites\n\n\n\n\n\nWe can add the points on top of the boxplot with the geom_jitter(). It also allows for specifying other arguments like colors and width of the points.\n\n  ggplot(data = lfq4,\n       aes(x = site, y = tl_mm, fill = site)) +\n  geom_boxplot(alpha = 0.6, position = \"identity\", \n               outlier.colour = \"red\", outlier.color = )+\n  geom_jitter(width = .1, height = .5, alpha = 0.1)+\n  labs(x = \"Sites\", y = \"Total length (mm)\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 28: Boxplot with points of total length by sites\n\n\n\n\n\n\n\n\nInstead of using a boxplot, we can use a violin plot. Each group is represented by a “violin”, given by a rotated and duplicated density plot:\n\n  ggplot(data = lfq4,\n       aes(x = site, y = tl_mm, fill = site)) +\n  geom_violin(alpha = 0.6, position = \"identity\")+\n  labs(x = \"Sites\", y = \"Total length (mm)\")+\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 29: Violin of total length by sites\n\n\n\n\n\n\n\n\nBar graphs are perhaps the widely used plot. They are typically used to display count values on the y-axis for different groups on the x-axis. There is an important distinction you should be aware of when making bar graphs. The height of a bar in barplot may represent either the counts or percentage of elements in the dataset. Let’s begin with the former—count. We use the shrimps_cleaned.csv dataset, which contains weight and length of four shrimp species. To access the variable and values of this file we need to load the file using a read_csv function as the code in the chunk below highlight;\n\nshrimp = read_csv(\"../data/tidy/shrimps_cleaned.csv\")\n\nThe sample dataset of shrimp is shown in Table 2. It contain six variables year, season, tide, species, weight (total_wt_kg) and length (tl_mm).\n\n\n\n\nTable 2: Shrimp dataset\n\n\n\n\n\n\n\n\n\n\nyear\nseason\ntide\nspecies\ntotal_wt_kg\ntl_mm\n\n\n\n\n2008\nWET\nSTF\nMetapenaeus monoceros\n2.0\n21\n\n\n2008\nWET\nSTF\nMetapenaeus monoceros\n2.0\n20\n\n\n2008\nWET\nSTF\nMetapenaeus monoceros\n2.0\n19\n\n\n2012\nDRY\nSTN\nPenaeus monodon\n1.7\n12\n\n\n2012\nDRY\nSTN\nFenneropenaeus indicus\n1.7\n14\n\n\n2012\nDRY\nSTN\nPenaeus monodon\n1.7\n11\n\n\n\n\n\n\n\n\n\n\n\nWe realize that the scientific names are too long and may not fit into the plotting area. Therefore, we use a case_when function from dplyr package to change species name and assign it as a new variable called species.short\n\nshrimp = shrimp %&gt;% \n  mutate(species.short = case_when(\n    species == \"Metapenaeus monoceros\"~ \"M.monoceros\",\n    species == \"Penaeus monodon\"~ \"P.monodon\",\n    species == \"Fenneropenaeus indicus\"~ \"F.indicus\",\n    species == \"Penaeus semisulcatus\"~ \"P.semisulcatus\")\n    ) %&gt;% \n  relocate(species.short, .after = species)\n\n\n\nTo make the bar graph that show the number of shrimp per species over the sampling period you you simply specify the the variable species in the x coordinates in the aesthetic and add the geom_bar()\n\nggplot(data = shrimp, aes(x = species.short))+\n  geom_bar()+\n  labs(x = \"Species\", y= \"Frequency\")\n\n\n\n\n\n\n\nFigure 30: Frequency of shrimp species\n\n\n\n\n\nThen to stack the bar based on the sampling season, we add the argument fill = season in aes() part\n\nggplot(data = shrimp, aes(x = species.short, fill = season))+\n  geom_bar()+\n  labs(x = \"Species\", y =\"Frequency\")\n\n\n\n\n\n\n\nFigure 31: Frequency of shrimp species by season\n\n\n\n\n\nYou can flip the order of bar with position = position_stack(reverse = TRUE)\n\nggplot(data = shrimp, aes(x = species.short, fill = season))+\n  geom_bar(position = position_stack(reverse = TRUE))+\n  labs(x = \"Species\", y =\"Frequency\")\n\n\n\n\n\n\n\nFigure 32: Frequency of shrimp species by season\n\n\n\n\n\nInstead of stacking, you can dodge the bar with position = position_dodge() argument\n\nggplot(data = shrimp, aes(x = species.short, fill = season))+\n  geom_bar(position = position_dodge())+\n  labs(x = \"Species\", y =\"Frequency\")\n\n\n\n\n\n\n\nFigure 33: Frequency of shrimp species by season with span\n\n\n\n\n\nWe notice that the species that only appear one season, the count for that species is span across and make the bar wideer than those species occur in both seasons. We can fix that by parsing position = position_dodge(preserve = \"single\") in the geom_bar function\n\nggplot(data = shrimp, aes(x = species.short, fill = season))+\n  geom_bar(position = position_dodge(preserve = \"single\"))+\n  labs(x = \"Species\", y =\"Frequency\")\n\n\n\n\n\n\n\nFigure 34: Frequency of shrimp species by season without span\n\n\n\n\n\nTo add a black stroke color of the bar, add the argument col = \"black\" inside the geom_bar()\n\nggplot(data = shrimp, aes(x = species.short, fill = season))+\n  geom_bar(position = position_dodge(preserve = \"single\"), color = \"black\")+\n  labs(x = \"Species\", y =\"Frequency\")\n\n\n\n\n\n\n\nFigure 35: Frequency of shrimp species by season without span with black bar color\n\n\n\n\n\nAnd to specify the width of the bar you specify a value in width=.75 argument in geom_bar()\n\nggplot(data = shrimp, aes(x = species.short, fill = season))+\n  geom_bar(position = position_dodge(preserve = \"single\"), color = \"black\", width = .75)+\n  labs(x = \"Species\", y =\"Frequency\")\n\n\n\n\n\n\n\nFigure 36: Frequency of shrimp species by season without span with black bar color\n\n\n\n\n\n\n\n\nWe have seen how to make barplot that show the count with geom_bar(). You can also use the barplot to show the values with the geom_col() function and specify what variables you want on the x and y axis. For instance, we want to show percentage of shrimp species by season. Because the geom_col() requires summarized statistics, we need to compute the percentage for each season as the chunk below highlight.\n\nshrimp.pct = shrimp %&gt;% \n  group_by(species.short, season) %&gt;% \n  summarise(n = n()) %&gt;% \n  mutate(pct = n/sum(n), \n         pct = (pct * 100) %&gt;% round(2))\n\nshrimp.pct\n\n# A tibble: 6 × 4\n# Groups:   species.short [4]\n  species.short  season     n   pct\n  &lt;chr&gt;          &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;\n1 F.indicus      DRY      404 100  \n2 M.monoceros    DRY      539  86.9\n3 M.monoceros    WET       81  13.1\n4 P.monodon      DRY      738  52.4\n5 P.monodon      WET      671  47.6\n6 P.semisulcatus DRY      227 100  \n\n\nOnce we have computed the statistics, we can use them to make barplot. Note that unlike the geom_bar(), which need only the x variable, geom_col() requires x and y variables specified. For illustration, we specified the x = species, and y = pct in the aes() to make a barplot that show the percentage of shrimp by season (Figure 37).\n\nggplot(data = shrimp.pct, aes(x = species.short, y = pct, fill = season))+\n  geom_col(position = position_dodge(preserve = \"single\"), color = \"black\", width = .75)+\n  labs(x = \"Species\", y =\"Percentage\")\n\n\n\n\n\n\n\nFigure 37: Percentage of shrimp species by season without span with black bar color\n\n\n\n\n\n\n\n\n\nA pie chart is a disk divided into pie-shaped pieces proportional to the relative frequencies of the classes. To obtain angle for any class, we multiply the relative frequencies by 360 degree, which corresponds to the complete circle. Either variables or attributes can be portrayed in this manner, but a pie chart is especially useful for attributes. A pie diagram for contribution of different fish groups/species to the total fish landings at a landing site of a river is shown in Figure 38.\n\nshrimp %&gt;% \n  group_by(species) %&gt;% \n  summarise(n = n()) %&gt;% \n  mutate(pct = round(n/sum(n)*100), 2) %&gt;% \n  mutate(species = str_replace(string = species, pattern = \" \", replacement = \"\\n\")) %&gt;% \n  mutate(label = paste0(\"(\",pct,\"%\",\")\")) %&gt;% \n  ggpubr::ggpie(x = \"pct\", label = \"label\", fill = \"species\", lab.pos = \"in\", palette = \"jama\", color = \"ivory\", ggtheme = theme_void(), )\n\n\n\n\n\n\n\nFigure 38: Percentage composition of prawn species\n\n\n\n\n\nAn extended pie chart is donut shown in Figure 39\n\nshrimp %&gt;% \n  group_by(species) %&gt;% \n  summarise(n = n()) %&gt;% \n  mutate(pct = round(n/sum(n)*100), 2) %&gt;% \n  mutate(species = str_replace(string = species, pattern = \" \", replacement = \"\\n\")) %&gt;% \n  mutate(label = paste0(\"(\",pct,\"%\",\")\")) %&gt;% \n  ggpubr::ggdonutchart(x = \"pct\", label = \"label\", fill = \"species\", lab.pos = \"in\", palette = \"jama\", color = \"ivory\", ggtheme = theme_void())\n\n\n\n\n\n\n\nFigure 39: Percentage composition of prawn species\n\n\n\n\n\n\n\n\n\nshrimp %&gt;% \n  group_by(species.short) %&gt;% \n  count() %&gt;% \n  arrange(-n) %&gt;% \n  ggplot(aes(x = reorder(species.short,n), y = n, \n             fill = species.short), stat = \"identity\")+\n  geom_col() +\n  coord_polar(theta = \"y\")+\n  theme_bw() +\n  theme(axis.title = element_blank(), legend.position = \"right\", axis.text.y = element_blank(), axis.ticks = element_blank())+\n  scale_fill_brewer(palette = \"Set2\", name = \"Species\")\n\n\n\n\n\n\n\nFigure 40: Barplot with polar transformation\n\n\n\n\n\n\nshrimp %&gt;% \n  ggplot() +\n  geom_bar(aes(x = tide, fill = species.short),\n           color = \"ivory\") +\n  labs(x = \"Tide\", y = \"Count\") +\n  coord_polar()+\n  theme_bw() +\n  theme(axis.title = element_blank(), \n        legend.position = \"right\")+\n  scale_fill_brewer(palette = \"Set2\", name = \"Species\")\n\n\n\n\n\n\n\nFigure 41: Stacked barplot with polar transformation\n\n\n\n\n\n\n\n\nAlthough the ggridges package provides geom_ridgeline and geom_density_ridges, we focus on the latter because it has ability to estimates data densities and then draws those using ridgelines.The geom geom_density_ridges calculates density estimates from the provided data and then plots those, using the ridgeline visualization.\n\nlfq4 %&gt;% \n  mutate(months = lubridate::month(date, label = TRUE)) %&gt;%\n  ggplot()+\n  ggridges::geom_density_ridges(aes(x = tl_mm, y = months, fill = site), alpha = .7)+\n  scale_fill_brewer(palette = \"Set2\", name = \"Sampling\\nsite\")+\n  theme_minimal()+\n  theme(legend.position = c(.85,.2), legend.background = element_rect())+\n  labs(y = \"Months\", x = \"Total length (mm.)\")"
  },
  {
    "objectID": "posts/basicplots/index.html#combining-multiple-plots",
    "href": "posts/basicplots/index.html#combining-multiple-plots",
    "title": "Basic plots with ggplot2",
    "section": "2 Combining multiple plots",
    "text": "2 Combining multiple plots\nWhen exploring data with many variables, you’ll often want to make the same kind of plot (e.g. a violin plot) for several variables. It will frequently make sense to place these side-by-side in the same plot window. The patchwork package extends ggplot2 by letting you do just that. Let’s install it:\ninstall.packages(\"patchwork\")\nThen load a package in the workspace\n\nrequire(patchwork)\n\nTo use patchwork (Pedersen, 2020), save each plot as a plot object :\n\nplot.tl = ggplot(data = lfq4,\n       aes(x = tl_mm, fill = site)) +\n  geom_density(alpha = 0.4, position = \"identity\")+\n  labs(x = \"Total length (mm)\", y = \"Frequency\")+\n  theme_minimal()\n\n plot.wt = ggplot(data = lfq4,\n       aes(x = wt_gm, fill = site)) +\n  geom_density(alpha = 0.4, position = \"identity\")+\n  labs(x = \"Weight (gram)\", y = \"Frequency\")+\n  theme_minimal()+\n   theme(legend.position = \"none\")\n\nthen add them together\n\nplot.tl + plot.wt\n\n\n\n\n\n\n\nFigure 42: Density plot by sites for total length (left panel) and weight (right panel)\n\n\n\n\n\n`\n\nplot.tl / plot.wt\n\n\n\n\n\n\n\nFigure 43: Density plot by sites for total length (top panel) and weight (bottom panel)\n\n\n\n\n\nWe need first to extract monsoon seasons from the dataset. We know from literature that the coasal waters of East Africa is affected by monsoon season, which is influenced trade winds, which is broadly categorized as;\n\nNortheast monsoon season — November through March\nSoutheast monsoon season — May to september\nIntermonsoon season — April and October\n\nWe can use the month information to break our dataset into three monsoon seasons as;\n\nlfq4.season = lfq4 %&gt;% \n  mutate(month = lubridate::month(date),\n         season = case_when(month &gt; 10 | month &lt; 4 ~ \"NE\",\n                            month &gt;=5 & month &lt; 10 ~ \"SE\",\n                            month == 4 | month == 10 ~ \"INT\")) %&gt;% select(-month)\n\n\nplot.int =lfq4.season %&gt;% \n  filter(site == \"Voi\" & season == \"INT\") %&gt;% \n  ggplot(aes(x = tl_mm, y = wt_gm))+\n  geom_point(alpha = .2)+\n  theme_bw()+\n  scale_x_continuous(name = \"Total length (mm)\")+\n  scale_y_continuous(name = \"Weight (gram)\")+\n  annotate(geom = \"label\",x = 120, y = 80, label = \"Northeast\\nSeason\")\n\nplot.ne = lfq4.season %&gt;% \n  filter(site == \"Voi\" & season == \"NE\") %&gt;% \n  ggplot(aes(x = tl_mm, y = wt_gm))+\n  geom_point(alpha = .2)+\n  theme_bw()+\n  scale_x_continuous(name = \"Total length (mm)\")+\n  scale_y_continuous(name = \"Weight (gram)\")+\n  annotate(geom = \"label\",x = 140, y = 80, label = \"Southeast\\nSeason\")\n\nplot.se = lfq4.season %&gt;% \n  filter(site == \"Voi\" & season == \"SE\") %&gt;% \n  ggplot(aes(x = tl_mm, y = wt_gm))+\n  geom_point(alpha = .2)+\n  theme_bw()+\n  scale_x_continuous(name = \"Total length (mm)\")+\n  scale_y_continuous(name = \"Weight (gram)\")+\n  annotate(geom = \"label\",x = 120, y = 80, label = \"Inter\\n Monsoon\")\n\nplot.all = lfq4.season %&gt;% \n  filter(site == \"Voi\") %&gt;% \n  ggplot(aes(x = tl_mm, y = wt_gm, color = season))+\n  geom_point(alpha = .2)+\n  theme_bw()+\n  scale_x_continuous(name = \"Total length (mm)\")+\n  scale_y_continuous(name = \"Weight (gram)\")+\n  theme(legend.position = c(.2,.8))\n\nYuo may plot One row with three plots and one row with a single plot\n\n(plot.ne + plot.se + plot.int)/\n  plot.all\n\n\n\n\n\n\n\n\nOr one column with three plots and one column with a single plot `\n\nplot.all | (plot.ne / plot.se / plot.int)"
  },
  {
    "objectID": "posts/basicplots/index.html#labelling-outliers",
    "href": "posts/basicplots/index.html#labelling-outliers",
    "title": "Basic plots with ggplot2",
    "section": "3 Labelling outliers",
    "text": "3 Labelling outliers\nInteractive plots are great when exploring a dataset but are not always possible to use in other contexts, e.g. for printed reports and some presentations. In these other cases, we can instead annotate the plot with notes about outliers. One way to do this is to use a geom called geom_text.\n\nlfq4.season %&gt;% \n  filter(!site == \"Voi\") %&gt;% \n  ggplot(aes(x = season, y = wt_gm, fill = season))+\n  geom_boxplot(alpha = .4, width = .29)+\n  ggrepel::geom_text_repel(aes(label = if_else( wt_gm &gt; 175, site, \"\"))) +\n  theme_bw()+\n  scale_x_discrete(name = \"Monsoon season\")+\n  scale_y_continuous(name = \"Weight (gram)\")+\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/basicplots/index.html#add-on-packages",
    "href": "posts/basicplots/index.html#add-on-packages",
    "title": "Basic plots with ggplot2",
    "section": "4 Add-on packages",
    "text": "4 Add-on packages\nThe R community has developed packages that extend the capability of ggplot2. Some of the packages include:\n\nmetR: Provide addition tools for plotting filled contour, and label contour lines\nggrepel: Contains tools for automatically position non-overlapping text labels\nggspatial: Spatial Data Framework for ggplot2\nRcolorBrewer: Contains color palettes for continuous and discrete plots\ncowplot: Contains addition themes and tools to combine ggplot2 plots in one panel\negg: Provide tools for plot aligning and symmetrised ggplot2 plots\noce: Provide color pallete for visualization of Oceanographic Data\nggsn: Provide tools for mapping North symbols and scale bars on maps created with ggplot2\ngganimate: convert static ggplot2 plots to animations\nggformula: adds some additional plot options to ggplot2\nsf : Add capabilities of ggplot2 to map spatial data such as simple features\nggthemes: contains extra themes, scales, and geoms, and functions for and related to ggplot2\nggridges: extend the geom_density function by plotiing closed polygons insted of ridgelines\nggpmisc"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International",
    "section": "",
    "text": "Creative Commons Corporation (“Creative Commons”) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an “as-is” basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible.\nUsing Creative Commons Public Licenses\nCreative Commons public licenses provide a standard set of terms and conditions that creators and other rights holders may use to share original works of authorship and other material subject to copyright and certain other rights specified in the public license below. The following considerations are for informational purposes only, are not exhaustive, and do not form part of our licenses.\n\nConsiderations for licensors: Our public licenses are intended for use by those authorized to give the public permission to use material in ways otherwise restricted by copyright and certain other rights. Our licenses are irrevocable. Licensors should read and understand the terms and conditions of the license they choose before applying it. Licensors should also secure all rights necessary before applying our licenses so that the public can reuse the material as expected. Licensors should clearly mark any material not subject to the license. This includes other CC-licensed material, or material used under an exception or limitation to copyright. More considerations for licensors.\nConsiderations for the public: By using one of our public licenses, a licensor grants the public permission to use the licensed material under specified terms and conditions. If the licensor’s permission is not necessary for any reason–for example, because of any applicable exception or limitation to copyright–then that use is not regulated by the license. Our licenses grant only permissions under copyright and certain other rights that a licensor has authority to grant. Use of the licensed material may still be restricted for other reasons, including because others have copyright or other rights in the material. A licensor may make special requests, such as asking that all changes be marked or described. Although not required by our licenses, you are encouraged to respect those requests where reasonable. More considerations for the public.\n\n\n\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\n\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nNonCommercial means not primarily intended for or directed towards commercial advantage or monetary compensation. For purposes of this Public License, the exchange of the Licensed Material for other material subject to Copyright and Similar Rights by digital file-sharing or similar means is NonCommercial provided there is no payment of monetary compensation in connection with the exchange.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\n\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part, for NonCommercial purposes only; and\nB. produce and reproduce, but not Share, Adapted Material for NonCommercial purposes only.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties, including when the Licensed Material is used other than for NonCommercial purposes.\n\n\n\n\n\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material, You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nFor the avoidance of doubt, You do not have permission under this Public License to Share Adapted Material.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\n\n\n\n\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database for NonCommercial purposes only and provided You do not Share Adapted Material;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material; and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\n\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\n\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\n\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\n\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "license.html#creative-commons-attribution-noncommercial-noderivatives-4.0-international-public-license",
    "href": "license.html#creative-commons-attribution-noncommercial-noderivatives-4.0-international-public-license",
    "title": "Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International",
    "section": "",
    "text": "By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\n\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nNonCommercial means not primarily intended for or directed towards commercial advantage or monetary compensation. For purposes of this Public License, the exchange of the Licensed Material for other material subject to Copyright and Similar Rights by digital file-sharing or similar means is NonCommercial provided there is no payment of monetary compensation in connection with the exchange.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\n\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part, for NonCommercial purposes only; and\nB. produce and reproduce, but not Share, Adapted Material for NonCommercial purposes only.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties, including when the Licensed Material is used other than for NonCommercial purposes.\n\n\n\n\n\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material, You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nFor the avoidance of doubt, You do not have permission under this Public License to Share Adapted Material.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\n\n\n\n\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database for NonCommercial purposes only and provided You do not Share Adapted Material;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material; and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\n\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\n\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\n\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\n\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "feedback.html",
    "href": "feedback.html",
    "title": "Site Feedback",
    "section": "",
    "text": "Site Feedback\nYou feed it back into the site\n\n\n\n  Please leave your valuable feedback\n  \n\n\n  \n    Please leave your valuable feedback\n    \n      \n        \n          First Name:\n          \n        \n        \n          Last Name:\n          \n        \n      \n      \n        Email Address:\n        \n      \n      \n        Message:\n        \n      \n      \n        Submit\n        Reset"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Datasets",
    "section": "",
    "text": "Chinook – contains the lengths and weights for Chinook Salmon from three locations in Argentina. you can simply download the file from the internet using the code chunk below;\n\n\nchinook = readr::read_csv(\n  file = \"https://raw.githubusercontent.com/lugoga/kitaa/main/datasets/chinook_lw.csv\")\n\n\n\ntlwloc120.117.9Argentina115.017.2Argentina111.216.8Argentina29.20.3Puyehue25.20.3Puyehue18.00.1Puyehue\n\n\n\nMauna Loa Atmospheric CO2 Concentration– store Atmospheric concentrations of CO2 are expressed in parts per million (ppm) and reported in the preliminary 1997 SIO manometric mole fraction scale. The link to download is found in the chunk\n\n\nco = readr::read_csv(\"https://raw.githubusercontent.com/lugoga/kitaa/main/datasets/long_form_co2.csv\")\n\n`\n\n\ndaymonthsyearco21511,959315.421521,959316.311531,959316.5015101,997360.8315111,997362.4915121,997364.34\n\n\n\nLength frequency data – the dataset is organized to contain detailed information including the date of recording, identification of species, size measurements in centimeters, categorization of size ranges, types of gear used during sampling, and the exact locations where the data was gathered.\n\n\nlfq = readr::read_csv(\"https://raw.githubusercontent.com/lugoga/kitaa/main/datasets/LFQ_sample_1.xls\")\n\n\n\nDateSpeciesSize (cm)Size ClassGear typeLanding_site2019-05-30Siganus sutor16.518SpeargunPwani2019-05-30Siganus sutor12.012SpeargunPwani2019-05-30Siganus sutor15.015SpeargunPwani2020-07-25Lutjanus fulviflamma13.015SpeargunBahari2020-07-25Lutjanus fulviflamma17.018SpeargunBahari2020-07-25Lutjanus fulviflamma18.521SpeargunBahari\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that you can import the dataset from online API to Rstudio with easy as the chunk above highlight"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "DATIKA",
    "section": "",
    "text": "Data Visualization\n\n\n\n\n\n\nData Science\n\n\nData Visualization\n\n\n\nData visualization simplifies complex information, making it easier to understand and leading to more informed decision-making\n\n\n\n\n\nMay 5, 2024\n\n\nMasumbuko Semba\n\n\n\n\n\n\n\n\n\n\n\n\nData cleaning, merging, and appending\n\n\n\n\n\n\nvisualization\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 1, 2024\n\n\nMasumbuko Semba\n\n\n\n\n\n\n\n\n\n\n\n\nTidying Data frame\n\n\n\n\n\n\nvisualization\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 29, 2024\n\n\nMasumbuko Semba\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Descriptive Statistics\n\n\n\n\n\n\nvisualization\n\n\ncode\n\n\nStatistics\n\n\n\nExamine measures of center and dispersion of the data so that we can gain valuable insights into the characteristics and distribution of various metrics that are in a dataset\n\n\n\n\n\nApr 25, 2024\n\n\nMasumbuko Semba\n\n\n\n\n\n\n\n\n\n\n\n\nBasic plots with ggplot2\n\n\n\n\n\n\nvisualization\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 1, 2024\n\n\nMasumbuko Semba\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing data with grammar of graphics\n\n\n\n\n\n\nvisualization\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 25, 2024\n\n\nMasumbuko Semba\n\n\n\n\n\n\n\n\n\n\n\n\nImporting table files into R\n\n\n\n\n\n\nvisualization\n\n\ncode\n\n\n\nLearning to import tabular files from local directory intot R session is an important skills in R programming\n\n\n\n\n\nFeb 26, 2024\n\n\nMasumbuko Semba\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding vector and dataframe\n\n\n\n\n\n\nvisualization\n\n\ncode\n\n\n\nUnderstanding vectoor and dataframe as core data storage in R is an important part, which allows for data analysis and visualization\n\n\n\n\n\nFeb 12, 2024\n\n\nMasumbuko Semba\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Data Structures in R\n\n\n\n\n\n\nvisualization\n\n\ncode\n\n\n\nLearn the primary data structures (vector and data frame) in R, which are the foundation of data manipulation and analysis in R \n\n\n\n\n\nFeb 3, 2024\n\n\nMasumbuko Semba\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding and using Data types in R\n\n\n\n\n\n\nvisualization\n\n\ncode\n\n\n\nLearn the primary data types in R, which are the foundation of data programming in R \n\n\n\n\n\nJan 26, 2024\n\n\nMasumbuko Semba\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Started with R and RStudio\n\n\n\n\n\n\nvisualization\n\n\ncode\n\n\n\nLearn the basic of programming with R using RStudio. We’ll install R, and RStudio RStudio, an extremely popular development environment for R \n\n\n\n\n\nJan 24, 2024\n\n\nMasumbuko Semba\n\n\n\n\n\n\n\n\n\n\n\n\nThe basics of R programming\n\n\n\n\n\n\nData Science\n\n\nR Basic\n\n\n\nThe R programming language serves as a powerful tool for statistical computing and graphics, offering extensive capabilities for data analysis, visualization, and modeling\n\n\n\n\n\nJan 24, 2024\n\n\nMasumbuko Semba\n\n\n\n\n\n\n\n\n\n\n\n\nThe basics of R and Rstudio\n\n\n\n\n\n\nvisualization\n\n\ncode\n\n\nanalysis\n\n\n\nUnderstanding the building blocks of R and its working environment Rstudio for smooth operations in data science\n\n\n\n\n\nJan 24, 2024\n\n\nMasumbuko Semba\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a Timeline graphic using R and ggplot2\n\n\n\n\n\n\nvisualization\n\n\ncode\n\n\nanalysis\n\n\n\nCreating a timeline graphic using ggplot2, which is a powerful data visualization library in R\n\n\n\n\n\nNov 24, 2023\n\n\nMasumbuko Semba\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "DATIKA",
    "section": "",
    "text": "I’m glad you’re here at our website, datika! As an avid enthusiast of statistics and the R programming language, we’ve created this platform to share our experiences, research and projects with fellows.\nDatika is more than just a showcase of our work – it’s a thriving community where you can engage with the content, expand your knowledge, and connect with others in the fascinating world of statistics and R.\nNavigating through the five main sections of our website with ease:\n\n“Posts”: In this tab, we share our reflections and questions that have emerged throughout my R language learning journey, providing engaging insights and perspectives on various aspects of R programming and statistics.\n“Presentations”: Keep up to date with my most recent presentations, where I delve into topical subjects and share research findings, offering a wealth of knowledge on the latest advancements in R and statistics.\n“Books”: Explore the real-world projects I’ve undertaken, demonstrating the power and versatility of R in addressing intricate statistical challenges, and find inspiration for your own projects.\n“Datasets”: Here you’ll find a systematic approach to learning R. Explore comprehensive tutorials and guides crafted to guide you toward proficiency in R programming.\n“About”: You can find out our background, what we’re currently working on, and our research achievements. Our passion for statistics, R programming, and knowledge-sharing is the driving force behind Stats & R.\n“Gallery”: Provide a scenic view of nature and environment, which we are connected with and glean ecosystem benefits\n\nOnce again, thank you for visiting datika. I hope you find the content engaging and informative, and that our platform serves as a valuable resource for you as you delve into the captivating realms of statistics and R programming. Enjoy your time and have fun exploring! 😘"
  },
  {
    "objectID": "about.html#welcome-to-datika",
    "href": "about.html#welcome-to-datika",
    "title": "DATIKA",
    "section": "",
    "text": "I’m glad you’re here at our website, datika! As an avid enthusiast of statistics and the R programming language, we’ve created this platform to share our experiences, research and projects with fellows.\nDatika is more than just a showcase of our work – it’s a thriving community where you can engage with the content, expand your knowledge, and connect with others in the fascinating world of statistics and R.\nNavigating through the five main sections of our website with ease:\n\n“Posts”: In this tab, we share our reflections and questions that have emerged throughout my R language learning journey, providing engaging insights and perspectives on various aspects of R programming and statistics.\n“Presentations”: Keep up to date with my most recent presentations, where I delve into topical subjects and share research findings, offering a wealth of knowledge on the latest advancements in R and statistics.\n“Books”: Explore the real-world projects I’ve undertaken, demonstrating the power and versatility of R in addressing intricate statistical challenges, and find inspiration for your own projects.\n“Datasets”: Here you’ll find a systematic approach to learning R. Explore comprehensive tutorials and guides crafted to guide you toward proficiency in R programming.\n“About”: You can find out our background, what we’re currently working on, and our research achievements. Our passion for statistics, R programming, and knowledge-sharing is the driving force behind Stats & R.\n“Gallery”: Provide a scenic view of nature and environment, which we are connected with and glean ecosystem benefits\n\nOnce again, thank you for visiting datika. I hope you find the content engaging and informative, and that our platform serves as a valuable resource for you as you delve into the captivating realms of statistics and R programming. Enjoy your time and have fun exploring! 😘"
  },
  {
    "objectID": "about.html#license",
    "href": "about.html#license",
    "title": "DATIKA",
    "section": "License",
    "text": "License\n\ndatika by The team is licensed under Attribution-NonCommercial-NoDerivatives 4.0 International\n\nVisit here for more information about the license."
  },
  {
    "objectID": "about.html#lastest-posts",
    "href": "about.html#lastest-posts",
    "title": "DATIKA",
    "section": "Lastest posts",
    "text": "Lastest posts\n\n\n See all"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Data Visualization\n\n\n\n\n\n\n\n\nMay 5, 2024\n\n\n\n\n\n\n\nData cleaning, merging, and appending\n\n\n\n\n\n\n\n\nMay 1, 2024\n\n\n\n\n\n\n\nTidying Data frame\n\n\n\n\n\n\n\n\nApr 29, 2024\n\n\n\n\n\n\n\nUnderstanding Descriptive Statistics\n\n\n\n\n\n\n\n\nApr 25, 2024\n\n\n\n\n\n\n\nBasic plots with ggplot2\n\n\n\n\n\n\n\n\nApr 1, 2024\n\n\n\n\n\n\n\nVisualizing data with grammar of graphics\n\n\n\n\n\n\n\n\nMar 25, 2024\n\n\n\n\n\n\n\nImporting table files into R\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\n\n\n\n\n\nUnderstanding vector and dataframe\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\n\n\n\n\n\nMastering Data Structures in R\n\n\n\n\n\n\n\n\nFeb 3, 2024\n\n\n\n\n\n\n\nUnderstanding and using Data types in R\n\n\n\n\n\n\n\n\nJan 26, 2024\n\n\n\n\n\n\n\nGetting Started with R and RStudio\n\n\n\n\n\n\n\n\nJan 24, 2024\n\n\n\n\n\n\n\nThe basics of R programming\n\n\n\n\n\n\n\n\nJan 24, 2024\n\n\n\n\n\n\n\nThe basics of R and Rstudio\n\n\n\n\n\n\n\n\nJan 24, 2024\n\n\n\n\n\n\n\nCreating a Timeline graphic using R and ggplot2\n\n\n\n\n\n\n\n\nNov 24, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Books",
    "section": "",
    "text": "Learning to code with the R language opens up a vast world of possibilities in data analysis, visualization, and spatial data handling. While the journey may seem daunting at first, having the right resources can significantly ease the learning curve. Here’s a curated list of relevant books to help you navigate through the intricacies of R programming:\n\nR for Data Science R for Data Science is a practical guide that serves as an excellent starting point for beginners. Authored by Hadley Wickham and Garrett Grolemund, this book introduces R programming for data analysis and visualization, with a strong emphasis on the tidyverse approach. Through hands-on examples and clear explanations, readers learn how to manipulate, explore, and visualize data effectively.\nModern R with the tidyverse takes a contemporary approach to R programming, focusing on modern packages, particularly those from the tidyverse ecosystem. Written by authors Thomas Mock and Rick Scavetta, this book goes beyond traditional R concepts, introducing readers to efficient data manipulation techniques and best practices for writing clean, readable code.\nPractical Spatial Data is an invaluable resource for those interested in working with spatial data, particularly in coastal and marine environments. Authored by Masumbuko Semba, this book provides a comprehensive introduction to R programming with a specific focus on handling spatial data. From importing geographic information to performing spatial analysis, readers gain practical insights and hands-on experience in utilizing R for geospatial applications.\nGeospatial Technology and Spatial Analysis in R delves deeper into the realm of geospatial data analysis using R. Written by Masumbuko Semba, this book explores the latest tools and packages available for modern spatial data handling and manipulation. Through step-by-step tutorials and real-world examples, readers learn how to harness the power of R for tasks such as geographic visualization, spatial statistics, and remote sensing analysis."
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "DATIKA",
    "section": "",
    "text": "Who are you?\nWhy did you start Stats & R?\nCan I use the content on your website for my own purposes?\nDo you accept guest posts or contributions?\nHow can I support your website or contribute to your work?\nAre there any other software or resources you recommend for data science?"
  },
  {
    "objectID": "faq.html#sec-who-are-you-",
    "href": "faq.html#sec-who-are-you-",
    "title": "DATIKA",
    "section": "Who are you?",
    "text": "Who are you?\nI’m an ordinary person who is down-to-earth. If you’d like to learn more about me, feel free to visit my About page."
  },
  {
    "objectID": "faq.html#sec-why-did-you-start-stats--r-",
    "href": "faq.html#sec-why-did-you-start-stats--r-",
    "title": "DATIKA",
    "section": "Why did you start Stats & R ?",
    "text": "Why did you start Stats & R ?\nThe purpose of starting Stats & R is to motivate myself to continuously learn new knowledge, as well as to showcasing my achievements for those interested in R language and data science to read, offer feedback and engage in mutual exchange."
  },
  {
    "objectID": "faq.html#sec-can-i-use-the-content-on-your-website-for-my-own-purposes",
    "href": "faq.html#sec-can-i-use-the-content-on-your-website-for-my-own-purposes",
    "title": "DATIKA",
    "section": "Can I use the content on your website for my own purposes?",
    "text": "Can I use the content on your website for my own purposes?\nFeel free to utilize the content on my website for your own use, but please adhere to the following guidelines:\n\nCommercial use of my content is prohibited.\nProper credit must be given in your works.\nI am not liable for any issues related to the code or results in the content.\n\nIf you incorporate my code or material into your project, kindly reference the following details:\n\nAuthor’s name: “NING LI”\nTitle of the article or tutorial\nWebsite name: “Stats & R”\nURL of the article or tutorial\nDate of access or code reproduction\n\nFor example:\nNING LI, R Basic, Stats & R. Retrieved from https://stats-r.com/series/r%20basic/, accessed on March 6th, 2023."
  },
  {
    "objectID": "faq.html#sec-do-you-accept-guest-posts-or-contributions",
    "href": "faq.html#sec-do-you-accept-guest-posts-or-contributions",
    "title": "DATIKA",
    "section": "Do you accept guest posts or contributions?",
    "text": "Do you accept guest posts or contributions?\nOf course. I warmly welcome you to publish posts and make valuable contributions on my website.\nIf you are interested in publishing a post on my website, please contact me through Twitter or github. Once your submission has been reviewed and approved, I will publish it on my website, and the copyright will belong to you. Additionally, if you have any suggestions or feedback regarding my content, please contact me through the contact form. I will respond to your message promptly."
  },
  {
    "objectID": "faq.html#sec-how-can-i-support-your-website-or-contribute-to-your-work",
    "href": "faq.html#sec-how-can-i-support-your-website-or-contribute-to-your-work",
    "title": "DATIKA",
    "section": "How can I support your website or contribute to your work?",
    "text": "How can I support your website or contribute to your work?\nIf you find my content helpful, please consider supporting me by buying me a coffee.\nOr, You are also welcome to buy me by Wechat and Alipay:"
  },
  {
    "objectID": "faq.html#sec-are-there-any-other-software-or-resources-you-recommend-for-data-science",
    "href": "faq.html#sec-are-there-any-other-software-or-resources-you-recommend-for-data-science",
    "title": "DATIKA",
    "section": "Are there any other software or resources you recommend for data science?",
    "text": "Are there any other software or resources you recommend for data science?\nAs a science researcher, I deal with various scientific tools every day. Here, I have listed some of the software I use. If anyone knows better software, please leave a comment in the section below.\n\nStatistical software\nStatistical software is the most commonly used, and I mainly focus on R language, for no other reason than it being free and powerful!!!\n\nR (free and open source)\nRstudio (the most popular R IDE)\nIBM SPSS\nOriginLab\njamovi\n\n\n\nReference management software\nGood reference management software will greatly improve your efficiency in selecting and citing literature.\n\nEndnote\nZotero\nMendeley\n\n\n\nFlowchart software\n\nDiagrams.net"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Datika!!!",
    "section": "",
    "text": "Welcome to Datika!!!\nWe’are glad you’re here at our website, datika! As an avid enthusiast of statistics and the R programming language, we’ve created this platform to share our experiences, research and projects with fellows\nDatika is more than just a showcase of our work – it’s a thriving community where you can engage with the content, expand your knowledge, and connect with others in the fascinating world of statistics and R. Meet the people that make datika up and running!\n\n\nPause for a moment and Join us\n\n\n\n\n\n\n\n\n\nMr. Masumbuko Semba\n\nRole: Mentor\nSemba works at the Nelson Mandela African Institution of Science and Technology. He use programming language like R and Python to manage and analyse data and report using Web Applications, Website or blogs. Semba also design graphics and automate plots, maps for static and interactive document.\n lugosemba@gmail.com |  +255 717 603 703\n\n\n\n\n\n\n\n\n\nMs. Nyamisi Peter\n\nRole: Mentor\nNyamisi works at University of Dar es Salaam. She is phytoplankton exeprt using earth observation data and automate data acquisation, process, analyse. She is excellent in using quarto that combine R codes and text to automate generation of technical documents in word, pdf or html formats.\n nyamisip@gmail.com |  +255 717 516 711\n\n\n\n\n\n\n\nMr. Kassim Said\n\nRole: Mentee\nKassim is an accountant and financial analyst. He works as an accountant at Letshego Faidika Bank and as a financial consultant at Asasi ya Uwezeshaji Tanzania under the USAID Heshimu Bahari Project.\n kassim.salum@asuta.or.tz |  +255 743 956 226\n\n\n\n\n\nMr. Kessy Revocatus\n\nRole: Mentor\nKessy Revocatus is a mathematician current teaches at Hannah Bennie Schools School. Recognizing the role of data in driving business, he is learning R programming to automate data analysis, model and reporting.\n kessyluhegaa@gmail.com |  +255 711 396 392\n\n\n\n\n\n\n\nMr. Barakael Matulu\n\nRole: Mentee\nBarakael works on the USAID Heshimu Bahari Project. He is an aquatic ecologist with interests in marine resources management and climate change issues. Recently, he has also developed an interest in automating data acquisition, processing, analysis, and reporting in R, as well as graphic design.\n bmatulu@gmail.com |  +255 716 349 126\n\n\n\n\n\nMr. Emmanuel Mpina\n\nRole: Facilitator\nAgnatquo digento tatqui officae rehentor reped quibero officae consenda que nobis ini tet libus, sanda debis sitatem pelignima doluptas eos sust parchitem dolor arume cumquia si coribus voluptio ent rem qui beatateseque nonsento modicia eprat.\n emmanuel.mpina@tnc.org |  +255 758 327 749\n\n\n\n\n\n\n\nMs. Amina Kibola\n\nRole: Mentee\nAmina works at National Environment Management Council. She has academic background in ecohydrology and natural resources management, coupled with research experience. Working in the field of environment where role of data in policy development is crucial for informing decision, I have developed interest in learning R programming to be able to automate data; process; analyse and reporting in R.\n amina.kibola@nemc.or.tz |  +255 784 368 987\n\n\n\n\n\nMr. Juma Charles\n\nRole: Mentee\nJuma Charles works at TANESCO as land surveyor. He uses tools like Excel, ArcGIS Pro, and AutoCAD to transform spatial data into information. Currently, he’s expanding his expertise by mastering R to enhance his career capabilities.\n charlesjuma85@gmail.com |  +255 783 293 841\n\n\n\n\n\n\n\nMr. Amon Xanda\n\nRole: Mentee\nAmon has a strong background in GIS from various sectors such as forestry and health. Currently, he works as a Marine Spatial Planning Scientist at TNC, using his GIS expertise to drive impactful projects for marine conservation. He is actively looking to expand his skills by exploring the potential of R for advanced data analysis and visualization.\n xxxx@gmail.com |  +255 684 044 789\n\n\n\n\n\nMs. Elika Kileo\n\nRole: Mentee\nElikananyi Kileo, a medical doctor at Kibada Health Centre, has an interest in paediatrics and child health. Recently, she has also developed an interest in automating data, processing, analysis, and reporting in R\n elikananyikileo1@gmail.com |  +255 756 048 083\n\n\n\n\n\n\n\nMr. Hilary Mkai\n\nRole: Mentee\nHillary is contributing to the USAID Heshimu Bahari project under ASUTA. He’s on a journey of discovery, mastering R to enhance his skills in data management and analysis. With an eye on the web development horizon, he’s gearing up to tackle data challenges head-on, making every click and scroll count.\n mkaihillary01@gmail.com |  +255 738 238 530\n\n\n\n\n\nMr. Humphrey Mahundi\n\nRole: Mentee\nHumphrey Mahudi, a Senior Marine Conservation Warden with MPRU’s research and monitoring department in Tanga Coelacanth Marine Park, collaborates in biodiversity assessments using environmental DNA (eDNA). He seeks to enhance his R skills to further his work and research efforts.\n xxxxxxxxxx@gmail.com |  +255 716 196 131\n\n\n\n\n\n\n\nMs. Kulwa Mtaki\n\nRole: Mentee\nMs. Kulwa Mtaki serves as a Marine Conservation Warden at the Marine Parks and Reserves Unit in Tanzania. Her expertise lies in fisheries and aquaculture, marine ecosystem monitoring, and data management and analysis using R-program.\n mtakikulwa@yahoo.com |  +255 759 226 268\n\n\n\n\n\nMr. Kaijage Laurian\n\nRole: Facilitator\nKaijage Laurian, an Ecological Data Manager at Mwambao Coastal Community Network. With background in R for data-driven decisions, Kaijage seeks to integrate machine learning and artificial intelligence into ecological researches.\n kaijagelaurian25@gmail.com |  +255 654 592 215\n\n\n\n\n\n\n\nMr. Paschal Mkongola\n\nRole: Mentee\nPaschal works at Marine parks and Reserves. I have also developed an interest in automatic data, Processing, analysis and Reporting in R, as well as graphic design, career capabilities.\n paschalmkongola@gmail.com |  +255 752 918 484\n\n\n\n\n\nMr. Samson Job\n\nRole: Facilitator\nSamson Job is an aquatic scientists whose interest revolves around climate changes issues and aquatic pollution. To overcome these challenges, he is advancing his skills using R to facilitate data-driven and evidence-based policy making in combating the problems of aquatic pollution and climate changes.\n samjob4321@gmail.com |  +255 767 525 022\n\n\n\n\n\n\n\nMr.Stephano Semba\n\nRole: Mentee\nStephano Semba is employed by the Tanzania Forestry Service (TFS) as a Forestry Conservator. Understanding the importance of programming, I have dedicated myself to acquiring knowledge in data science, report writing, and spatial analysis to address the challenges in conservation.\n xxx@gmail.com |  +255 677 250 711\n\n\n\n\n\nMr. Gabriely J. Namate\n\nRole: Mentee\nNamate is employed at Pangani District Council as a Fisheries Officer and Aquaculture Specialist. Recognizing the power of R programming in data, he decided to learn and acquired necessary skills for statistical analysis, plotting, and sharing the finding.\n xxx@gmail.com |  +255 654 989 271\n\n\n\n\n\n\n\nMr.Shadrack Nyanda\n\nRole: Mentee\nShadrack Nyanda is a dedicated Health, Safety & Environment (HSE) practitioner passionate about using R programming for data computation, analysis, and image generation. With a focus on data manipulation and visualization in the context of health, safety, and environmental fields, Shadrack aims to drive efficiency and promote safety and sustainability.\n sylivestershade@gmail.com |  +255 744 034 000\n\n\n\n\n\nMs. Joyceline David\n\nRole: Mentee\nAgnatquo digento tatqui officae rehentor reped quibero officae consenda que nobis ini tet libus, sanda debis sitatem pelignima doluptas eos sust parchitem dolor arume cumquia si coribus voluptio ent rem qui beatateseque nonsento modicia eprat.\n gonsalvesjoyceline@gmail.com |  +44 7909 858960\n\n\n\n\n\n\n\nMr. Zac Maritime\n\nRole: Facilitator\nZac manages the blue economy (oceans for business) in the Western Indian Ocean for The Nature Conservancy (TNC). Before that, he worked on using maps and data for conservation across Africa at WWF. He’s an expert in planning how we use the ocean sustainably, environmental protections, and land/sea surveying.\n xxx@gmail.com |  +254 721 671642\n\n\n\n\n\nMs. Edina Godfrey Swai\n\nRole: Mentee\nMs Edina current works as Seascape coordinator based in Mtwara seascape (Mnazi Bay~Ruvuma Estuary Marine Park) under Asuta-Heshimu Bahari project. She has experience in climate change – adaptation, mitigation, She also is an expert in natural resource management and environment management and conservation. She is current in R programming session with prime aim to learn modern techniques in data analytics and computation.\n xxx@gmail.com |  +255 768 486 385\n\n\n\n\n\n\n\nMr. James Lusana\n\nRole: Mentee\nJames Lusana, an Assistant Lecturer at the University of Dar es Salaam, is researching catfish in Lake Tanganyika for his PhD. He uses advanced imaging techniques to study their physical traits and analyzes their diet and genetics to understand their ecological role and evolution. His work aims to uncover how these fish have adapted and diversified in their environment.\n xxx@gmail.com |  +255 768 310 668\n\n\n\n\n\nMr. KELVIN KARISTO\n\nRole: Mentee\nWith a background in transport and logistics at SAMEKI COMPANY LTD, he is taking a proactive approach to his professional development. Recognizing the increasing importance of data analysis in his field, he’s currently engrossed in learning R programming.\n xxx@gmail.com |  +255 713 363 510\n\n\n\n\n\n\n\nMs. Swaumu Haruna\n\nRole: Mentee\nMs Swaumu Haruna, a has a bachelor’s degree in environmental science and management from Sokoine University of Agriculture. She is currently learning R programming to enhance her skills in data analysis, statistical computation, and data visualization. She aim to leverag coding to automate data analysis and generate data-driven solutions for environmental matters.\n xxx@gmail.com |  +255 627 851 120\n\n\n\n\n\nMr. Ekumbi Boniphace\n\nRole: Facilitator\nEkumbi Boniphace is currently working on the project, where he is responsible for promoting schools in Tanzania. He utilizes various tools such as ArcGIS, Kobo, and Adobe Photoshop to carry out his tasks. Recently, he is expanding his skill set by learning R and Python for data processing, analysis and reporting.\n ekumbiboni@gmail.com |  +255 716 494 125\n\n\n\n\n\n\n\nMs. Maria Pentzel\n\nRole: Mentee\nMs. Maria Pentzel, Marine Conservation Warden at Marine Parks and Reserves Unit (MPRU). Her expertise lies in fisheries, aquaculture and Marine Spatial Planning. Seeking to enhance her skills in data analysis.\n xxx@gmail.com |  +255 782 702 024\n\n\n\n\n\nMr. Daniel Mallya\n\nRole: Mentee\nDaniel Mallya a Marine Community Conservation Warden with MPRU in Mafia Island Marine Park I’m current in R programming session with Prime aim to learn modern techniques in data analytic and computation\n johndan818@gmail.com |  +255 687 352 428\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: The video “CERN: The Journey of Discovery”"
  },
  {
    "objectID": "photos.html",
    "href": "photos.html",
    "title": "Photo Gallery",
    "section": "",
    "text": "Conserving our environment\n\n\n\n\n\nConserving our environment\n\n\n\n\n\nConserving our environment\n\n\n\n\n\nConserving our environment\n\n\n\n\n\nConserving our environment\n\n\n\n\n\nConserving our environment\n\n\n\n\n\nConserving our environment\n\n\n\n\n\nConserving our environment"
  },
  {
    "objectID": "photos.html#marine",
    "href": "photos.html#marine",
    "title": "Photo Gallery",
    "section": "",
    "text": "Conserving our environment\n\n\n\n\n\nConserving our environment\n\n\n\n\n\nConserving our environment\n\n\n\n\n\nConserving our environment\n\n\n\n\n\nConserving our environment\n\n\n\n\n\nConserving our environment\n\n\n\n\n\nConserving our environment\n\n\n\n\n\nConserving our environment"
  },
  {
    "objectID": "photos.html#seaweed",
    "href": "photos.html#seaweed",
    "title": "Photo Gallery",
    "section": "Seaweed",
    "text": "Seaweed\n\n\n\n\nConserving our environment"
  },
  {
    "objectID": "photos.html#fisheries",
    "href": "photos.html#fisheries",
    "title": "Photo Gallery",
    "section": "Fisheries",
    "text": "Fisheries\n\n\n\n\nConserving our environment\n\n\n\n\n\nConserving our environment\n\n\n\n\n\nConserving our environment\n\n\n\n\n\nConserving our environment\n\n\n\n\n\nConserving our environment\n\n\n\n\n\nConserving our environment\n\n\n\n\n\nConserving our environment\n\n\n\n\n\n\nConserving our environment\nConserving our environment\nConserving our environment\nConserving our environment\nConserving our environment\nConserving our environment\nConserving our environment\nConserving our environment\nConserving our environment\nConserving our environment\nConserving our environment\nConserving our environment\nConserving our environment\nConserving our environment\nConserving our environment\nConserving our environment"
  },
  {
    "objectID": "posts/datastructures/index.html",
    "href": "posts/datastructures/index.html",
    "title": "Mastering Data Structures in R",
    "section": "",
    "text": "In data analysis and statistical computing, mastering data structures is essential for efficient data manipulation and analysis. In R, a powerful language for statistical computing and graphics, two fundamental data structures are vectors and data frames. Additionally, the newer tibble data structure offers enhanced features for data manipulation and visualization. In this comprehensive guide, we will explore these data structures in detail, providing illustrative examples along the way.Before we dive in, let pause for a moment and watch video in Figure 1\n\n\n\n\n\n\nFigure 1: Primary data structure in R"
  },
  {
    "objectID": "posts/datastructures/index.html#introduction",
    "href": "posts/datastructures/index.html#introduction",
    "title": "Mastering Data Structures in R",
    "section": "",
    "text": "In data analysis and statistical computing, mastering data structures is essential for efficient data manipulation and analysis. In R, a powerful language for statistical computing and graphics, two fundamental data structures are vectors and data frames. Additionally, the newer tibble data structure offers enhanced features for data manipulation and visualization. In this comprehensive guide, we will explore these data structures in detail, providing illustrative examples along the way.Before we dive in, let pause for a moment and watch video in Figure 1\n\n\n\n\n\n\nFigure 1: Primary data structure in R"
  },
  {
    "objectID": "posts/datastructures/index.html#vectors",
    "href": "posts/datastructures/index.html#vectors",
    "title": "Mastering Data Structures in R",
    "section": "Vectors:",
    "text": "Vectors:\nVectors are one-dimensional arrays that can hold numeric, character, logical, or other atomic data types. They are the simplest and most basic data structure in R.\n\nCreating Vectors:\nCreating vectors in R is straightforward using the c() function, which concatenates elements into a vector.\n# Creating a numeric vector\nnumeric_vector &lt;- c(1, 2, 3, 4, 5)\n\n# Creating a character vector\ncharacter_vector &lt;- c(\"apple\", \"banana\", \"orange\", \"grape\", \"pineapple\")\n\n# Creating a logical vector\nlogical_vector &lt;- c(TRUE, FALSE, TRUE, TRUE, FALSE)\n\n\nUsing Vectors to Create Data Frames:\nData frames are two-dimensional data structures that resemble tables, where each column can be a different data type. They are commonly used for storing and analyzing structured data.\n# Using vectors to create a data frame\ndata &lt;- data.frame(\n  numeric_col = numeric_vector,\n  character_col = character_vector,\n  logical_col = logical_vector\n)\n\n# View the created data frame\nprint(data)"
  },
  {
    "objectID": "posts/datastructures/index.html#data-frames",
    "href": "posts/datastructures/index.html#data-frames",
    "title": "Mastering Data Structures in R",
    "section": "Data Frames:",
    "text": "Data Frames:\nData frames are the workhorse of R for storing tabular data. They are similar to matrices but offer more flexibility, as each column can be of a different data type.\n\nCreating Data Frames:\nData frames can be created directly using the data.frame() function, where each column is specified as a vector.\n# Creating a data frame directly\nstudent_data &lt;- data.frame(\n  name = c(\"John\", \"Alice\", \"Bob\", \"Emma\", \"Michael\"),\n  age = c(25, 23, 27, 22, 24),\n  grade = c(\"A\", \"B\", \"B\", \"C\", \"A\")\n)\n\n# View the created data frame\nprint(student_data)\n\nUsing Tibbles:\nTibbles are a modern alternative to data frames, introduced by the tidyverse ecosystem. They are more user-friendly, provide enhanced printing, and have better support for data analysis pipelines.\n# Creating a tibble from vectors\nlibrary(tibble)\n\n# Creating a tibble directly\nstudent_tibble &lt;- tibble(\n  name = c(\"John\", \"Alice\", \"Bob\", \"Emma\", \"Michael\"),\n  age = c(25, 23, 27, 22, 24),\n  grade = c(\"A\", \"B\", \"B\", \"C\", \"A\")\n)\n\n# View the created tibble\nprint(student_tibble)"
  },
  {
    "objectID": "posts/datastructures/index.html#conclusion",
    "href": "posts/datastructures/index.html#conclusion",
    "title": "Mastering Data Structures in R",
    "section": "Conclusion:",
    "text": "Conclusion:\nUnderstanding data structures such as vectors, data frames, and tibbles is crucial for effective data manipulation and analysis in R. Whether you’re working with numeric data, text data, or logical data, these data structures provide the foundation for organizing and analyzing your data efficiently. By mastering these data structures, you’ll be well-equipped to tackle a wide range of data analysis tasks in R.\nIn this guide, we’ve covered how to create vectors, use them to construct data frames, and introduced the newer tibble data structure. Armed with this knowledge, you’re ready to dive deeper into the world of data analysis and unlock the full potential of R for your projects. Whether you’re a beginner or an experienced R user, mastering these fundamental data structures will pave the way for more advanced data analysis and modeling techniques."
  },
  {
    "objectID": "posts/datastructures/index.html#references",
    "href": "posts/datastructures/index.html#references",
    "title": "Mastering Data Structures in R",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "posts/dataviz/index.html",
    "href": "posts/dataviz/index.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Data visualization is the graphical representation of information and data. By using visual elements like charts, graphs, and maps, data visualization tools provide an accessible way to see and understand trends, outliers, and patterns in data. Additionally, it provides an excellent way for employees or business owners to present data to non-technical audiences without confusion.\nIn the world of Big Data, data visualization tools and technologies are essential to analyze massive amounts of information and make data-driven decisions."
  },
  {
    "objectID": "posts/dataviz/index.html#what-is-data-visualization",
    "href": "posts/dataviz/index.html#what-is-data-visualization",
    "title": "Data Visualization",
    "section": "",
    "text": "Data visualization is the graphical representation of information and data. By using visual elements like charts, graphs, and maps, data visualization tools provide an accessible way to see and understand trends, outliers, and patterns in data. Additionally, it provides an excellent way for employees or business owners to present data to non-technical audiences without confusion.\nIn the world of Big Data, data visualization tools and technologies are essential to analyze massive amounts of information and make data-driven decisions."
  },
  {
    "objectID": "posts/dataviz/index.html#overview",
    "href": "posts/dataviz/index.html#overview",
    "title": "Data Visualization",
    "section": "Overview",
    "text": "Overview\nAfter completing this section, we will:\n\nunderstand the importance of data visualization for communicating data-driven findings.\nbe able to use distributions to summarize data.\nbe able to use the average and the standard deviation to understand the normal distribution\nbe able to access how well a normal distribution fit the data using a quantile-quantile plot.\nbe able to interpret data from a box plot"
  },
  {
    "objectID": "posts/dataviz/index.html#introduction-to-data-visualization",
    "href": "posts/dataviz/index.html#introduction-to-data-visualization",
    "title": "Data Visualization",
    "section": "Introduction to Data Visualization",
    "text": "Introduction to Data Visualization\n\nKey Point:\n\nPlots of data easily communicate information that is difficult to extract from table of raw values.\nData visualization is a key component of exploratory data analysis (EDA), in which the properties of data are explored through visualization and summarization techniques.\nData visualization can help discover biases, systematic errors, mistakes and other unexpected problems in data before those data are incorporated into potentially flawed analysis.\nBasics of data visualization and EDA will be covered in R by using the ggplot2 package and motivating examples from world health, economics and infections disease.\n\n\n\nCode:\n\nlibrary(dslabs)\n\nWarning: package 'dslabs' was built under R version 4.3.3\n\ndata(murders)\nhead(murders)\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65"
  },
  {
    "objectID": "posts/dataviz/index.html#introduction-to-distributions",
    "href": "posts/dataviz/index.html#introduction-to-distributions",
    "title": "Data Visualization",
    "section": "Introduction to Distributions",
    "text": "Introduction to Distributions\n\nKey Points:\n(Variance/Deviation Var)方差: 方差越大，数据的波动越大；方差越小，数据的波动就越小。\n(Standard Deviation)标准差: 方差开根号。\n\nThe most basic statistical summary of a list of object is its distribution.\nWe will learn ways to visualize and analyze distributions in the upcoming videos.\nIn some cases, data can be summarized by two-number summary: the average and standard deviation.I will learn to use data visualization to determine when that is appropriate.\n\n\n\nData Types\nIn R, there are 6 basic data types:\n\nlogical\nnumeric\ninteger\ncomplex\ncharacter\nraw\n\n\n\n\n\n\n\nImportant\n\n\n\nCategorical data are variables that are defined by a small number of groups.\n\nOrdinal categorical data have an inherent order to the categories (mild/medium/hot, for example).\nNon-ordinal categorical data have no order to the categories.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNumerical data take a variety of numeric values.\n\nContinuous variables can take any value.\nDiscrete variables are limited to sets of specific values.\n\n\n\n\n\n\n\n\nflowchart LR\n  A[Main variable types] --&gt; B{Catrgorical}\n  A[Main variable types] --&gt; C{Numeric}\n  B{Catrgorical} --&gt; D[ordinal]\n  B{Catrgorical} --&gt; E[non-ordinal]\n  C{Numeric} --&gt; F[continuous]\n  C{Numeric} --&gt; G[discrete]\n\n\n\n\n\n\n\n\nExercise\n\n# extract the variable names from a dataset\nnames(x)\n# explore how many unique values are used in dataset\nunique(x)\n# determine how many variable were reported\nlength(x)\n# determine how many unique variable were reported\nlength(unique(x))\n# to compute the frequencies of each unique value\ntable(x)"
  },
  {
    "objectID": "posts/dataviz/index.html#describe-heights-to-et",
    "href": "posts/dataviz/index.html#describe-heights-to-et",
    "title": "Data Visualization",
    "section": "Describe Heights to ET",
    "text": "Describe Heights to ET\n\nkey point:\n\nA distribution is a function or description that shows the possible values of a variable and how often those values occur.\nFor categorical variables, the distribution describes the proportions of each category.\nA frequency table is the simplest way to show a categorical distribution. Use prop.table() to convert a table of counts to a frequency table. Barplots display the distribution of categorical variables and are a way to visualize the information in frequency tables.\nFor continuous numerical data, reporting the frequency of each unique entry is not an effective summary as many or most values are unique. Instead, a distribution function is required.\nThe cumulative distribution function (CDF) is a function that reports the proportion of data below a value \\(a\\) for all values of \\(a\\) :\\(F(a)=Pr(x≤a)\\).\nThe proportion of observations between any two values \\(a\\) and \\(b\\) can be computed from the CDF as \\(F(b)-F(a)\\).\nA histogram divides data into non-overlapping bins of the same size and plots the counts of number of values that fall in that interval.\n\n\n\nCode:\nR 语言学习 - table() 结果提取.\n\n# load the dataset\nlibrary(dslabs)\ndata(heights)\n# make a table of category proportions\nprop.table(table(heights$sex))"
  },
  {
    "objectID": "posts/dataviz/index.html#cumulative-distribution-function",
    "href": "posts/dataviz/index.html#cumulative-distribution-function",
    "title": "Data Visualization",
    "section": "Cumulative Distribution Function",
    "text": "Cumulative Distribution Function\nEvery continuous distribution has cumulative distribution function (CDF). The CDF defines the proportion of the data below a given value for all values of \\(a\\) :\n\n\n\nCumulative Distribution Function (CDF)\n\n\nAs defined above, this plot of the CDF for male heights has height value a on the x-axis and the proportion of student with heights of that value or lower(F(a)) on the y-axis.\nThe CDF is essential for calculating probabilities related to continuous data. In a continuous dataset, the probability of a specific exact value is not informative because most entries are unique. For example, in the student heights data, only one individual reported a height of 68.8976377952726 inches, but many students rounded similar heights to 69 inches. If we computed exact value probabilities, we would find that being exactly 69 inches is much more likely than being a non-integer exact height, which does not match our understanding that height is continuous. We can instead use the CDF to obtain a useful summary, such as the probability that a student is between 68.5 and 69.5 inches.\nFor datasets that are not normal, the CDF can be calculated manually by defining a function to compute the probability above. This function can then be applied to a range of values across the range of the dataset to calculate a CDF. Given a datasetmy_data, the CDF can be calculated and plotted like this:\nR语言中的[apply()]，[lapply()]，[sapply()]，tapply()函数以及示例\n\nCode for CDF:\n\n# Cumulative Distribution Function \na &lt;- seq(min(x), max(x), length) # define range of the values\ncdf_function &lt;- function(x) {\n    mean(my_data &lt;= x)\n}\ncdf_values &lt;- sapply(a, cdf_function)\nplot(a, cdf_values)\n\n\n\nCode for student height:\n\n# example for student heights\na &lt;- seq(min(heights$height), max(heights$height), length = 100)\ncdf_function &lt;- function(x){\n  mean(heights$height &lt;= x)\n}\ncdf_value &lt;- sapply(a, cdf_function)\nplot(a, cdf_value)\n\n\n\n\n\n\n\n\nThe CDF defines that proportion of data below a cut-off \\(a\\). To define the proportion of values above \\(a\\), we compute: \\(1-F(a)\\)\nTo define the proportion of values between \\(a\\) and \\(b\\), we compute: \\(F(b)-F(a)\\)\nNote that the CDF can help compute probabilities. The probability of observing a randomly chosen value between \\(a\\) and \\(b\\) is equal to the proportion of values between \\(a\\) and \\(b\\), which we compute with the CDF."
  },
  {
    "objectID": "posts/dataviz/index.html#smooth-density-plots",
    "href": "posts/dataviz/index.html#smooth-density-plots",
    "title": "Data Visualization",
    "section": "Smooth Density Plots",
    "text": "Smooth Density Plots\n\nKey Point:\n\n\n\n\n\n\nA further note on histograms\n\n\n\nThe choice of binwidth has a determinative effect on sharp. There is no “correct” choice for binwidth, and you can sometimes gain insights into the data by experimenting with binwidths.\n\n\n\nSmooth density plots can be thought of as histograms where the binwidth is extremely or infinitely small. The smoothing function makes estimates of the true continuous trend of the data given the available sample of data points.\nThe degree of smoothness can be controlled by an argument in the plotting function.\nWhile the histogram is an assumption-free summary, the smooth density plot is shaped by assumptions and choices you make as a data analyst.\nThe y-axis is scaled so that the area under the density curve sums to 1. This means that interpreting value on the y-axis is not straightforward. To determine the proportion of data in between two values, compute the area under the smooth density curve in the region between those values.\nAn advantage of smooth densities over histograms is that densities are easier to compare visually."
  },
  {
    "objectID": "posts/dataviz/index.html#normal-distribution",
    "href": "posts/dataviz/index.html#normal-distribution",
    "title": "Data Visualization",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\nKey Points:\n\nThe normal distribution:\n\nis centered around one value, the mean\nis symmetric(对称) around the mean.\nis defined completely by its mean(\\(\\mu\\)) and standard deviation(\\(\\sigma\\))\nAlways has the same proportion of observations within a given distance of the mean (for example, 95% with 2\\(\\sigma\\))\n\nThe standard deviation is the average distance between a value and the mean value.\nCalculate the mean using the mean() function.\nCalculate the standard deviation using the sd() function or manually.\nStandard units describe how many standard deviations a value is away from the mean. The z-score, or number of standard deviation an observation is away from the mean \\(\\mu\\):\n\\[\n  z = (x-\\mu)/\\sigma\n  \\]\nComputer standard units with the scale() function.\nImportant: to calculate the proportion of value that meet a certain condition, use the mean function on a logical vector. Because TRUE is converted to 1 and FALSE is converted to 0, taking the mean of this vector yields the proportion of TURE."
  },
  {
    "objectID": "posts/dataviz/index.html#equation-for-the-normal-distribution",
    "href": "posts/dataviz/index.html#equation-for-the-normal-distribution",
    "title": "Data Visualization",
    "section": "Equation for the normal distribution",
    "text": "Equation for the normal distribution\nThe normal distribution is mathematically defined by the following formula for any mean \\(\\mu\\) and standard deviation \\(\\sigma\\):\n\\[\nPr(a &lt; x &lt; b) = \\int_{a}^b\\frac{1}{\\sqrt{2\\pi\\mu}}{e}^{-\\frac{1}{2}(\\frac{x-\\mu^2}{\\sigma})}dx\n\\]\nWhen standard unites \\(z=0\\), the normal distribution is at a maximum, the mean \\(\\mu\\). The function is defined to be symmetric around \\(z=0\\).\nThe normal distribution of z-score is called the standard normal distribution and is defined by \\(\\mu=0\\) and \\(\\sigma=1\\).\nZ-score are useful to quickly evalute whether an observation is average or extreme. Z-scores near 0 are average. Z-score above 2 or below -2 are significantly above or blew the mean, and z-scores above 3 or below -3 are extrmely rate.\n\nCode:\n\n# define x as vector of male heights\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(heights)\nindex &lt;- heights$sex==\"Male\"\nx &lt;- heights$height[index]\n\n# calculate the mean and standard deviation manually\naverage &lt;- sum(x)/length(x)\nSD &lt;- sqrt(sum((x-average)^2)/length(x))\n\n# built-in mean and sd functions - note that the audio and printed value disagree\naverage &lt;- mean(x)\nSD &lt;- sd(x)\nc(average = average, SD = SD)\n\n# calculate standard units\nz &lt;- scale(x)\n\n# calculate proportion of value within 2 SD of mean\nmean(abs(z) &lt; 2)\n\nfunction sd():The built-in R function sd() calculates the standard deviation, but it divides by length(x)-1 instead of length(x). When the length of the list is large, this difference is negligible and you can use the built-in sd() function. Otherwise, you should compute σ by hand. For this course series, assume that you should use the sd() function unless you are told not to do so.\nHere we will learn more about benchmark z-score value and their corresponding probabilities.\n\n\nThe 68-95-99.7 Rule\nThe normal distribution is associated with the 68-95-99.7 rule. This rule describes the probability of observing events within a ceration number of standard deviations of the mean.\n\n\n\nNormal Distribution Probabilities\n\n\nThe probability distribution function for the normal distribution is defined such that:\n\nAbout 68% of observations will be within one standard deviation of the mean(\\(\\mu\\pm\\sigma\\)). In standard units, this is equivalent to a z-score of \\(|z|\\leq2\\)\n\n\n\n\nProbability of an observation within 1 SD of mean\n\n\n\nAbout 95% of observations will be within two standard seviations of the mean(\\(\\mu\\pm2\\sigma\\)). In standard units, this is equivalent to a z-sore of \\(|z|\\leq2\\).\n\n\n\n\nProbability of an ovservation within 2 SD of mean\n\n\n\nAbout 99.7% of observations will be within three standard deviations of the mean(\\(\\mu\\pm3\\sigma\\)). In standard units, this is equivalent to a z-score of \\(|z|\\leq3\\).\n\n\n\n\nProbability of an observation within 3 SD of mean"
  },
  {
    "objectID": "posts/dataviz/index.html#the-normal-cdf-and-pnorm",
    "href": "posts/dataviz/index.html#the-normal-cdf-and-pnorm",
    "title": "Data Visualization",
    "section": "The Normal CDF and pnorm",
    "text": "The Normal CDF and pnorm\n\nKey points:\n\nThe normal distribution has a mathematically defined CDF which can be computed in R with the function pnorm.\npnom(a, avg, s) gives the value of the cumculative distribution function F(a) for the normal distribution defined by average avg and standard deviation s.\nwe say that a random quantity is normally distributed with average avg and standard deviation s if the approximate pnorm(a, avg, s) holds for all values of a.\nIf we are willing to use the normal approximation for height, we can estimate the distribution simply from the mean and standard deviation of our values.\nIf we treat the height data as discrete rather than categorical, we see that the data are not very useful because integer values are more common that expected due to rounding. This is called discretization.\nWith rounded data, the normal approximation is particularly useful when computing probabilities of intervals of length 1 that include exactly over integer.\n\n\n\nCode: Using pnorm to calculate probabilities\nGiven male heights x:\n\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(\"heights\")\nx &lt;- heights %&gt;% filter(sex==\"Male\") %&gt;% pull(height)\n\nwe can estimate the probability that a male is taller than 70.5 inches with:\n\n1 - pnorm(70.5, mean(x), sd(x))\n\n\n\nCode: Discretization and the normal approximation\n\n# plot distribution of exact heights in data\nplot(prop.table(table(x)), xlab = \"a = Height in inches\", ylab = \"Pr(x = a)\")\n\n\n\n\n\n\n\n\n\n# probabilities in actual data over length 1 ranges containing a integer\nmean(x &lt;= 68.5) - mean(x &lt;= 67.5)\nmean(x &lt;= 69.5) - mean(x &lt;= 68.5)\nmean(x &lt;= 70.5) - mean(x &lt;= 69.5)\n\n# probabilities in normal approximation match well\npnorm(68.5, mean(x), sd(x)) - pnorm(67.5, mean(x), sd(x))\npnorm(69.5, mean(x), sd(x)) - pnorm(68.5, mean(x), sd(x))\npnorm(70.5, mean(x), sd(x)) - pnorm(69.5, mean(x), sd(x))\n\n# probabilities in actual data over other ranges don't match normal approx as well\nmean(x &lt;= 70.9) - mean(x &lt;= 70.1)\npnorm(70.9, mean(x), sd(x)) - pnorm(70.1, mean(x), sd(x))"
  },
  {
    "objectID": "posts/dataviz/index.html#definition-of-quantiles",
    "href": "posts/dataviz/index.html#definition-of-quantiles",
    "title": "Data Visualization",
    "section": "Definition of quantiles",
    "text": "Definition of quantiles\n\nDefinition of quantiles\nQuantiles are cut off points that divide a dataset into intervals with set probability. The qth quantile is the value at which q% of the observation are equal to or less than that value.\n\n\nUsing the quantile function\nGiven a dataset data and desired quantile q, you can find the q the quantile of data with:\n\nquantile(data,q)\n\n\n\nPercentiles\nPercentiles are the quantiles that divide a dataset into 100 intervals each with 1% probability. You can determine all percentiles of a dataset data like this:\n\np &lt;- seq(0.01, 0.09, 0.01)\nquantile(data, p)\n\n\n\nQuartiles\nQuartiles divide a dataset into 4 parts each with 25% probability. They are equal to the 25th, 50th and 75th percentiles. The 25th percentile is also known as the 1st quartile, the 50th percentile is also konwn as the median, and the 75th percentile is also knowns as the 3rd quartile.\nThe summary() function returns the minimum, quartiles and maximum of a vector.\n\n\nExamples\nLoad the heights dataset from the dslabs package:\n\nlibrary(dslabs)\ndata(\"heights\")\n\nUsesummaryon the heights$height variable to find the quartiles:\n\nsummary(heights$height)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  50.00   66.00   68.50   68.32   71.00   82.68 \n\n\nFind the percentiles of height$height:\n\np &lt;- seq(0.01, 0.99, 0.01)\npercentiles &lt;- quantile(heights$height, p)\n\nConfirm that the 25th and 75th percentiles match that 1st and 3rd quartiles. Note that quantile() returns a named vector. You can access the 25th and 75th percentiles like this (adapt the code for other percentile value):\n\npercentiles[names(percentiles) == \"25%\"]\n\n25% \n 66 \n\npercentiles[names(percentiles) == \"75%\"]\n\n75% \n 71"
  },
  {
    "objectID": "posts/dataviz/index.html#finding-quantile-with-qnorm",
    "href": "posts/dataviz/index.html#finding-quantile-with-qnorm",
    "title": "Data Visualization",
    "section": "Finding quantile with qnorm",
    "text": "Finding quantile with qnorm\n\nDefiniton of qnorm\n简单来说,qnorm是正态分布累积分布函数(CDF)的反函数， 也就是说它可以视为pnorm的反函数, 这里q指的是quantile, 即分位数\nThe qnorm() function gives the theoretical value of a quantile with probability p of observing a value equal to or less than that quantile value a normal distribution with mean mu and standard deviation sigma:\n\nqnorm(p, mu, sigma)\n\nBy default, mu=0 and sigma=1. Therefore, calling qnorm() with no arguments gives quantiles for the standard normal distribution.\n\nqnorm(p)\n\nRecall that quantiles are defined such that \\(p\\) is the probability of a random observation less than or equal to the quantile.\n\n\nRealation to pnorm\nThe pnorm() function gives the probability that a value from a standard normal distribution will be less than or equal to a z-score value z. consider: \\[pnorm(-1.96)\\approx0.025\\] The result of pnorm() is the quantile. Note that: \\[qnorm(0.025)\\approx-1.96\\] qnorm() and pnorm are inverse functions: \\[pnorm(qnorm(0.025))\\equiv0.025\\]\n\n\nTheoretical quantiles\nYou can use qnorm() to determine the theoretical quantiles of a dataset: that is, the theoretical value of quantiles assuming that a dataset follows a normal distribution. Run the qnorm() function with the desired probabilities p, mean mu and standard deviation sigma.\nSuppose male heights follow a normal distribution with a mean of 69 inches and standard deviation of 3 inches. The theoretical quantiles are:\n\np &lt;- seq(0.01, 0.99, 0.01)\ntheoretical_quantiles &lt;- qnorm(p, 69, 3)\n\nTheoretical quantiles can be compared to sample quantiles determined with the quantile function in order to evaluate whether the sample follows a normal distribution."
  },
  {
    "objectID": "posts/dataviz/index.html#quantile-quantile-plots",
    "href": "posts/dataviz/index.html#quantile-quantile-plots",
    "title": "Data Visualization",
    "section": "Quantile-Quantile Plots",
    "text": "Quantile-Quantile Plots\n\nKey Points:\n\nQuantile-quantile plots, or QQ-plot, are used to check whether distributions are well-approximated by a normal distribution.\nGiven a proportion p, the quantile q is the value such that the proportion of values in the data blew q is p.\nIn a QQ-plot, the sample quantiles in the observed data are compared to the theoretical quantiles expected from the normal distribution. If the data are well-approximated by the normal distribution, then the points on the QQ-plot will fall near the identity line(sample = theoretical).\nCalculate sample quantiles (observed quantiles) using the quantile() function.\nCalculate theoretical quantiles with the qnorm() function. qnorm() will caculate quantiles for the standard normal distribution (\\(\\mu=0, \\sigma=1\\)) by default, but it can calculate quantiles for any normal distribution given mean() and sd() arguments.\n\n\n\nCode:\n\n# define x and z\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(heights)\n\nindex &lt;- heights$sex==\"Male\"\nx &lt;- heights$height[index]\nz &lt;- scale(x)\n\n# proportion of data below 69.5\nmean(x &lt;= 69.5)\n\n[1] 0.5147783\n\n# calculate observed and theoretical quantiles\np &lt;- seq(0.05, 0.95, 0.05)\nobserved_quantiles &lt;- quantile(x, p)\ntheoretical_quantiles &lt;- qnorm(p, mean = mean(x), sd = sd(x))\n\n# make QQ-plot\nplot(theoretical_quantiles, observed_quantiles)\nabline(0,1)\n\n\n\n\n\n\n\n# make QQ-plot with scaled values\nobserved_quantiles &lt;- quantile(z, p)\ntheoretical_quantiles &lt;- qnorm(p)\nplot(theoretical_quantiles, observed_quantiles)\nabline(0,1)"
  },
  {
    "objectID": "posts/dataviz/index.html#percentiles-1",
    "href": "posts/dataviz/index.html#percentiles-1",
    "title": "Data Visualization",
    "section": "Percentiles",
    "text": "Percentiles\n\nKey Points:\n\nPercentiles are the quantiles obtained when defining \\(p\\) as 0.01, 0.02,…,0.99. They summarize the values at which a certain percent of the observations are equal to or less than that value.\nThe 50th percentile is also known as the median.\nThe quartiles are the 25th, 50th and 75th percentiles."
  },
  {
    "objectID": "posts/dataviz/index.html#boxplots",
    "href": "posts/dataviz/index.html#boxplots",
    "title": "Data Visualization",
    "section": "Boxplots",
    "text": "Boxplots\nR语言如何绘制箱线图\n\nKey Points:\n\nWhen data do not follow a normal distribution and cannot be succinctly summarized by only the mean and standard deviation, an alternative is to report a five-number summary: range (ignoring outliers) and the quartiles (25th, 50th, 75th percentile).\nIn a boxplot, the box is defined by the 25th and 75th percentiles and the median is a horizontal line through the box. The whiskers show the range excluding outliers, and outliers are plotted separately as individual points.\nThe interquartile range is the distance between the 25th and 75th percentiles.\nBoxplots are particularly useful when comparing multiple distributions."
  },
  {
    "objectID": "posts/dataviz/index.html#distribution-of-female-heights",
    "href": "posts/dataviz/index.html#distribution-of-female-heights",
    "title": "Data Visualization",
    "section": "Distribution of Female Heights",
    "text": "Distribution of Female Heights\n\nKey Points:\n\nIf a distribution is not normal, it cannot be summarized with only the mean and standard seviation. Provide a histogram, smooth density or boxplot instead.\nA plot can force us to see unexpected results that make us question the quality or implication of our data."
  },
  {
    "objectID": "posts/dataviz/index.html#overview-1",
    "href": "posts/dataviz/index.html#overview-1",
    "title": "Data Visualization",
    "section": "Overview",
    "text": "Overview\nAfter completing ggplot2, we will:\n\nbe able to use ggplot2 to create data visualizations in R.\nbe able to explain what the data component of a graph is.\nbe able to identify the geometry component of a graph and know when to use which type of geometry. be able to explain what the aesthetic mapping component of a graph is.\nbe able to understand the scale component of a graph and select an appropriate scale component to use."
  },
  {
    "objectID": "posts/dataviz/index.html#ggplot",
    "href": "posts/dataviz/index.html#ggplot",
    "title": "Data Visualization",
    "section": "ggplot",
    "text": "ggplot\n\nggplot2\n\nData visualization with ggolot2\nData visualization with ggplot2: Cheat Sheet\nThe R graph gallery example\n\n\n\nkey Points:\n\nThroughout the series, we will create plots with the ggplot2 package. ggplot2 is part of the tidyverse suite of package, which you can load with library(tidyverse).\nNote that you can also load ggplot2 alone using the command library(ggplot2), instead of loading the entire tidyverse.\nggplot2 uses a grammar of graphics to break plots into building blocks that have intuitive syntax, making it easy to create relatively complex and aesthetically pleasing plots with relatively simple and readable code.\nggplot2 is designed to work excusively with tidy data (rows are observations and columns are variables)."
  },
  {
    "objectID": "posts/dataviz/index.html#graph-components",
    "href": "posts/dataviz/index.html#graph-components",
    "title": "Data Visualization",
    "section": "Graph Components",
    "text": "Graph Components\n\nKey Points:\n\nPlots in ggplot2 consist of 3 main components:\n\nData: The dataset being summarized\nGeometry: The type of plot(scatterplot, boxplot, barplot, histogram, qqplot, smooth desity, etc.)\nAesthetic mapping: Variable mapped to visual cues, such as x-axis and y-axis values and color.\n\n\n\n\nCode:\n\nlibrary(dslabs)\ndata(murders)"
  },
  {
    "objectID": "posts/dataviz/index.html#creating-a-new-plot",
    "href": "posts/dataviz/index.html#creating-a-new-plot",
    "title": "Data Visualization",
    "section": "Creating a New Plot",
    "text": "Creating a New Plot\n\nKey Points:\n\nYou can associated a dataset x with a ggplot object with any of the 3 commands:\n\nggplot(data = x)\nggplot(x)\nx %&gt;% ggplot()\n\nYou can assign a ggplot object to a variable. If the object is not assigned to a variable, it will automatically be displayed.\nYou can display a ggplot object assigned to a variable by printing that variable.\n\nCode:\n\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(murders)\n\nggplot(data = murders)\n\nmurders %&gt;% ggplot()\n\np &lt;- ggplot(data = murders)\n\nclass(p)\n\nprint(p) # this is equivalent to simply typing p\np"
  },
  {
    "objectID": "posts/dataviz/index.html#layers",
    "href": "posts/dataviz/index.html#layers",
    "title": "Data Visualization",
    "section": "Layers",
    "text": "Layers\n\nKey Points:\n\nIn ggplot2, graphs are created by adding layers to the ggplot object: DATA %&gt;% ggplot() + LAYER_1 + LAYER_2 + … + LAYER_N\nThe geometry layer defines that plot type and takes the format geom_x where x is the plot type.\nAesthetic mappings describe how properties of the data connect with features of the graph (axis position, color, size, etc.) define aesthetic mapping with aes() function.\naes() uses variable names from the object component (for example, total rather than murders$total).\ngeom_point() creates a scatterplot and requires x and y aesthetic mappings.\ngeom_text() and geom_label add text to a scatterplot and require x, y, and label aesthetic mappings.\nTo determine which aesthetic mappings are required for a geometry, read the help file for that geometry.\nYou can add layers with different aesthetic mappings to the same graph.\n\nCode: Adding layers to a plot\n\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(murders)\n\nmurders %&gt;% ggplot() +\n  geom_point(aes(x = population/10^6, y = total))\n\n\n# add points layer to predefined ggplot object\np &lt;- ggplot(data = murders)\np + geom_point(aes(population/10^6, total))\n\n\n\n\n\n\n\n# add text layer to scatterplot\np + geom_point(aes(population/10^6, total)) +\n  geom_text(aes(population/10^6, total, label = abb))\n\n\n\n\n\n\n\n\nCode: Example of aes behavior\n\n# no error from this call\np_test &lt;- p + geom_text(aes(population/10^6, total, lable = abb))\n\n# error - \"abb\" is not a globally defined variable and cannot be found outside of aes\np_test &lt;- p + geom_text(aes(population/10^6, total), label = abb)"
  },
  {
    "objectID": "posts/dataviz/index.html#thinkering",
    "href": "posts/dataviz/index.html#thinkering",
    "title": "Data Visualization",
    "section": "Thinkering",
    "text": "Thinkering\n\nKey Points:\n\nYou can modify arguments to geometry functions others than aes() and the data.\nThese arguments are not aesthetic mappings: the affect all data points the same way.\nGlobal aesthetic mappings apply to all geometries and can be defined when you initially call ggplot(). All the geometries added as layers will default to this mapping. Local aesthetic mapping add additional information or override the default mappings.\n\n\n\n\n\n\n\nNudge points a fixed distance\n\n\n\nposition_nudge(x = 0, y = 0) is generally useful for adjusting the position of items on discrete scales by a small amount. Nudging is built in to geom_text() because it’s so useful for moving labels a small distance from what they’re labeling.\n\n\nCode:\n\n# change the size of the points\np + geom_point(aes(population/10^6, total), size = 3) +\n    geom_text(aes(population/10^6, total, label = abb))\n\n\n\n\n\n\n\n# move text labels slightly to the right\np + geom_point(aes(population/10^6, total), size = 3) +\n    geom_text(aes(population/10^6, total, label = abb), nudge_x = 1)\n\n\n\n\n\n\n\n# simplify code by adding global aesthetic\np &lt;- murders %&gt;% ggplot(aes(population/10^6, total, label = abb))\np + geom_point(size = 3) +\n    geom_text(nudge_x = 1.5)\n\n\n\n\n\n\n\n# local aesthetics override global aesthetics\np + geom_point(size = 3) +\n  geom_text(aes(x = 10, y = 800, label = \"Hello there!\"))\n\nWarning in geom_text(aes(x = 10, y = 800, label = \"Hello there!\")): All aesthetics have length 1, but the data has 51 rows.\nℹ Did you mean to use `annotate()`?"
  },
  {
    "objectID": "posts/dataviz/index.html#scales-labels-and-colors",
    "href": "posts/dataviz/index.html#scales-labels-and-colors",
    "title": "Data Visualization",
    "section": "Scales, Labels, and Colors",
    "text": "Scales, Labels, and Colors\n\nTextbook links:\n\nTextbook section on scales\nTextbook section on labels and titles\nTextbook section on categories as colors\nTextbook section on annotation, shapes and adjustments\n\n\n\nKey Points:\n\nConvert the x-axis to log scale with scale_x_continuous(trans = \"log10\") or scale_x_log10(). Similar function exist for the y-axis.\nAdd axis title with xlab() and ylab() function. Add a plot title with the ggtitle() function.\nAdd a color mapping that colors points by a varaibale by defining col argument within aes(). To color all pints the same way, define col outside of aes().\nAdd a line with the geom_abline() geometry. geom_abline() takes arguments slop (default = 1) and intercept(default = 0). Change the color with col or color and line type with lty.\nPlacing the line layer after the point layer will overlay the the line on top of the points. To overlay points on the line, place the line layer before the point layer.\nThere are many additional ways to tweak your graph that can be found in the ggplot2 documentation, cheat sheet or on the internet. For example, you can change the legend title with scale_color_discrete.\n\n\n\nCode: Log-scale the x-axis and y-axis\n\n# define p\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(murders)\n\np &lt;- murders %&gt;% ggplot(aes(population/10^6, total, label = abb))\n\n# log base 10 scale the x-axis and y-axis\np + geom_point(size = 3) +\n    geom_text(nudge_x = 0.05) +\n    scale_x_continuous(trans = \"log10\") +\n    scale_y_continuous(trans = \"log10\")\n\n\n# efficient log scaling of the axes\np + geom_point(size = 3) +\n    geom_text(nudge_x = 0.05) +\n    scale_x_log10() +\n    scale_y_log10()\n\n\n\n\n\n\n\n\n\n\nCode: Add labels and title\n\np + geom_point(size = 3) +\n    geom_text(nudge_x = 0.05) +\n    scale_x_log10() +\n    scale_y_log10() +\n    xlab(\"Population in million(log scale)\") +\n    ylab(\"Total number of murders(log scale)\") +\n    ggtitle(\"US Gun Murders in 2010\")\n\n\n\n\n\n\n\n\n\n\nCode: Change color of the points\n\n# redefine p to be everything except the points layer\np &lt;- murders %&gt;% \n     ggplot(aes(population/10^6, total, label = abb)) +\n     geom_text(nudge_x = 0.075) +\n     scale_x_log10() +\n     scale_y_log10() +\n     xlab(\"Population in million(log scale)\") +\n     ylab(\"Total number of murders(log scale)\") +\n     ggtitle(\"US Gun Murders in 2010\")\n\n\n# make all points blue\np + geom_point(size = 3, color = \"blue\")\n\n\n\n\n\n\n\n\n\n# color points by region\np + geom_point(aes(col = region), size = 3)\n\n\n\n\n\n\n\n\n\n\nCode: Add a line with average murder rate\n\nr &lt;- murders %&gt;% \n     summarize(rate = sum(total) / sum(population) * 10^6) %&gt;%      pull(rate)\n\np &lt;- p + geom_point(aes(col = region), size = 3) +\n         geom_abline(intercept = log10(r)) # slop is default of 1\n\n# change line to dashed and dark grey, line under points\np + geom_abline(intercept = log(r), lty = 2, color = \"darkgrey\") +\n    geom_point(aes(col = region), size = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLine types in R: Ity\n\n\n\nThe different line types available in R are shown in the figure hereafter. The argument lty can be used to specify the line type. To change line width, the argument lwd can be used.\n\n\n\n\nCode: Change legend title\n\n# capitalize legend title\np &lt;- p + scale_color_discrete(name = \"Region\")\np"
  },
  {
    "objectID": "posts/dataviz/index.html#add-on-packages",
    "href": "posts/dataviz/index.html#add-on-packages",
    "title": "Data Visualization",
    "section": "Add-on packages",
    "text": "Add-on packages\n\nTextbook links:\n\nTextbook section on add-on packages\nTextbook section on putting it all together\n\n\n\nKey Points\n\nThe style of a ggplot graph can be changed using the theme() function.\nThe ggthemes package adds additional themes.\nThe ggrepel package includes a geometry that repels text labels, ensuring they do not overlap with each other: geom_text_repel().\n\n\n\nCode: Adding themes\n\n# theme used for graphs in the textbook and course\nlibrary(dslabs)\nds_theme_set()\n\n\n# themes from ggthemes\nlibrary(ggthemes)\n\n\np + theme_economist()    # style of the Economist magazine\n\n\n\n\n\n\n\np + theme_fivethirtyeight()    # style of the FiveThirtyEight website\n\n\n\n\n\n\n\n\n\n\nCode: Putting it all together to assemble the plot\n\n# load libraries\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(ggthemes)\nlibrary(dslabs)\ndata(murders)\n\n\n# define the intercept\nr &lt;- murders %&gt;%\n    summarize(rate = sum(total) / sum(population) * 10^6) %&gt;%\n    .$rate\n    \n# make the plot, combining all elements\nmurders %&gt;%\n    ggplot(aes(population/10^6, total, label = abb)) +\n    geom_abline(intercept = log10(r), lty = 2, color = \"darkgrey\") +\n    geom_point(aes(col = region), size = 3) +\n    geom_text_repel() +\n    scale_x_log10() +\n    scale_y_log10() +\n    xlab(\"Population in millions (log scale)\") +\n    ylab(\"Total number of murders (log scale)\") +\n    ggtitle(\"US Gun Murders in 2010\") +\n    scale_color_discrete(name = \"Region\") +\n    theme_economist()"
  },
  {
    "objectID": "posts/dataviz/index.html#other-examples",
    "href": "posts/dataviz/index.html#other-examples",
    "title": "Data Visualization",
    "section": "Other Examples",
    "text": "Other Examples\n\nTextbook links:\n\nTextbook section on histograms\nTextbook section on density plots\nTextbook section on grids of plots\n\n\n\nKey points\n\ngeom_histogram() creates a histogram. Use the binwidth argument to change the width of bins, the fill argument to change the bar fill color, and the col argument to change bar outline color.\ngeom_density() creates smooth density plots. Change the fill color of the plot with the fill argument.\ngeom_qq() creates a quantile-quantile plot. This geometry requires the sample argument. By default, the data are compared to a standard normal distribution with a mean of 0 and standard deviation of 1. This can be changed with the dparams argument, or the sample data can be scaled.\nPlots can be arranged adjacent to each other using the grid.arrange() function from the gridExtra package. First, create the plots and save them to objects (p1, p2, …). Then pass the plot objects to grid.arrange().\n\n\n\nCode: Histograms in ggplot2\n\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(heights)\n\n# define p\np &lt;- heights %&gt;% \n  filter(sex == \"Male\") %&gt;% \n  ggplot(aes(x=height))\n\n\n# basic histograms\np + geom_histogram() + ggtitle(\"binwidth is default\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\np + geom_histogram(binwidth = 1) + ggtitle(\"binwidth is 1\")\n\n\n\n\n\n\n\n# histogram with blue fill, black outline, labels and title\np + geom_histogram(binwidth = 1, fill =\"blue\", col = \"black\") + \n  xlab(\"Male heights in inches\") +\n  ggtitle(\"histogram\")\n\n\n\n\n\n\n\n\n\n\nCode: Smooth density plots in ggplot2\n\np + geom_density()\n\n\n\n\n\n\n\np + geom_density(fill = \"blue\", col = \"red\") +\n  xlab(\"Male heights in inches\") +\n  ylab(\"proportion of Male heights\") +\n  ggtitle(\"Male heights distribution\")\n\n\n\n\n\n\n\n\n\n\nCode: Quantile-quantile plots in ggplot2\n\n# basic QQ-plot\np &lt;- heights %&gt;% filter(sex == \"Male\") %&gt;% \n  ggplot(aes(sample = height))\np + geom_qq()\n\n\n\n\n\n\n\n# QQ-plot against a normal distribution with same mean/sd as data\nparams &lt;- heights %&gt;% \n  filter(sex == \"Male\") %&gt;% \n  summarize(mean = mean(height), sd = sd(height))\np + geom_qq(dparams = params) +\n  geom_abline()\n\n\n\n\n\n\n\n# QQ-plot of scaled data against the standard normal distribution\nheights %&gt;% \n  ggplot(aes(sample = scale(height))) +\n  geom_qq() +\n  geom_abline()\n\n\n\n\n\n\n\n\n\n# define plots p1, p2, p3\np &lt;- heights %&gt;% filter(sex == \"Male\") %&gt;% ggplot(aes(x = height))\np1 &lt;- p + geom_histogram(binwidth = 1, fill = \"blue\", col = \"black\")\np2 &lt;- p + geom_histogram(binwidth = 2, fill = \"blue\", col = \"black\")\np3 &lt;- p + geom_histogram(binwidth = 3, fill = \"blue\", col = \"black\")\n\n\n# arrange plots next to each other in 1 row, 3 columns\nlibrary(gridExtra)\n\n\ngrid.arrange(p1, p2, p3, ncol = 3)"
  },
  {
    "objectID": "posts/dataviz/index.html#overview-2",
    "href": "posts/dataviz/index.html#overview-2",
    "title": "Data Visualization",
    "section": "Overview",
    "text": "Overview\nAfter completing Gapminder, you will: - understand how Hans Rosling and the Gapminder Foundation use effective data visualization to convey data-based trends.\n\nbe able to apply the ggplot2 techniques from the previous section to answer questions using data.\nunderstand how fixed scales across plots can ease comparisons.\nbe able to modify graphs to improve data visualization."
  },
  {
    "objectID": "posts/dataviz/index.html#introduction-to-gapminder",
    "href": "posts/dataviz/index.html#introduction-to-gapminder",
    "title": "Data Visualization",
    "section": "Introduction to Gapminder",
    "text": "Introduction to Gapminder\nCase study: Trends in World Health and Economics\nData Source form Gapminder\nWe will use this data to answer the following questions about World Health and Economics: - Is it still fair to consider the world as divided into the West and the developing world? - Has income inequality across countries worsened over the last 40 years?"
  },
  {
    "objectID": "posts/dataviz/index.html#gapminder-dataset",
    "href": "posts/dataviz/index.html#gapminder-dataset",
    "title": "Data Visualization",
    "section": "Gapminder Dataset",
    "text": "Gapminder Dataset\n\nKey Points\n\nA selection of world health and economics statistics from the Gapminder project can be found in the dslabs package as data(gapminder).\nMost people have misconceptions about world health and economics, which can be addressed by considering real data.\n\n\n\nCode\n\nlibrary(dslabs)\ndata(\"gapminder\")\n\n\nhead(gapminder)\n\n              country year infant_mortality life_expectancy fertility\n1             Albania 1960           115.40           62.87      6.19\n2             Algeria 1960           148.20           47.50      7.65\n3              Angola 1960           208.00           35.98      7.32\n4 Antigua and Barbuda 1960               NA           62.97      4.43\n5           Argentina 1960            59.87           65.39      3.11\n6             Armenia 1960               NA           66.86      4.55\n  population          gdp continent          region\n1    1636054           NA    Europe Southern Europe\n2   11124892  13828152297    Africa Northern Africa\n3    5270844           NA    Africa   Middle Africa\n4      54681           NA  Americas       Caribbean\n5   20619075 108322326649  Americas   South America\n6    1867396           NA      Asia    Western Asia\n\nnames(gapminder)\n\n[1] \"country\"          \"year\"             \"infant_mortality\" \"life_expectancy\" \n[5] \"fertility\"        \"population\"       \"gdp\"              \"continent\"       \n[9] \"region\"          \n\n\n\ngapminder %&gt;% \n  filter(year == 2015 & country %in% c(\"Sri Lanka\", \"Turkey\")) %&gt;% \n  select(country, infant_mortality)\n\n    country infant_mortality\n1 Sri Lanka              8.4\n2    Turkey             11.6"
  },
  {
    "objectID": "posts/dataviz/index.html#life-expectancy-and-fertility-rates",
    "href": "posts/dataviz/index.html#life-expectancy-and-fertility-rates",
    "title": "Data Visualization",
    "section": "Life Expectancy and Fertility Rates",
    "text": "Life Expectancy and Fertility Rates\n\nKey Points\n\nA prevalent worldview is that the world is divided into two groups of countries:\n\nWestern world: high life expectancy, low fertility rate\nDeveloping world: lower life expectancy, higher fertility rate\n\nGapminder data can be used to evaluate the validity of this view.\nA scatterplot of life expectancy versus fertility rate in 1962 suggests that this viewpoint was grounded in reality 50 years ago. Is it still the case today?\n\n\n\nCode\n\n# basic scatterplot of life expectancy versus fertility\nds_theme_set() # set plot theme\nfilter(gapminder, year == 1962) %&gt;% \n  ggplot(aes(fertility, life_expectancy)) +\n  geom_point()\n\n\n\n\n\n\n\n# add color as continent\nfilter(gapminder, year == 1962) %&gt;% \n  ggplot(aes(fertility, life_expectancy, color = continent)) +\n  geom_point()"
  },
  {
    "objectID": "posts/dataviz/index.html#faceting",
    "href": "posts/dataviz/index.html#faceting",
    "title": "Data Visualization",
    "section": "Faceting",
    "text": "Faceting\n\nKey Points\n\nFaceting makes multiple side-by-side plots stratified by some variable. This is a way to ease comparisons.\nThe facet_grid() function allows faceting by up to two variables, with rows faceted by one variable and columns faceted by the other variable. To facet by only one variable, use the dot operator as the other variable.\nThe facet_wrap() function facets by one variable and automatically wraps the series of plots so they have readable dimensions.\nFaceting keeps the axes fixed across all plots, easing comparisons between plots.\nThe data suggest that the developing versus Western world view no longer makes sense in 2012.\n\n ggplot2-分面(facet) 一页多图数据可视化章节学习facet\n\n\nCode\n\n# facet by continent and year\nfilter(gapminder, year %in% c(1962, 2012)) %&gt;% \n  ggplot(aes(fertility, life_expectancy, col = continent)) +\n  geom_point() +\n  facet_grid(continent ~ year)\n\n\n\n\n\n\n\n# facet by year only \nfilter(gapminder, year %in% c(1962, 2012)) %&gt;% \n  ggplot(aes(fertility, life_expectancy, col = continent)) +\n  geom_point() +\n  facet_grid(. ~ year)\n\n\n\n\n\n\n\n# facet by year, plots wrapped onto multiple rows\nyears &lt;- c(1962, 1980, 1990, 2000, 2012)\ncontinents &lt;- c(\"Europ\", \"Asia\")\ngapminder %&gt;% \n  filter(year %in% years & continent %in% continent) %&gt;% \n  ggplot(aes(fertility, life_expectancy, col = continent)) +\n  geom_point() +\n  facet_wrap(. ~ year)"
  },
  {
    "objectID": "posts/dataviz/index.html#time-series-plots",
    "href": "posts/dataviz/index.html#time-series-plots",
    "title": "Data Visualization",
    "section": "Time Series Plots",
    "text": "Time Series Plots\n\nKey Points\n\nTime series plots have time on the x-axis and a variable of interest on the y-axis.\nThe geom_line() geometry connects adjacent data points to form a continuous line. A line plot is appropriate when points are regularly spaced, densely packed and from a single data series.\nYou can plot multiple lines on the same graph. Remember to group or color by a variable so that the lines are plotted independently.\nLabeling is usually preferred over legends. However, legends are easier to make and appear by default. Add a label with geom_text(), specifying the coordinates where the label should appear on the graph.\n\n\n\nCode: Single Time Series\n\n# scatterplot of US fertility by year\ngapminder %&gt;% \n  filter(country == \"United States\") %&gt;% \n  ggplot(aes(year, fertility)) +\n  geom_point()\n\n\n\n\n\n\n\n# line plot of US fertility by year\ngapminder %&gt;% \n  filter(country == \"United States\") %&gt;% \n  ggplot(aes(year, fertility)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\nCode: Multiple Time Series\n\n# line plot fertility time series for two countries- only one line (incorrect)\ncountries &lt;- c(\"South Korea\", \"Germany\")\ngapminder %&gt;% filter(country %in% countries) %&gt;%\n    ggplot(aes(year, fertility)) +\n    geom_line()\n\n\n\n\n\n\n\n# line plot fertility time series for two countries - one line per country\ngapminder %&gt;% filter(country %in% countries) %&gt;%\n    ggplot(aes(year, fertility, group = country)) +\n    geom_line()\n\n\n\n\n\n\n\n# fertility time series for two countries - lines colored by country\ngapminder %&gt;% filter(country %in% countries) %&gt;%\n    ggplot(aes(year, fertility, col = country)) +\n    geom_line()\n\n\n\n\n\n\n\n\n\n\nCode: Adding text labels to a plot\n\n\n\n\n\n\nNote\n\n\n\nlabels data frame as the data to ensure where to start label text \n\n\n\n# life expectancy time series - lines colored by country and labeled, no legend\nlabels &lt;- data.frame(country = countries, x = c(1975, 1965), y = c(60, 72))\ngapminder %&gt;% filter(country %in% countries) %&gt;%\n    ggplot(aes(year, life_expectancy, col = country)) +\n    geom_line() +\n    geom_text(data = labels, aes(x, y, label = country), size = 5) +\n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/dataviz/index.html#transformations",
    "href": "posts/dataviz/index.html#transformations",
    "title": "Data Visualization",
    "section": "Transformations",
    "text": "Transformations\n\nKey Points\n\nWe use GDP data to compute income in US dollars per day, adjusted for inflation.\nLog transformations covert multiplicative changes into additive changes.\ncommon transformations are the log base 2 transformation and the log base 10 transformation. The choice of base depends on the range of the data. The natural log is not recommended for visualization because it is difficult to interpret.\nThe mode of a distribution is the value with the highest frequency. The mode of a normal distribution is the average. A distribution can have multiple local modes.\nThere are two ways to use log transformations in plots: transform the data before plotting or transform the axes of the plot. Log scales have the advantage of showing the original values as axis labels, while log transformed values ease interpretation of intermediate values between labels.\nScale the x-axis using scale_x_continuous() or scale_x_log10() layers in ggplot2. Similar functions exist for the y-axis.\nIn 1970, income distribution is bimodal, consistent with the dichotomous Western versus developing worldview.\n\n\n\nCode\n\n# add dollars per day variable\ngapminder &lt;- gapminder %&gt;% \n  mutate(dollars_per_day = gdp/population/365)\n\n# histogram of dollars per day\npast_year &lt;- 1970\ngapminder %&gt;% \n  filter(year == past_year & !is.na(gdp)) %&gt;% \n  ggplot(aes(dollars_per_day)) +\n  geom_histogram(binwidth = 1, color = \"black\")\n\n\n\n\n\n\n\n# repeat histogram with log2 scaled data\ngapminder %&gt;%\n    filter(year == past_year & !is.na(gdp)) %&gt;%\n    ggplot(aes(log2(dollars_per_day))) +\n    geom_histogram(binwidth = 1, color = \"black\")\n\n\n\n\n\n\n\n# repeat histogram with log2 scaled x-axis\ngapminder %&gt;%\n    filter(year == past_year & !is.na(gdp)) %&gt;%\n    ggplot(aes(dollars_per_day)) +\n    geom_histogram(binwidth = 1, color = \"black\") +\n    scale_x_continuous(trans = \"log2\")"
  },
  {
    "objectID": "posts/dataviz/index.html#stratify-and-boxplot",
    "href": "posts/dataviz/index.html#stratify-and-boxplot",
    "title": "Data Visualization",
    "section": "Stratify and Boxplot",
    "text": "Stratify and Boxplot\n\nKey Points\n\nMake boxplots stratified by a categorical variable using the geom_boxplot() geometry.\nRotate axis labels by changing the theme through element_text(). You can change the angle and justification of the text labels.\nConsider ordering your factors by a meaningful value with the reorder function, which changes the order of factor levels based on a related numeric vector. This is a way to ease comparisons.\nShow the data by adding data points to the boxplot with a geom_point layer. This adds information beyond the five-number summary to your plot, but too many data points it can obfuscate your message.\n\n\n\nCode: Boxplot of GDP by region\n\n# add dollars per day variable\ngapminder &lt;- gapminder %&gt;% \n  mutate(dollars_per_day = gdp/population/365)\n\n# number of regions\nlength(levels(gapminder$region))\n\n[1] 22\n\n# boxplot of GDP by region in 1970\npast_year &lt;- 1970\np &lt;- gapminder %&gt;% \n     filter(year == past_year & !is.na(gdp)) %&gt;% \n     ggplot(aes(region, dollars_per_day))\np + geom_boxplot()\n\n\n\n\n\n\n\n# roation name on x-axis\np + geom_boxplot() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n\n\n\nCode: The reorder function\n\n\n\n\n\n\nTip\n\n\n\nReorder a variable with ggplot2\n\n\n\n# by default, factor order is alphabetical\nfac &lt;- factor(c(\"Asia\", \"Asia\", \"West\", \"West\", \"West\"))\nlevels(fac)\n\n[1] \"Asia\" \"West\"\n\n# reorder factor by the category means\nvalue &lt;- c(10, 11, 12, 6, 4)\nfac &lt;- reorder(fac, value, FUN = mean)\nlevels(fac)\n\n[1] \"West\" \"Asia\"\n\n\n\n\nCode: Enhanced boxplot ordered by median income, scaled, and showing data\n\n# reorder by median income and color by continent \np &lt;- gapminder %&gt;%\n    filter(year == past_year & !is.na(gdp)) %&gt;%\n    mutate(region = reorder(region, dollars_per_day, FUN = median)) %&gt;%  # reorder\n    ggplot(aes(region, dollars_per_day, fill = continent)) + # color by continent \n    geom_boxplot() +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n    xlab(\"\")\np\n\n\n\n\n\n\n\n# log2 scale y-axis\np + scale_y_continuous(trans = \"log2\")\n\n\n\n\n\n\n\n# add data points\np + scale_y_continuous(trans = \"log2\") + geom_point(show.legend = FALSE)"
  },
  {
    "objectID": "posts/dataviz/index.html#comparing-distributions",
    "href": "posts/dataviz/index.html#comparing-distributions",
    "title": "Data Visualization",
    "section": "Comparing Distributions",
    "text": "Comparing Distributions\n\n\n\n\n\n\nImportant\n\n\n\nintersect(交集);union(并集);setdiff(找不同);setequal(判断相同)\n\n\n\nKey Points\n\nUse intersect to find the overlap between two vectors.\nTo make boxplots where grouped variables are adjacaent, color the boxplot by a factor instead of faceting by that factor. This is a way to ease comparisions.\nThe data suggest that the income gap between rich and poor countries has narrowed, not expended.\n\n\n\nCode: Histogram of income in West versus developing world, 1970 and 2010\n\n# add dollars per day variable and define past year\ngapminder &lt;- gapminder %&gt;% \n  mutate(dollars_per_day = gdp/population/365)\npast_year &lt;- 1970\n\n# define Western countries\nwest &lt;- c(\"Western Europe\", \"Northern Europe\", \"Southern Europe\", \"Northern America\", \"Australia and New Zealand\")\n\n# facet by West vs Devloping \ngapminder %&gt;% \n  filter(year == past_year & !is.na(gdp)) %&gt;% \n  mutate(group = ifelse(region %in% west, \"West\", \"Developing\")) %&gt;% \n  ggplot(aes(dollars_per_day)) +\n  geom_histogram(binwidth = 1, color = \"black\") +\n  scale_x_continuous(trans = \"log2\") +\n  facet_grid(. ~group)\n\n\n\n\n\n\n\n# facet by West/Developing and year\npresent_year &lt;- 2010\ngapminder %&gt;%\n    filter(year %in% c(past_year, present_year) & !is.na(gdp)) %&gt;%\n    mutate(group = ifelse(region %in% west, \"West\", \"Developing\")) %&gt;%\n    ggplot(aes(dollars_per_day)) +\n    geom_histogram(binwidth = 1, color = \"black\") +\n    scale_x_continuous(trans = \"log2\") +\n    facet_grid(year ~ group)\n\n\n\n\n\n\n\n\n\n\nCode: Income distribution of West verseus Developing world, only countries with data\n\n# define countries that have data available in both years\ncountry_list_1 &lt;- gapminder %&gt;% \n  filter(year == past_year & !is.na(dollars_per_day)) %&gt;% .$country\n\ncountry_list_2 &lt;- gapminder %&gt;% \n  filter(year == present_year & !is.na(dollars_per_day)) %&gt;% .$country\n\ncountry_list &lt;- intersect(country_list_1, country_list_2)\n\n# make histogram including only countries with data availabe in both years\ngapminder %&gt;% \n  filter(year %in% c(past_year, present_year) & country %in% country_list) %&gt;% # keep only selected countries\n  mutate(group = ifelse(region %in% west, \"West\", \"Developing\")) %&gt;% \n  ggplot(aes(dollars_per_day)) +\n  geom_histogram(binwidth = 1, color = \"black\") +\n  scale_x_continuous(trans = \"log2\") +\n  facet_grid(year ~ group)\n\n\n\n\n\n\n\n\n\n\nCode: Boxplots of income in West versus Developing world, 1970 and 2010\n\np &lt;- gapminder %&gt;% \n  filter(year %in% c(past_year, present_year) & country %in% country_list) %&gt;%\n  mutate(region = reorder(region, dollars_per_day, FUN = median)) %&gt;% \n  ggplot() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  xlab(\"\") + scale_y_continuous(trans = \"log2\") \n\np + geom_boxplot(aes(region, dollars_per_day, fill = continent)) +\n  facet_grid(year ~ .)\n\n\n\n\n\n\n\n# arrange matching boxplots next to each other, colored by year\np + geom_boxplot(aes(region, dollars_per_day, fill = factor(year)))"
  },
  {
    "objectID": "posts/dataviz/index.html#density-plots",
    "href": "posts/dataviz/index.html#density-plots",
    "title": "Data Visualization",
    "section": "Density Plots",
    "text": "Density Plots\n\n\n\n\n\n\nTip\n\n\n\n\ndplyr处理数据时常用的的函数\n在 R Dplyr 包中使用 case when 语句\n\n\n\n\nKey Points\n\nChange the y-axis of density plots to variable counts using ..count.. as the y argument.\nThe case_when() function defines a factor whose levels are defined by a variety of logical operations to group data.\nPlot stacked density plots using position=\"stack\".\nDefine a weight aesthetic mapping to change the relative weights of density plots-for example, this allow weighting of plots by population rather than number of countries.\n\n\n\nCode: Faceted smooth density plots\n\n# see the code below the previous video for variable definitions\n\n# smooth density plots - area under each curve adds to 1\ngapminder %&gt;% \n  filter(year == past_year & country %in% country_list) %&gt;% \n  mutate(group = ifelse(region %in% west, \"West\", \"Developing\")) %&gt;% group_by(group) %&gt;% \n  summarize(n = n()) %&gt;% knitr::kable()\n\n\n\n\ngroup\nn\n\n\n\n\nDeveloping\n87\n\n\nWest\n21\n\n\n\n\n# smooth density plots - variable counts on y-axis\np &lt;- gapminder %&gt;% \n  filter(year == past_year & country %in% country_list) %&gt;% \n  mutate(group = ifelse(region %in% west, \"West\", \"Developing\")) %&gt;%\n  ggplot(aes(dollars_per_day, y = ..count.., fill = group)) +\n  scale_x_continuous(trans = \"log2\")\np + geom_density(alpha = 0.2, bw = 0.75) + facet_grid(year ~ .)\n\n\n\n\n\n\n\n\n\n\nCode: Add new region group with case_when\n\n# add group as a factor, grouping regions\ngapminder &lt;- gapminder %&gt;% \n  mutate(group = case_when(\n    .$region %in% west ~ \"West\",\n    .$region %in% c(\"Eastern Asia\", \"South-Eastern Asia\") ~ \"East Asia\", \n    .$region %in% c(\"Caribbean\", \"Central America\", \"South America\") ~ \"Latin America\",\n    .$continent == \"Africa\" & .$region != \"Northern Africa\" ~ \"Sub-Saharan Africa\", TRUE ~ \"Others\"))\n\n# reorder factor levels\ngapminder &lt;- gapminder %&gt;% \n  mutate(group = factor(group, levels = c(\"Others\", \"Latin America\", \"East Asia\", \"Sub-Saharan Africa\", \"West\")))\n\n\n\nCode: Stacked density plot\n\n# note you must redefine p with the new gapminder object first\np &lt;- gapminder %&gt;% \n  filter(year %in% c(past_year, present_year) & country %in% country_list) %&gt;% \n  ggplot(aes(dollars_per_day, fill = group)) +\n  scale_x_continuous(trans = \"log2\")\n\n# stacked density plot\np + geom_density(alpha = 0.2, bw = 0.75, position = \"stack\") +\n  facet_grid(year ~ .)\n\n\n\n\n\n\n\n\n\n\nCode: Weighted stacked density plot\n\ngapminder %&gt;% \n  filter(year %in% c(past_year, present_year) & country %in% country_list) %&gt;% \n  group_by(year) %&gt;% \n  mutate(weight = population/sum(population*2)) %&gt;% \n  ungroup() %&gt;% \n  ggplot(aes(dollars_per_day, fill = group, weight = weight)) +\n  scale_x_continuous(trans = \"log2\") +\n  geom_density(alpha = 0.2, bw = 0.75, position = \"stack\") + facet_grid(year ~ .)"
  },
  {
    "objectID": "posts/dataviz/index.html#ecological-fallacy",
    "href": "posts/dataviz/index.html#ecological-fallacy",
    "title": "Data Visualization",
    "section": "Ecological Fallacy",
    "text": "Ecological Fallacy\n\nTextbook link\nEcological Fallacy\n\n\nKey Points\n\nThe breaks argument allows us to set the location of the axis labels and tick marks.\nthe logistic or logit transformation is defined as \\(f(p)=log\\frac{1}{1-p}\\), or the log of odds. This scale is useful for highlighting difference near 0 or near 1 and converts fold changes into constant increase.\nThe ecological fallacy is assuming that conclusion made from the average of a group apply to all members of that group.\n\n\n\nCode\n\n# define gapminder\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(gapminder)\n\n\n# add additional cases\ngapminder &lt;- gapminder %&gt;%\n    mutate(group = case_when(\n        .$region %in% west ~ \"The West\",\n        .$region %in% \"Northern Africa\" ~ \"Northern Africa\",\n        .$region %in% c(\"Eastern Asia\", \"South-Eastern Asia\") ~ \"East Asia\",\n        .$region == \"Southern Asia\" ~ \"Southern Asia\",\n        .$region %in% c(\"Central America\", \"South America\", \"Caribbean\") ~ \"Latin America\",\n        .$continent == \"Africa\" & .$region != \"Northern Africa\" ~ \"Sub-Saharan Africa\",\n        .$region %in% c(\"Melanesia\", \"Micronesia\", \"Polynesia\") ~ \"Pacific Islands\"))\n\n# define a data frame with group average income and average infant survival rate\nsurv_income &lt;- gapminder %&gt;%\n    filter(year %in% present_year & !is.na(gdp) & !is.na(infant_mortality) & !is.na(group)) %&gt;%\n    group_by(group) %&gt;%\n    summarize(income = sum(gdp)/sum(population)/365,\n                        infant_survival_rate = 1 - sum(infant_mortality/1000*population)/sum(population))\nsurv_income %&gt;% arrange(income)\n\n# A tibble: 7 × 3\n  group              income infant_survival_rate\n  &lt;chr&gt;               &lt;dbl&gt;                &lt;dbl&gt;\n1 Sub-Saharan Africa   1.76                0.936\n2 Southern Asia        2.07                0.952\n3 Pacific Islands      2.70                0.956\n4 Northern Africa      4.94                0.970\n5 Latin America       13.2                 0.983\n6 East Asia           13.4                 0.985\n7 The West            77.1                 0.995\n\n# plot infant survival versus income, with transformed axes\nsurv_income %&gt;% ggplot(aes(income, infant_survival_rate, label = group, color = group)) +\n    scale_x_continuous(trans = \"log2\", limit = c(0.25, 150)) +\n    scale_y_continuous(trans = \"logit\", limit = c(0.875, .9981),\n                                       breaks = c(.85, .90, .95, .99, .995, .998)) +\n    geom_label(size = 3, show.legend = FALSE)"
  },
  {
    "objectID": "posts/dataviz/index.html#overview-3",
    "href": "posts/dataviz/index.html#overview-3",
    "title": "Data Visualization",
    "section": "Overview",
    "text": "Overview\nData visualization principles covers some general principles that can serve as guides for effective data visualization.\nAfter completing this section, you will:\n\nunderstand basic principles of effective data visualization.\nunderstand the importance of keeping your goal in mind when deciding on a visualization approach.\nunderstand principles for encoding data, including position, aligned lengths, angles, area, brightness, and color hue.\nknow when to include the number zero in visualizations.\nbe able to use techniques to ease comparisons, such as using common axes, putting visual cues to be compared adjacent to one another, and using color effectively."
  },
  {
    "objectID": "posts/dataviz/index.html#encoding-data-using-visual-cues",
    "href": "posts/dataviz/index.html#encoding-data-using-visual-cues",
    "title": "Data Visualization",
    "section": "Encoding Data Using Visual Cues",
    "text": "Encoding Data Using Visual Cues\n\nKey Points\n\nVisual cues for encoding data include position, length, angle, area, brightness and color hue.\nPosition and length are the preferred way to display quantities, followed by angles, which are preferred over area. Brightness and color are even harder to quantify but can sometimes be useful.\nPie charts represent visual cues as both angles and area, while donut charts use only area. Humans are not good at visually quantifying angles and are even worse at quantifying area. Therefore pie and donut charts should be avoided - use a bar plot instead. If you must make a pie chart, include percentages as labels.\nBar plots represent visual cues as position and length. Humans are good at visually quantifying linear measures, making bar plots a strong alternative to pie or donut charts."
  },
  {
    "objectID": "posts/dataviz/index.html#know-when-to-include-zero",
    "href": "posts/dataviz/index.html#know-when-to-include-zero",
    "title": "Data Visualization",
    "section": "Know when to Include Zero",
    "text": "Know when to Include Zero\n\nKey Points\n\nWhen using bar plots, always start at 0. It is deceptive not to start at 0 because bar plots imply length is proportional to the quantity displayed. Cutting off the y-axis can make differences look bigger than they actually are.\nWhen using position rather than length, it is not necessary to include 0 (scatterplot, dot plot, boxplot)."
  },
  {
    "objectID": "posts/dataviz/index.html#do-not-distort-quantitles",
    "href": "posts/dataviz/index.html#do-not-distort-quantitles",
    "title": "Data Visualization",
    "section": "Do not Distort Quantitles",
    "text": "Do not Distort Quantitles\n\nKey Points\n\nMake sure your visualizations encode the correct quantities.\nFor example, if you are using a plot that relies on circle area, make sure the area (rather than the radius) is proportional to the quantity."
  },
  {
    "objectID": "posts/dataviz/index.html#order-by-a-meaningful-value",
    "href": "posts/dataviz/index.html#order-by-a-meaningful-value",
    "title": "Data Visualization",
    "section": "Order by a Meaningful Value",
    "text": "Order by a Meaningful Value\n\nKey Points\n\nIt is easiest to visually extract information from a plot when categories are ordered by a meaningful value. The exact value on which to order will depend on your data and the message you wish to convey with your plot.\nThe default ordering for categories is alphabetical if the categories are strings or by factor level if factors. However, we rarely want alphabetical order."
  },
  {
    "objectID": "posts/dataviz/index.html#show-the-data",
    "href": "posts/dataviz/index.html#show-the-data",
    "title": "Data Visualization",
    "section": "Show the Data",
    "text": "Show the Data\n\nKey Points\n\nA dynamite plot - a bar graph of group averages with error bars denoting standard errors - provides almost no information about a distribution.\nBy showing the data, you provide viewers extra information about distributions.\nJitter is adding a small random shift to each point in order to minimize the number of overlapping points. To add jitter, use the geom_jitter() geometry instead of geom_point(). (See example below.)\nAlpha blending is making points somewhat transparent, helping visualize the density of overlapping points. Add an alpha argument to the geometry.\n\n\n\nCode\n\n# dot plot showing the data\nheights %&gt;% ggplot(aes(sex, height)) + geom_point()\n\n\n\n\n\n\n\n# jittered, alpha blended point plot\nheights %&gt;% ggplot(aes(sex, height)) + geom_jitter(width = 0.1, alpha = 0.2)"
  },
  {
    "objectID": "posts/dataviz/index.html#ease-comparisons-use-common-axes",
    "href": "posts/dataviz/index.html#ease-comparisons-use-common-axes",
    "title": "Data Visualization",
    "section": "Ease Comparisons: Use Common Axes",
    "text": "Ease Comparisons: Use Common Axes\n\nKey Points\n\nEase comparisons by keeping axes the same when comparing data across multiple plots.\nAlign plots vertically to see horizontal changes. Align plots horizontally to see vertical changes.\nBar plots are useful for showing one number but not useful for showing distributions."
  },
  {
    "objectID": "posts/dataviz/index.html#consider-transformations",
    "href": "posts/dataviz/index.html#consider-transformations",
    "title": "Data Visualization",
    "section": "Consider Transformations",
    "text": "Consider Transformations\n\nKey Points\n\nUse transformations when warranted to ease visual interpretation.\nThe log transformation is useful for data with multiplicative changes. The logistic transformation is useful for fold changes in odds. The square root transformation is useful for count data."
  },
  {
    "objectID": "posts/dataviz/index.html#ease-comparisons-compared-visual-cues-should-be-adjacent",
    "href": "posts/dataviz/index.html#ease-comparisons-compared-visual-cues-should-be-adjacent",
    "title": "Data Visualization",
    "section": "Ease Comparisons: Compared Visual Cues Should Be Adjacent",
    "text": "Ease Comparisons: Compared Visual Cues Should Be Adjacent\n\nTextbook links\n\nTextbook section on compared visual cues being adjacent\nTextbook section on using color\nTextbook section on considering the color blind\n\n\n\nKey Points\n\nWhen two groups are to be compared, it is optimal to place them adjacent in the plot.\nUse color to encode groups to be compared.\nConsider using a color blind friendly palette.\n\n\n\nCode\n\ncolor_blind_friendly_cols &lt;- c(\"#999999\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\np1 &lt;- data.frame(x = 1:8, y = 1:8, col = as.character(1:8)) %&gt;%\n    ggplot(aes(x, y, color = col)) +\n    geom_point(size = 5)\np1 + scale_color_manual(values = color_blind_friendly_cols)"
  },
  {
    "objectID": "posts/dataviz/index.html#slope-charts",
    "href": "posts/dataviz/index.html#slope-charts",
    "title": "Data Visualization",
    "section": "Slope Charts",
    "text": "Slope Charts\n\nTextbook link\nPlots for two variables\n\n\nKey Points\n\nConsider using a slope chart or Bland-Altman plot when comparing one variable at two different time points, especially for a small number of observations.\nSlope charts use angle to encode change. Use geom_line() to create slope charts. It is useful when comparing a small number of observations.\nThe Bland-Altman plot (Tukey mean difference plot, MA plot) graphs the difference between conditions on the y-axis and the mean between conditions on the x-axis. It is more appropriate for large numbers of observations than slope charts.\n\n\n\nCode: Slope chart\n\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(gapminder)\n\n\nwest &lt;- c(\"Western Europe\", \"Northern Europe\", \"Southern Europe\", \"Northern America\", \"Australia and New Zealand\")\n\ndat &lt;- gapminder %&gt;%\n    filter(year %in% c(2010, 2015) & region %in% west & !is.na(life_expectancy) & population &gt; 10^7)\n\ndat %&gt;%\n    mutate(location = ifelse(year == 2010, 1, 2),\n           location = ifelse(year == 2015 & country %in% c(\"United Kingdom\", \"Portugal\"),\n                             location + 0.22, location),\n           hjust = ifelse(year == 2010, 1, 0)) %&gt;%\n    mutate(year = as.factor(year)) %&gt;%\n    ggplot(aes(year, life_expectancy, group = country)) +\n    geom_line(aes(color = country), show.legend = FALSE) +\n    geom_text(aes(x = location, label = country, hjust = hjust), show.legend = FALSE) +\n    xlab(\"\") +\n    ylab(\"Life Expectancy\") \n\n\n\n\n\n\n\n\n\n\nCode: Bland-Altman Plot\n\nlibrary(ggrepel)\n\n\ndat %&gt;%\n    mutate(year = paste0(\"life_expectancy_\", year)) %&gt;%\n    select(country, year, life_expectancy) %&gt;% spread(year, life_expectancy) %&gt;%\n    mutate(average = (life_expectancy_2015 + life_expectancy_2010)/2,\n                difference = life_expectancy_2015 - life_expectancy_2010) %&gt;%\n    ggplot(aes(average, difference, label = country)) +\n    geom_point() +\n    geom_text_repel() +\n    geom_abline(lty = 2) +\n    xlab(\"Average of 2010 and 2015\") +\n    ylab(\"Difference between 2015 and 2010\")"
  },
  {
    "objectID": "posts/dataviz/index.html#encoding-a-third-variable",
    "href": "posts/dataviz/index.html#encoding-a-third-variable",
    "title": "Data Visualization",
    "section": "Encoding a Third Variable",
    "text": "Encoding a Third Variable\n\nTextbook link\nEncoding a third variable\n\n\nKey Points\n\nEncode a categorical third variable on a scatterplot using color hue or shape. Use the shape argument to control shape.\nEncode a continuous third variable on a using color intensity or size."
  },
  {
    "objectID": "posts/dataviz/index.html#case-study-vaccines",
    "href": "posts/dataviz/index.html#case-study-vaccines",
    "title": "Data Visualization",
    "section": "Case Study: Vaccines",
    "text": "Case Study: Vaccines\n\nTextbook link\nCase study: vaccines and infectious diseases\ngeom_vline: Add vertical lines\n\n\nKey Points\n\nVaccines save millions of lives, but misinformation has led some to question the safety of vaccines. The data support vaccines as safe and effective. We visualize data about measles incidence in order to demonstrate the impact of vaccination programs on disease rate.\nThe RColorBrewer package offers several color palettes. Sequential color palettes are best suited for data that span from high to low. Diverging color palettes are best suited for data that are centered and diverge towards high or low values.\nThe geom_tile() geometry creates a grid of colored tiles. Position and length are stronger cues than color for numeric values, but color can be appropriate sometimes.\n\n\n\nCode: Tile plot of measles rate by year and state\n\n# import data and inspect\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(us_contagious_diseases)\nstr(us_contagious_diseases)\n\n\n# assign dat to the per 10,000 rate of measles, removing Alaska and Hawaii and adjusting for weeks reporting\nthe_disease &lt;- \"Measles\"\ndat &lt;- us_contagious_diseases %&gt;%\n    filter(!state %in% c(\"Hawaii\", \"Alaska\") & disease == the_disease) %&gt;%\n    mutate(rate = count / population * 10000 * 52/weeks_reporting) %&gt;%\n    mutate(state = reorder(state, rate))\n\n# plot disease rates per year in California\ndat %&gt;% filter(state == \"California\" & !is.na(rate)) %&gt;%\n    ggplot(aes(year, rate)) +\n    geom_line() +\n    ylab(\"Cases per 10,000\") +\n    geom_vline(xintercept=1963, col = \"blue\")\n\n\n\n\n\n\n\n# tile plot of disease rate by state and year\ndat %&gt;% ggplot(aes(year, state, fill=rate)) +\n    geom_tile(color = \"grey50\") +\n    scale_x_continuous(expand = c(0,0)) +\n    scale_fill_gradientn(colors = RColorBrewer::brewer.pal(9, \"Reds\"), trans = \"sqrt\") +\n    geom_vline(xintercept = 1963, col = \"blue\") +\n    theme_minimal() + theme(panel.grid = element_blank()) +\n    ggtitle(the_disease) +\n    ylab(\"\") +\n    xlab(\"\")\n\n\n\n\n\n\n\n\n\n\nCode: Line plot of measles rate by year and state\n\n# compute US average measles rate by year\navg &lt;- us_contagious_diseases %&gt;%\n    filter(disease == the_disease) %&gt;% group_by(year) %&gt;%\n    summarize(us_rate = sum(count, na.rm = TRUE)/sum(population, na.rm = TRUE)*10000)\n\n# make line plot of measles rate by year by state\ndat %&gt;%\n    filter(!is.na(rate)) %&gt;%\n    ggplot() +\n    geom_line(aes(year, rate, group = state), color = \"grey50\", \n        show.legend = FALSE, alpha = 0.2, size = 1) +\n    geom_line(mapping = aes(year, us_rate), data = avg, size = 1, col = \"black\") +\n    scale_y_continuous(trans = \"sqrt\", breaks = c(5, 25, 125, 300)) +\n    ggtitle(\"Cases per 10,000 by state\") +\n    xlab(\"\") +\n    ylab(\"\") +\n    geom_text(data = data.frame(x = 1955, y = 50),\n        mapping = aes(x, y, label = \"US average\"), color = \"black\") +\n    geom_vline(xintercept = 1963, col = \"blue\")"
  },
  {
    "objectID": "posts/dataviz/index.html#avoid-pseudo-and-gratuitous-3d-plots",
    "href": "posts/dataviz/index.html#avoid-pseudo-and-gratuitous-3d-plots",
    "title": "Data Visualization",
    "section": "Avoid Pseudo and Gratuitous 3D Plots",
    "text": "Avoid Pseudo and Gratuitous 3D Plots\n\nTextbook link\nAvoid pseudo-three-dimensional plots\n\n\nKey Points\nIn general, pseudo-3D plots and gratuitous 3D plots only add confusion. Use regular 2D plots instead."
  },
  {
    "objectID": "posts/dataviz/index.html#avoid-too-many-significant-digits",
    "href": "posts/dataviz/index.html#avoid-too-many-significant-digits",
    "title": "Data Visualization",
    "section": "Avoid Too Many Significant Digits",
    "text": "Avoid Too Many Significant Digits\n\nTextbook link\nAvoid too many significant digits\n\n\nKey points\n\nIn tables, avoid using too many significant digits. Too many digits can distract from the meaning of your data.\nReduce the number of significant digits globally by setting an option. For example, options(digits = 3) will cause all future computations that session to have 3 significant digits.\nReduce the number of digits locally using round() or signif()."
  },
  {
    "objectID": "posts/getStarted/index.html",
    "href": "posts/getStarted/index.html",
    "title": "Getting Started with R and RStudio",
    "section": "",
    "text": "In this tutorial we'll learn how to begin programming with R using RStudio. We'll install R, and RStudio RStudio, an extremely popular development environment for R. We'll learn the key RStudio features in order to start programming in R on our own."
  },
  {
    "objectID": "posts/getStarted/index.html#introduction",
    "href": "posts/getStarted/index.html#introduction",
    "title": "Getting Started with R and RStudio",
    "section": "",
    "text": "In this tutorial we'll learn how to begin programming with R using RStudio. We'll install R, and RStudio RStudio, an extremely popular development environment for R. We'll learn the key RStudio features in order to start programming in R on our own."
  },
  {
    "objectID": "posts/getStarted/index.html#getting-started-with-rstudio",
    "href": "posts/getStarted/index.html#getting-started-with-rstudio",
    "title": "Getting Started with R and RStudio",
    "section": "Getting Started with RStudio",
    "text": "Getting Started with RStudio\nRStudio is an open-source tool for programming in R. RStudio is a flexible tool that helps you create readable analyses, and keeps your code, images, comments, and plots together in one place. It's worth knowing about the capabilities of RStudio for data analysis and programming in R.\nUsing RStudio for data analysis and programming in R provides many advantages. Here are a few examples of what RStudio provides:\n\nAn intuitive interface that lets us keep track of saved objects, scripts, and figures\nA text editor with features like color-coded syntax that helps us write clean scripts\nAuto complete features save time\nTools for creating documents containing a project's code, notes, and visuals\nDedicated Project folders to keep everything in one place\n\nRStudio can also be used to program in other languages including SQL, Python, and Bash, to name a few.\nBut before we can install RStudio, we'll need to have a recent version of R installed on our computer."
  },
  {
    "objectID": "posts/getStarted/index.html#install-r",
    "href": "posts/getStarted/index.html#install-r",
    "title": "Getting Started with R and RStudio",
    "section": "1. Install R",
    "text": "1. Install R\nR is available to download from the official R website. Look for this section of the web page:\nThe version of R to download depends on our operating system. Below, we include installation instructions for Mac OS X, Windows, and Linux (Ubuntu).\nMAC OS X\n\nSelect the Download R for (Mac) OSX option.\nLook for the most up-to-date version of R (new versions are released frequently and appear toward the top of the page) and click the .pkg file to download.\nOpen the .pkg file and follow the standard instructions for installing applications on MAC OS X.\nDrag and drop the R application into the Applications folder.\n\nWindows\n\nSelect the Download R for Windows option.\nSelect base, since this is our first installation of R on our computer.\nFollow the standard instructions for installing programs for Windows. If we are asked to select Customize Startup or Accept Default Startup Options, choose the default options.\n\nLinux/Ubuntu\n\nSelect the Download R for Linux option.\nSelect the Ubuntu option.\nAlternatively, select the Linux package management system relevant to you if you are not using Ubuntu.\n\nRStudio is compatible with many versions of R (R version 3.0.1 or newer as of July, 2020). Installing R separately from RStudio enables the user to select the version of R that fits their needs."
  },
  {
    "objectID": "posts/getStarted/index.html#install-rstudio",
    "href": "posts/getStarted/index.html#install-rstudio",
    "title": "Getting Started with R and RStudio",
    "section": "2. Install RStudio",
    "text": "2. Install RStudio\nNow that R is installed, we can install RStudio. Navigate to the RStudio downloads page.\nWhen we reach the RStudio downloads page, let's click the \"Download\" button of the RStudio Desktop Open Source License Free option:\nOur operating system is usually detected automatically and so we can directly download the correct version for our computer by clicking the \"Download RStudio\" button. If we want to download RStudio for another operating system (other than the one we are running), navigate down to the \"All installers\" section of the page."
  },
  {
    "objectID": "posts/getStarted/index.html#first-look-at-rstudio",
    "href": "posts/getStarted/index.html#first-look-at-rstudio",
    "title": "Getting Started with R and RStudio",
    "section": "3. First Look at RStudio",
    "text": "3. First Look at RStudio\nWhen we open RStudio for the first time, we'll probably see a layout like this:\n But the background color will be white, so don't expect to see this blue-colored background the first time RStudio is launched. Check out this Dataquest blog to learn how to customize the appearance of RStudio.\nWhen we open RStudio, R is launched as well. A common mistake by new users is to open R instead of RStudio. To open RStudio, search for RStudio on the desktop, and pin the RStudio icon to the preferred location (e.g. Desktop or toolbar).\n\n\n\n\n\n\nFigure 1: Watch the video that guide you through the key step of using Rstudio in R."
  },
  {
    "objectID": "posts/getStarted/index.html#the-console",
    "href": "posts/getStarted/index.html#the-console",
    "title": "Getting Started with R and RStudio",
    "section": "4. The Console",
    "text": "4. The Console\nLet's start off by introducing some features of the Console. The Console is a tab in RStudio where we can run R code.\nNotice that the window pane where the console is located contains three tabs: Console, Terminal and Jobs (this may vary depending on the version of RStudio in use). We'll focus on the Console for now.\nWhen we open RStudio, the console contains information about the version of R we're working with. Scroll down, and try typing a few expressions like this one. Press the enter key to see the result.\n1 + 2\nAs we can see, we can use the console to test code immediately. When we type an expression like 1 + 2, we'll see the output below after hitting the enter key.\nWe can store the output of this command as a variable. Here, we've named our variable result:\nresult &lt;- 1 + 2\nThe &lt;- is called the assignment operator. This operator assigns values to variables. The command above is translated into a sentence as:\n\n&gt; The result variable gets the value of one plus two.\n\nOne nice feature from RStudio is the keyboard shortcut for typing the assignment operator &lt;-:\n\nMac OS X: Option + -\nWindows/Linux: Alt + -\n\nWe highly recommend that you memorize this keyboard shortcut because it saves a lot of time in the long run!\nWhen we type result into the console and hit enter, we see the stored value of 3:\n&gt; result &lt;- 1 + 2 &gt; result [1] 3\nWhen we create a variable in RStudio, it saves it as an object in the R global environment. We'll discuss the environment and how to view objects stored in the environment in the next section."
  },
  {
    "objectID": "posts/getStarted/index.html#the-global-environment",
    "href": "posts/getStarted/index.html#the-global-environment",
    "title": "Getting Started with R and RStudio",
    "section": "5. The Global Environment",
    "text": "5. The Global Environment\nWe can think of the global environment as our workspace. During a programming session in R, any variables we define, or data we import and save in a dataframe, are stored in our global environment. In RStudio, we can see the objects in our global environment in the Environment tab at the top right of the interface:\nWe'll see any objects we created, such as result, under values in the Environment tab. Notice that the value, 3, stored in the variable is displayed.\nSometimes, having too many named objects in the global environment creates confusion. Maybe we'd like to remove all or some of the objects. To remove all objects, click the broom icon at the top of the window:\nTo remove selected objects from the workspace, select the Grid view from the dropdown menu:\nHere we can check the boxes of the objects we'd like to remove and use the broom icon to clear them from our Global Environment."
  },
  {
    "objectID": "posts/getStarted/index.html#install-the-tidyverse-packages",
    "href": "posts/getStarted/index.html#install-the-tidyverse-packages",
    "title": "Getting Started with R and RStudio",
    "section": "6. Install the tidyverse Packages",
    "text": "6. Install the tidyverse Packages\nMuch of the functionality in R comes from using packages. Packages are shareable collections of code, data, and documentation. Packages are essentially extensions, or add-ons, to the R program that we installed above.\nOne of the most popular collection of packages in R is known as the \"tidyverse\". The tidyverse is a collection of R packages designed for working with data. The tidyverse packages share a common design philosophy, grammar, and data structures. Tidyverse packages \"play well together\". The tidyverse enables you to spend less time cleaning data so that you can focus more on analyzing, visualizing, and modeling data.\nLet's learn how to install the tidyverse packages. The most common \"core\" tidyverse packages are:\n\nreadr, for data import.\nggplot2, for data visualization.\ndplyr, for data manipulation.\ntidyr, for data tidying.\npurrr, for functional programming.\ntibble, for tibbles, a modern re-imagining of dataframes.\nstringr, for string manipulation.\nforcats, for working with factors (categorical data).\n\nTo install packages in R we use the built-in install.packages() function. We could install the packages listed above one-by-one, but fortunately the creators of the tidyverse provide a way to install all these packages from a single command. Type the following command in the Console and hit the enter key.\ninstall.packages(\"tidyverse\")\nThe install.packages() command only needs to be used to download and install packages for the first time."
  },
  {
    "objectID": "posts/getStarted/index.html#load-the-tidyverse-packages-into-memory",
    "href": "posts/getStarted/index.html#load-the-tidyverse-packages-into-memory",
    "title": "Getting Started with R and RStudio",
    "section": "7. Load the tidyverse Packages into Memory",
    "text": "7. Load the tidyverse Packages into Memory\nAfter a package is installed on a computer's hard drive, the library() command is used to load a package into memory:\nlibrary(readr) library(ggplot2)\nLoading the package into memory with library() makes the functionality of a given package available for use in the current R session. It is common for R users to have hundreds of R packages installed on their hard drive, so it would be inefficient to load all packages at once. Instead, we specify the R packages needed for a particular project or task.\nFortunately, the core tidyverse packages can be loaded into memory with a single command. This is how the command and the output looks in the console:\nlibrary(tidyverse)## ── Attaching packages ───────────────────────────────────────────────── tidyverse 1.3.0 ──## ✓ ggplot2 3.3.2 ✓ purrr 0.3.4 ## ✓ tibble 3.0.3 ✓ dplyr 1.0.0 ## ✓ tidyr 1.1.0 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.5.0## ── Conflicts ──────────────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag()\nThe Attaching packages section of the output specifies the packages and their versions loaded into memory. The Conflicts section specifies any function names included in the packages that we just loaded to memory that share the same name as a function already loaded into memory. Using the example above, now if we call the filter() function, R will use the code specified for this function from the dplyr package. These conflicts are generally not a problem, but it's worth reading the output message to be sure."
  },
  {
    "objectID": "posts/getStarted/index.html#identify-loaded-packages",
    "href": "posts/getStarted/index.html#identify-loaded-packages",
    "title": "Getting Started with R and RStudio",
    "section": "8. Identify Loaded Packages",
    "text": "8. Identify Loaded Packages\nIf we need to check which packages we loaded, we can refer to the Packages tab in the window at the bottom right of the console.\nWe can search for packages, and checking the box next to a package loads it (the code appears in the console).\nAlternatively, entering this code into the console will display all packages currently loaded into memory:\n(.packages())\nWhich returns:\n[1] \"forcats\" \"stringr\" \"dplyr\" \"purrr\" \"tidyr\" \"tibble\" \"tidyverse\" [8] \"ggplot2\" \"readr\" \"stats\" \"graphics\" \"grDevices\" \"utils\" \"datasets\" [15] \"methods\" \"base\"\nAnother useful function for returning the names of packages currently loaded into memory is search():\n&gt; search()  [1] \".GlobalEnv\" \"package:forcats\" \"package:stringr\" \"package:dplyr\"  [5] \"package:purrr\" \"package:readr\" \"package:tidyr\" \"package:tibble\"  [9] \"package:ggplot2\" \"package:tidyverse\" \"tools:rstudio\" \"package:stats\" [13] \"package:graphics\" \"package:grDevices\" \"package:utils\" \"package:datasets\" [17] \"package:methods\" \"Autoloads\" \"package:base\""
  },
  {
    "objectID": "posts/getStarted/index.html#get-help-on-a-package",
    "href": "posts/getStarted/index.html#get-help-on-a-package",
    "title": "Getting Started with R and RStudio",
    "section": "9. Get Help on a Package",
    "text": "9. Get Help on a Package\nWe've learned how to install and load packages. But what if we'd like to learn more about a package that we've installed? That's easy! Clicking the package name in the Packages tab takes us to the Help tab for the selected package. Here's what we see if we click the tidyr package:\nAlternatively, we can type this command into the console and achieve the same result:\nhelp(package = \"tidyr\")\nThe help page for a package provides quick access to documentation for each function included in a package. From the main help page for a package you can also access \"vignettes\" when they are available. Vignettes provide brief introductions, tutorials, or other reference information about a package, or how to use specific functions in a package.\nvignette(package = \"tidyr\")\nWhich results in this list of available options:\nVignettes in package ‘tidyr’:nest nest (source, html) pivot Pivoting (source, html) programming Programming with tidyr (source, html) rectangle rectangling (source, html) tidy-data Tidy data (source, html) in-packages Usage and migration (source, html)\nFrom there, we can select a particular vignette to view:\nvignette(\"pivot\")\nNow we see the Pivot vignette is displayed in the Help tab. This is one example of why RStudio is a powerful tool for programming in R. We can access function and package documentation and tutorials without leaving RStudio!"
  },
  {
    "objectID": "posts/getStarted/index.html#get-help-on-a-function",
    "href": "posts/getStarted/index.html#get-help-on-a-function",
    "title": "Getting Started with R and RStudio",
    "section": "10. Get Help on a Function",
    "text": "10. Get Help on a Function\nAs we learned in the last section, we can get help on a function by clicking the package name in Packages and then click on a function name to see the help file. Here we see the pivot_longer() function from the tidyr package is at the top of this list:\nAnd if we click on \"pivot_longer\" we get this:\nWe can achieve the same results in the Console with any of these function calls:\nhelp(\"pivot_longer\") help(pivot_longer) ?pivot_longer\nNote that the specific Help tab for the pivot_longer() function (or any function we're interested in) may not be the default result if the package that contains the function is not loaded into memory yet. In general it's best to ensure a specific package is loaded before seeking help on a function."
  },
  {
    "objectID": "posts/getStarted/index.html#rstudio-projects",
    "href": "posts/getStarted/index.html#rstudio-projects",
    "title": "Getting Started with R and RStudio",
    "section": "11. RStudio Projects",
    "text": "11. RStudio Projects\nRStudio offers a powerful feature to keep you organized; Projects. It is important to stay organized when you work on multiple analyses. Projects from RStudio allow you to keep all of your important work in one place, including code scripts, plots, figures, results, and datasets.\nCreate a new project by navigating to the File tab in RStudio and select New Project.... Then specify if you would like to create the project in a new directory, or in an existing directory. Here we select \"New Directory\":\nRStudio offers dedicated project types if you are working on an R package, or a Shiny Web Application. Here we select \"New Project\", which creates an R project:\nNext, we give our project a name. \"Create project as a subdirectory of:\" is showing where the folder will live on the computer. If we approve of the location select \"Create Project\", if we do not, select \"Browse\" and choose the location on the computer where this project folder should live.\nNow in RStudio we see the name of the project is indicated in the upper-right corner of the screen. We also see the .Rproj file in the Files tab. Any files we add to, or generate-within, this project will appear in the Files tab.\nRStudio Projects are useful when you need to share your work with colleagues. You can send your project file (ending in .Rproj) along with all supporting files, which will make it easier for your colleagues to recreate the working environment and reproduce the results."
  },
  {
    "objectID": "posts/getStarted/index.html#save-your-real-work.-delete-the-rest.",
    "href": "posts/getStarted/index.html#save-your-real-work.-delete-the-rest.",
    "title": "Getting Started with R and RStudio",
    "section": "12. Save Your \"Real\" Work. Delete the Rest.",
    "text": "12. Save Your \"Real\" Work. Delete the Rest.\nThis tip comes from our 23 RStudio Tips, Tricks, and Shortcuts blog post, but it's so important that we are sharing it here as well!\nPractice good housekeeping to avoid unforeseen challenges down the road. If you create an R object worth saving, capture the R code that generated the object in an R script file. Save the R script, but don't save the environment, or workspace, where the object was created.\nTo prevent RStudio from saving your workspace, open Preferences &gt; General and un-select the option to restore .RData into workspace at startup. Be sure to specify that you never want to save your workspace, like this:\nNow, each time you open RStudio, you will begin with an empty session. None of the code generated from your previous sessions will be remembered. The R script and datasets can be used to recreate the environment from scratch.\nOther experts agree that not saving your workspace is best practice when using RStudio."
  },
  {
    "objectID": "posts/getStarted/index.html#r-scripts",
    "href": "posts/getStarted/index.html#r-scripts",
    "title": "Getting Started with R and RStudio",
    "section": "13. R Scripts",
    "text": "13. R Scripts\nAs we worked through this tutorial, we wrote code in the Console. As our projects become more complex, we write longer blocks of code. If we want to save our work, it is necessary to organize our code into a script. This allows us to keep track of our work on a project, write clean code with plenty of notes, reproduce our work, and share it with others.\nIn RStudio, we can write scripts in the text editor window at the top left of the interface:\n To create a new script, we can use the commands in the file menu:\nWe can also use the keyboard shortcut Ctrl + Shift + N. When we save a script, it has the file extension .R. As an example, we'll create a new script that includes this code to generate a scatterplot:\nlibrary(ggplot2) ggplot(data = mpg,        aes(x = displ, y = hwy)) +   geom_point()\nTo save our script we navigate to the File menu tab and select Save. Or we enter the following command:\n\nMac OS X: Cmd + S\nWindows/Linux: Ctrl + S"
  },
  {
    "objectID": "posts/getStarted/index.html#run-code",
    "href": "posts/getStarted/index.html#run-code",
    "title": "Getting Started with R and RStudio",
    "section": "14. Run Code",
    "text": "14. Run Code\nTo run a single line of code we typed into our script, we can either click Run at the top right of the script, or use the following keyboard commands when our cursor is on the line we want to run:\n\nMac OS X: Cmd + Enter\nWindows/Linux: Ctrl + Enter\n\nIn this case, we'll need to highlight multiple lines of code to generate the scatterplot. To highlight and run all lines of code in a script enter:\n\nMac OS X: Cmd + A + Enter\nWindows/Linux: Ctrl + A + Enter\n\nLet's check out the result when we run the lines of code specified above:\nSide note: this scatterplot is generated using data from the mpg dataset that is included in the ggplot2 package. The dataset contains fuel economy data from 1999 to 2008, for 38 popular models of cars.\nIn this plot, the engine displacement (i.e. size) is depicted on the x-axis (horizontal axis). The y-axis (vertical axis) depicts the fuel efficiency in miles-per-gallon. In general, fuel economy decreases with the increase in engine size. This plot was generated with the tidyverse package ggplot2. This package is very popular for data visualization in R."
  },
  {
    "objectID": "posts/getStarted/index.html#access-built-in-datasets",
    "href": "posts/getStarted/index.html#access-built-in-datasets",
    "title": "Getting Started with R and RStudio",
    "section": "15. Access Built-in Datasets",
    "text": "15. Access Built-in Datasets\nWant to learn more about the mpg dataset from the ggplot2 package that we mentioned in the last example? Do this with the following command:\ndata(mpg, package = \"ggplot2\")\nFrom there you can take a look at the first six rows of data with the head() function:\nhead(mpg)\n## # A tibble: 6 x 11 ##   manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class ## ## 1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa… ## 2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa… ## 3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa… ## 4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa… ## 5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p     compa… ## 6 audi         a4      2.8  1999     6 manual(m5) f        18    26 p     compa…\nObtain summary statistics with the summary() function:\nsummary(mpg)\n##  manufacturer          model               displ            year ##  Length:234         Length:234         Min.   :1.600   Min.   :1999 ##  Class :character   Class :character   1st Qu.:2.400   1st Qu.:1999 ##  Mode  :character   Mode  :character   Median :3.300   Median :2004 ##                                        Mean   :3.472   Mean   :2004 ##                                        3rd Qu.:4.600   3rd Qu.:2008 ##                                        Max.   :7.000   Max.   :2008 ##       cyl           trans               drv                 cty ##  Min.   :4.000   Length:234         Length:234         Min.   : 9.00 ##  1st Qu.:4.000   Class :character   Class :character   1st Qu.:14.00 ##  Median :6.000   Mode  :character   Mode  :character   Median :17.00 ##  Mean   :5.889                                         Mean   :16.86 ##  3rd Qu.:8.000                                         3rd Qu.:19.00 ##  Max.   :8.000                                         Max.   :35.00 ##       hwy             fl               class ##  Min.   :12.00   Length:234         Length:234 ##  1st Qu.:18.00   Class :character   Class :character ##  Median :24.00   Mode  :character   Mode  :character ##  Mean   :23.44 ##  3rd Qu.:27.00 ##  Max.   :44.00\nOr open the help page in the Help tab, like this:\nhelp(mpg)\nFinally, there are many datasets built-in to R that are ready to work with. Built-in datasets are handy for practicing new R skills without searching for data. View available datasets with this command:\ndata()"
  },
  {
    "objectID": "posts/getStarted/index.html#additional-resources",
    "href": "posts/getStarted/index.html#additional-resources",
    "title": "Getting Started with R and RStudio",
    "section": "Additional Resources",
    "text": "Additional Resources\nIf you enjoyed this tutorial, come learn with us at Dataquest! If you are new to R and RStudio, we recommend starting with the Dataquest Introduction to Data Analysis in R course. This is the first course in the Dataquest Data Analyst in R path.\nFor more advanced RStudio tips check out the Dataquest blog post 23 RStudio Tips, Tricks, and Shortcuts.\nLearn how to load and clean data with tidyverse tools in this Dataquest blog post.\nRStudio has published numerous in-depth how to articles about using RStudio. Find them here.\nThere is an official RStudio Blog.\nIf you would like to learn R Markdown, check out these Dataquest blog posts:\n\nGetting Started with R Markdown — Guide and Cheatsheet\nR Markdown Tips, Tricks, and Shortcuts\n\nLearn R and the tidyverse with R for Data Science by Hadley Wickham. Solidify your knowledge by working through the exercises in RStudio and saving your work for future reference."
  },
  {
    "objectID": "posts/getStarted/index.html#references",
    "href": "posts/getStarted/index.html#references",
    "title": "Getting Started with R and RStudio",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "posts/importingData/index.html",
    "href": "posts/importingData/index.html",
    "title": "Importing table files into R",
    "section": "",
    "text": "So far, we’ve looked at several dataset in previous chapter and we have also created ourselves some datasets. While you can do all your data entry work in R or Excel, it is much more common to load data from other sources. Local and international organization have been collecting fisheries dependent and fisheries independent data for years. These historical dataset with fisheries information like fish catch, effort, landing sites, fishing ground and critical habitats can be obtained from several databases—some are open and other closed.\nMuch of the data we download or receive from is either comma-separated value files .csv or and Excel spreadsheets, .xlsx. .csv files are spreadsheets stored as text files - basically Excel files stripped down to the bare minimum - no formatting, no formulas, no macros. You can open and edit them in spreadsheet software like LibreOffice Calc, Google Sheets or Microsoft Excel. Many devices and databases can export data in .csv format, making it a commonly used file format that you are likely to encounter sooner rather than later.\nWhether that be a comma separated (csv) or a tab delimited file, there are multiple functions that can read these data into R. We will stick to loading these data from the tidyverse packages (Wickham and Wickham, 2017) but be aware these are not the only methods for doing this. We will use the tidyverse functions just to maintain consistency with everything else we do. The first package in tidyverse we will use is called readr (Wickham et al., 2017), which is a collection of functions to load the tabular data from working directory in our machine into R session. Some of its functions include:\n\nread_csv(): comma separated (CSV) files\nread_tsv(): tab separated files\nread_delim(): general delimited files\nread_fwf(): fixed width files\nread_table(): tabular files where columns are separated by white-space.\nread_log(): web log files\nreadxl reads in Excel files.\n\nBefore we import the data, we need to load the packages that we will use their functions in this chapter\n\nrequire(tidyverse)\nrequire(magrittr)\n\n\n\nA CSV file is a type of file where each line contains a single record, and all the columns are separated from each other via a comma. In order to load data from a file into R, you need its path - that is, you need to tell R where to find the file. Unless you specify otherwise, R will look for files in its current working directory. You can read .csv file using read_csv() function of the readr package (Wickham et al., 2017) as shown in the chunk below;\n\nimported.lfq = read_csv(\"dataset/project/tidy_LFQ_sample_4.csv\")\n\nlf4\n\nWe imported tidy_LFQ_sample_4.csv from working directory into R using read_csv() and specify the path to the file in your working directory and store as imported.lfq. If you get an error message, it means thattidy_LFQ_sample_4.csvis not in your working directory. Either move the file to the right directory (remember, you can use rungetwd()` to see what your working directory is) or change your working directory.\n\nimported.lfq = read_csv(\"../data/tidy/tidy_LFQ_sample_4.csv\")\n\nIf you glimpse the dataframe with glimpse() function, you should see the internal structure of the imported.lfq object we just loaded;\n\nimported.lfq %&gt;% \n  glimpse()\n\nRows: 6,185\nColumns: 6\n$ site  &lt;chr&gt; \"Mombasa\", \"Mombasa\", \"Mombasa\", \"Mombasa\", \"Mombasa\", \"Mombasa\"…\n$ date  &lt;date&gt; 2019-04-05, 2019-04-05, 2019-04-05, 2019-04-05, 2019-04-05, 201…\n$ tl_mm &lt;dbl&gt; 184, 185, 145, 189, 175, 165, 181, 176, 164, 154, 188, 186, 179,…\n$ fl_mm &lt;dbl&gt; 169, 169, 134, 173, 161, 153, 165, 163, 148, 142, 173, 173, 164,…\n$ wt_gm &lt;dbl&gt; 59.50, 54.71, 24.15, 61.36, 49.31, 38.54, 49.68, 45.27, 36.26, 3…\n$ sex   &lt;chr&gt; \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\",…\n\n\nThe dataset contains six variables and 6,185 records. The variables site and sex both contain text, and have been imported as character vectors4. The date column has been imported as date format, the variable tl_mm and fl_mm are measured length and have been imported as numeric vector measured in millimeters. The variable wt_gm is the weight of fish measured in grams and also have been imported as numeric vector.\nSo, what can you do in case you need to import data from a file that is not in your working directory? This is a common problem, as many of us store script files and data files in separate folders (or even on separate drives). One option is to use file.choose, which opens a pop-up window that lets you choose which file to open using a graphical interface:\nimported.lfq2 = read_csv(file.choose())\nThis solution work just fine if you just want to open a single file once. But if you want to reuse your code or run it multiple times, you probably don’t want to have to click and select your file each time. Instead, you can specify the path to your file in the call to read_csv.\n\n\n\nCommonly our data is stored as a Excel file. There are several packages that can be used to import Excel files to R. I prefer the readxl package (Wickham and Bryan, 2018), so let’s install that:\ninstall.packages(\"readxl\")\nThe package has read_exel() function that allows us to specify which sheet within the Excel file to read. The function automatically convert the worksheet into a .csv file and read it. Let’s us import the the data in first sheet of the tidy_LFQ_sample_4.xlsx. Is a similar dataset that just imported in the previous section, but is in Excel format. We will use this file to illustrate how to import the excel file into R workspace with readxl package (Wickham and Bryan, 2018).\n\nimported.lfq = readxl::read_excel(\"../data/tidy/tidy_LFQ_sample_4.xlsx\", sheet = 1)\n\n\nimported.lfq\n\n# A tibble: 6,185 × 6\n   site    date                tl_mm fl_mm wt_gm sex  \n   &lt;chr&gt;   &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n 1 Mombasa 2019-04-05 00:00:00   184   169  59.5 M    \n 2 Mombasa 2019-04-05 00:00:00   185   169  54.7 M    \n 3 Mombasa 2019-04-05 00:00:00   145   134  24.2 M    \n 4 Mombasa 2019-04-05 00:00:00   189   173  61.4 M    \n 5 Mombasa 2019-04-05 00:00:00   175   161  49.3 M    \n 6 Mombasa 2019-04-05 00:00:00   165   153  38.5 M    \n 7 Mombasa 2019-04-05 00:00:00   181   165  49.7 M    \n 8 Mombasa 2019-04-05 00:00:00   176   163  45.3 M    \n 9 Mombasa 2019-04-05 00:00:00   164   148  36.3 M    \n10 Mombasa 2019-04-05 00:00:00   154   142  31.9 M    \n# ℹ 6,175 more rows\n\n\n\nimported.lfq %&gt;% \n  skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n6185\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\nPOSIXct\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nsite\n0\n1\n3\n7\n0\n2\n0\n\n\nsex\n0\n1\n1\n1\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ntl_mm\n0\n1\n170.77\n21.08\n97.0\n157.00\n171.00\n183.00\n269.00\n▁▅▇▁▁\n\n\nfl_mm\n0\n1\n156.00\n19.26\n18.1\n144.00\n156.00\n168.00\n241.00\n▁▁▅▇▁\n\n\nwt_gm\n0\n1\n46.03\n19.51\n7.0\n32.77\n43.59\n55.28\n194.18\n▇▆▁▁▁\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ndate\n0\n1\n2016-03-31\n2020-09-11\n2020-02-25\n42"
  },
  {
    "objectID": "posts/importingData/index.html#importing-data",
    "href": "posts/importingData/index.html#importing-data",
    "title": "Importing table files into R",
    "section": "",
    "text": "So far, we’ve looked at several dataset in previous chapter and we have also created ourselves some datasets. While you can do all your data entry work in R or Excel, it is much more common to load data from other sources. Local and international organization have been collecting fisheries dependent and fisheries independent data for years. These historical dataset with fisheries information like fish catch, effort, landing sites, fishing ground and critical habitats can be obtained from several databases—some are open and other closed.\nMuch of the data we download or receive from is either comma-separated value files .csv or and Excel spreadsheets, .xlsx. .csv files are spreadsheets stored as text files - basically Excel files stripped down to the bare minimum - no formatting, no formulas, no macros. You can open and edit them in spreadsheet software like LibreOffice Calc, Google Sheets or Microsoft Excel. Many devices and databases can export data in .csv format, making it a commonly used file format that you are likely to encounter sooner rather than later.\nWhether that be a comma separated (csv) or a tab delimited file, there are multiple functions that can read these data into R. We will stick to loading these data from the tidyverse packages (Wickham and Wickham, 2017) but be aware these are not the only methods for doing this. We will use the tidyverse functions just to maintain consistency with everything else we do. The first package in tidyverse we will use is called readr (Wickham et al., 2017), which is a collection of functions to load the tabular data from working directory in our machine into R session. Some of its functions include:\n\nread_csv(): comma separated (CSV) files\nread_tsv(): tab separated files\nread_delim(): general delimited files\nread_fwf(): fixed width files\nread_table(): tabular files where columns are separated by white-space.\nread_log(): web log files\nreadxl reads in Excel files.\n\nBefore we import the data, we need to load the packages that we will use their functions in this chapter\n\nrequire(tidyverse)\nrequire(magrittr)\n\n\n\nA CSV file is a type of file where each line contains a single record, and all the columns are separated from each other via a comma. In order to load data from a file into R, you need its path - that is, you need to tell R where to find the file. Unless you specify otherwise, R will look for files in its current working directory. You can read .csv file using read_csv() function of the readr package (Wickham et al., 2017) as shown in the chunk below;\n\nimported.lfq = read_csv(\"dataset/project/tidy_LFQ_sample_4.csv\")\n\nlf4\n\nWe imported tidy_LFQ_sample_4.csv from working directory into R using read_csv() and specify the path to the file in your working directory and store as imported.lfq. If you get an error message, it means thattidy_LFQ_sample_4.csvis not in your working directory. Either move the file to the right directory (remember, you can use rungetwd()` to see what your working directory is) or change your working directory.\n\nimported.lfq = read_csv(\"../data/tidy/tidy_LFQ_sample_4.csv\")\n\nIf you glimpse the dataframe with glimpse() function, you should see the internal structure of the imported.lfq object we just loaded;\n\nimported.lfq %&gt;% \n  glimpse()\n\nRows: 6,185\nColumns: 6\n$ site  &lt;chr&gt; \"Mombasa\", \"Mombasa\", \"Mombasa\", \"Mombasa\", \"Mombasa\", \"Mombasa\"…\n$ date  &lt;date&gt; 2019-04-05, 2019-04-05, 2019-04-05, 2019-04-05, 2019-04-05, 201…\n$ tl_mm &lt;dbl&gt; 184, 185, 145, 189, 175, 165, 181, 176, 164, 154, 188, 186, 179,…\n$ fl_mm &lt;dbl&gt; 169, 169, 134, 173, 161, 153, 165, 163, 148, 142, 173, 173, 164,…\n$ wt_gm &lt;dbl&gt; 59.50, 54.71, 24.15, 61.36, 49.31, 38.54, 49.68, 45.27, 36.26, 3…\n$ sex   &lt;chr&gt; \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\",…\n\n\nThe dataset contains six variables and 6,185 records. The variables site and sex both contain text, and have been imported as character vectors4. The date column has been imported as date format, the variable tl_mm and fl_mm are measured length and have been imported as numeric vector measured in millimeters. The variable wt_gm is the weight of fish measured in grams and also have been imported as numeric vector.\nSo, what can you do in case you need to import data from a file that is not in your working directory? This is a common problem, as many of us store script files and data files in separate folders (or even on separate drives). One option is to use file.choose, which opens a pop-up window that lets you choose which file to open using a graphical interface:\nimported.lfq2 = read_csv(file.choose())\nThis solution work just fine if you just want to open a single file once. But if you want to reuse your code or run it multiple times, you probably don’t want to have to click and select your file each time. Instead, you can specify the path to your file in the call to read_csv.\n\n\n\nCommonly our data is stored as a Excel file. There are several packages that can be used to import Excel files to R. I prefer the readxl package (Wickham and Bryan, 2018), so let’s install that:\ninstall.packages(\"readxl\")\nThe package has read_exel() function that allows us to specify which sheet within the Excel file to read. The function automatically convert the worksheet into a .csv file and read it. Let’s us import the the data in first sheet of the tidy_LFQ_sample_4.xlsx. Is a similar dataset that just imported in the previous section, but is in Excel format. We will use this file to illustrate how to import the excel file into R workspace with readxl package (Wickham and Bryan, 2018).\n\nimported.lfq = readxl::read_excel(\"../data/tidy/tidy_LFQ_sample_4.xlsx\", sheet = 1)\n\n\nimported.lfq\n\n# A tibble: 6,185 × 6\n   site    date                tl_mm fl_mm wt_gm sex  \n   &lt;chr&gt;   &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n 1 Mombasa 2019-04-05 00:00:00   184   169  59.5 M    \n 2 Mombasa 2019-04-05 00:00:00   185   169  54.7 M    \n 3 Mombasa 2019-04-05 00:00:00   145   134  24.2 M    \n 4 Mombasa 2019-04-05 00:00:00   189   173  61.4 M    \n 5 Mombasa 2019-04-05 00:00:00   175   161  49.3 M    \n 6 Mombasa 2019-04-05 00:00:00   165   153  38.5 M    \n 7 Mombasa 2019-04-05 00:00:00   181   165  49.7 M    \n 8 Mombasa 2019-04-05 00:00:00   176   163  45.3 M    \n 9 Mombasa 2019-04-05 00:00:00   164   148  36.3 M    \n10 Mombasa 2019-04-05 00:00:00   154   142  31.9 M    \n# ℹ 6,175 more rows\n\n\n\nimported.lfq %&gt;% \n  skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n6185\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\nPOSIXct\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nsite\n0\n1\n3\n7\n0\n2\n0\n\n\nsex\n0\n1\n1\n1\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ntl_mm\n0\n1\n170.77\n21.08\n97.0\n157.00\n171.00\n183.00\n269.00\n▁▅▇▁▁\n\n\nfl_mm\n0\n1\n156.00\n19.26\n18.1\n144.00\n156.00\n168.00\n241.00\n▁▁▅▇▁\n\n\nwt_gm\n0\n1\n46.03\n19.51\n7.0\n32.77\n43.59\n55.28\n194.18\n▇▆▁▁▁\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ndate\n0\n1\n2016-03-31\n2020-09-11\n2020-02-25\n42"
  },
  {
    "objectID": "posts/importingData/index.html#saving-and-exporting-your-data",
    "href": "posts/importingData/index.html#saving-and-exporting-your-data",
    "title": "Importing table files into R",
    "section": "Saving and exporting your data",
    "text": "Saving and exporting your data\nIn many a case, data manipulation is a huge part of statistical work, and of course you want to be able to save a data frame after manipulating it. There are two options for doing this in R - you can either export the data as e.g. a .csv or a .xlsx file, or save it in R format as an .RData file.\n\nExporting data\nJust as we used the functions read_csv and read_excel to import data, we can use write_csvto export it. The code below saves the bookstore data frame as a .csv file file, which will be created in the current working directory. If you wish to store\n\nimported.lfq %&gt;%  write_csv(\"assets/fao_paul_dataset/tidy/tidy_lfq.csv\")\n\n\nSaving and loading R data\nBeing able to export to different spreadsheet formats is very useful, but sometimes you want to save an object that can’t be saved in a spreadsheet format. For instance, you may wish to save a multiple processed data, functions and formula that you’ve created. .RData files can be used to store one or more R objects. To save the objects bookstore and age in a .Rdata file, we can use the save function:\n\nsave.image(\"assets/fao_paul_dataset/tidy/myData.RData\")"
  },
  {
    "objectID": "posts/rbasics/index.html",
    "href": "posts/rbasics/index.html",
    "title": "The basics of R programming",
    "section": "",
    "text": "In this section, I will introduce you to R Basics, Functions, and Datatypes.\nIn this part, you will learn to:\n\nAppreciate the rationale for data analysis using R.\nDefine objects and perform basic arithmetic and logical operations.\nUse pre-defined functions to perform operations on objects.\nDistinguish between various data types.\n\n\n\n\n\n\nTo complete this course, you should install R locally on your computer. We also highly recommend installing RStudio, an integrated development environment (IDE), to edit and test your code.\nIn order to complete some assignments in the course, you will need your own copy of R. You may also find it helpful to follow along with the course videos in R or RStudio.\nBoth R and RStudio can be freely downloaded and installed.\n\n\n\n\n\nYou need to install R before using RStudio, which is an interactive desktop environment.\nSelect base subdirectory in CRAN and click download.\nSelect all default choices in the installation process.\nWe recommend selecting English for language to help you better follow the course.\nYou can try using the R console, but for productivity purposes, we can switch to RStudio.\n\n\n\n\n\n\n\n\nYou can download the latest version of RStudio at the RStudio website.\nThe free desktop version is more than enough for this course.\nMake sure to choose the version for your own operating system.\nChoose “Yes” for all defaults in the installation process.\n\n\n\n\n\n\n\n\nThe free desktop version of RStudio can be launched like other applications on your computer.\nWhen you start RStudio for the first time, you will see three panes. The left pane shows you the R console. On the right, the top pane includes three tabs, while the bottom pane shows you five tabs, file, plots, packages, help, and viewer.\nYou can download a cheat sheet of the most common RStudio commands directly from RStudio by going to “Help -&gt; Cheat Sheets -&gt; RStudio IDE Cheat Sheet.”\n\n\n\n\n\n\n\n\nR was developed by statisticians and data analysts as an interactive environment for data analysis.\nSome of the advantages of R are that:\n\nit is free and open source;\nit has the capability to save scripts;\nthere are numerous resources for learning;\nit is easy for developers to share software implementation.\n\nExpressions are evaluated in the R console when you type the expression into the console and hit Return.\nA great advantage of R over point and click analysis software is that you can save your work as scripts.\n“Base R” is what you get after you first install R. Additional components are available via packages.\n\n\n\n\nIn RStudio, you can upload additional functions and datasets in addition to the base R functions and datasets that come with R automatically. A common way to do this is by installing packages, which often contain extra functions and datasets. For this course, there are a few packages you will need to install. You only need to install each individual package once, but after you install a package, there are other steps you have to do whenever you want to use something from that package.\nTo install a package, you use the code install.packages(\"package_name\", dependencies = TRUE).\nTo load a package, you use the code library(package_name).\nIf you also want to use a dataset from a package you have loaded, then you use the code data(dataset_name). To see the dataset, you can take the additional step of View(dataset_name).\n\n\n\n\n\n\nWe recommend installing packages through RStudio, rather than through R, and the code provided works in both R and RStudio. Once a package has been installed, it is technically added onto R (even if you use RStudio to install it), which is why packages must be re-installed when R is updated. However, since we use R through RStudio, any packages that are installed can be used in both R and RStudio, regardless of which one was used to install the packages.\n\n\n\n\nThe base version of R is quite minimal, but you can supplement its functions by installing additional packages.\nWe will be using tidyverse and dslabs packages for this course.\nInstall packages from R console: install.packages(\"pkg_name\")\nInstall packages from RStudio interface: Tools &gt; Install Packages (allows autocomplete)\nOnce installed, we can use library(pkg_name) to load a package each time we want to use it\n\n\n\n\n\nIf you try to load a package with library(blahblah) and get a message like Error in library(blahblah) : there is no package called ‘blahblah’, it means you need to install that package first with install.packages().\nOn the DataCamp interface we use for some problems in the course, you cannot install additional packages. The problems have been set up with the packages you need to solve them.\nYou can add the option dependencies = TRUE, which tells R to install the other things that are necessary for the package or packages to run smoothly. Otherwise, you may need to install additional packages to unlock the full functionality of a package.\nThroughout the course materials and textbook, package names are in bold.\n\n\n\n\n\ninstall.packages(\"dslabs\") # to install a single package\ninstall.packages(c(\"tidyverse\", \"dslabs\")) # to install two packages at the same time\ninstalled.packages() # to see the list of all installed packages\n\n\n\n\n\n\n\n\nRStudio has many useful features as an R editor, including the ability to test code easily as we write scripts and several auto complete features.\nKeyboard shortcuts:\n\nSave a script: Ctrl+S on Windows and Command+S on Mac\nRun an entire script: Ctrl+Shift+Enter on Windows Command+Shift+Return on Mac, or click “Source” on the editor pane\nRun a single line of script: Ctrl+Enter on Windows and Command+Return on Mac while the cursor is pointing to that line, or select the chunk and click “run”\nOpen a new script: Ctrl+Shift+N on Windows and Command+Shift+N on Mac\n\n\n\n\n\n\n# Here is an example how to running commends while editing scripts\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(murders)\n\nmurders %&gt;% \n  ggplot(aes(population, total, label=abb, color=region)) +\n  geom_label()\n\n\n\n\n\n\n\n\nTo define a variable, we may use the assignment symbol, &lt;-.\n\n(1) type the variable name into the console and hit Return;\n\n\nuse the print() function by typing print(variable_name) and hitting Return.\n\n\nObjects are things that are stored in named containers in R. They can be variables, functions, etc.\nThe ls() function shows the names of the objects saved in your work space.\n\n\n\n\n\n# assigning values to variables\na &lt;- 1\nb &lt;- 1\nc &lt;- -1\n\n# solving the quadratic equation\n(-b + sqrt(b^2 - 4*a*c))/(2*a)\n(-b - sqrt(b^2 - 4*a*c))/(2*a)\n\n\n\n\n\n\n\n\nIn general, to evaluate a function we need to use parentheses. If we type a function without parenthesis, R shows us the code for the function. Most functions also require an argument, that is, something to be written inside the parenthesis.\nTo access help files, we may use the help function, help(function_name), or write the question mark followed by the function name, ?function_name.\nThe help file shows you the arguments the function is expecting, some of which are required and some are optional. If an argument is optional, a default value is assigned with the equal sign. The args() function also shows the arguments a function needs.\nTo specify arguments, we use the equals sign. If no argument name is used, R assumes you’re entering arguments in the order shown in the help file.\nCreating and saving a script makes code much easier to execute.\nTo make your code more readable, use intuitive variable names and include comments (using the “#” symbol) to remind yourself why you wrote a particular line of code.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe code data(\"dataset_name\") and data(dataset_name) do the same thing. The code will work regardless of whether the quotes are present. It is a bit faster to leave out the quotes (as we do in the Code at the bottom of this page), so that is usually what we recommend, but it is your choice.\n\n\n\n\n\nThe function class() helps us determine the type of an object.\nData frames can be thought of as tables with rows representing observations and columns representing different variables.\nTo access data from columns of a data frame, we use the dollar sign symbol, $, which is called the accessor.\nA vector is an object consisting of several entries and can be a numeric vector, a character vector, or a logical vector.\nWe use quotes to distinguish between variable names and character strings.\nFactors are useful for storing categorical data, and are more memory efficient than storing characters.\n\n\n\n\n\n\n\nKnowledge Extension\n\n\n\n\n\n\n\n\nflowchart LR\n  A{Data Type}---&gt; B[numeric]\n  A{Data Type}---&gt;C[integer]\n  A{Data Type} ---&gt;D[complex]\n  A{Data Type}---&gt;E[character]\n  A{Data Type}---&gt;F[logical]\n\n\n\n\n\n\n\n\nExplanation:Numeric: all real numbers with or without decimal values. e.g. 1, 2, 8, 1.1.Integer(整数): specifies real values without decimal points. we use the suffixL to specify integer data.Complex: specify purely imaginary values in R. We use the suffix i to specify the imaginary part. e.g. 3 + 2i.Character:specify character or string values in a variable. '' for character variables; \"\" for string variables.Logical: is known as boolean data type. It can only have two values: TRUE and FALSE\n\n\n\n\n# loading the dslabs package and the murders dataset\nlibrary(dslabs)\ndata(murders)\n\n# determining that the murders dataset is of the \"data frame\" class\nclass(murders)\n# finding out more about the structure of the object\nstr(murders)\n# showing the first 6 lines of the dataset\nhead(murders)\n\n# using the accessor operator to obtain the population column\nmurders$population\n# displaying the variable names in the murders dataset\nnames(murders)\n# determining how many entries are in a vector\npop &lt;- murders$population\nlength(pop)\n# vectors can be of class numeric and character\nclass(pop)\nclass(murders$state)\n\n# logical vectors are either TRUE or FALSE\nz &lt;- 3 == 2\nz\nclass(z)\n\n# factors are another type of class\nclass(murders$region)\n# obtaining the levels of a factor\nlevels(murders$region)"
  },
  {
    "objectID": "posts/rbasics/index.html#installing-r",
    "href": "posts/rbasics/index.html#installing-r",
    "title": "The basics of R programming",
    "section": "",
    "text": "To complete this course, you should install R locally on your computer. We also highly recommend installing RStudio, an integrated development environment (IDE), to edit and test your code.\nIn order to complete some assignments in the course, you will need your own copy of R. You may also find it helpful to follow along with the course videos in R or RStudio.\nBoth R and RStudio can be freely downloaded and installed.\n\n\n\n\n\nYou need to install R before using RStudio, which is an interactive desktop environment.\nSelect base subdirectory in CRAN and click download.\nSelect all default choices in the installation process.\nWe recommend selecting English for language to help you better follow the course.\nYou can try using the R console, but for productivity purposes, we can switch to RStudio."
  },
  {
    "objectID": "posts/rbasics/index.html#installing-rstudio",
    "href": "posts/rbasics/index.html#installing-rstudio",
    "title": "The basics of R programming",
    "section": "",
    "text": "You can download the latest version of RStudio at the RStudio website.\nThe free desktop version is more than enough for this course.\nMake sure to choose the version for your own operating system.\nChoose “Yes” for all defaults in the installation process."
  },
  {
    "objectID": "posts/rbasics/index.html#using-rstudio-for-the-first-time",
    "href": "posts/rbasics/index.html#using-rstudio-for-the-first-time",
    "title": "The basics of R programming",
    "section": "",
    "text": "The free desktop version of RStudio can be launched like other applications on your computer.\nWhen you start RStudio for the first time, you will see three panes. The left pane shows you the R console. On the right, the top pane includes three tabs, while the bottom pane shows you five tabs, file, plots, packages, help, and viewer.\nYou can download a cheat sheet of the most common RStudio commands directly from RStudio by going to “Help -&gt; Cheat Sheets -&gt; RStudio IDE Cheat Sheet.”"
  },
  {
    "objectID": "posts/rbasics/index.html#getting-started-using-r",
    "href": "posts/rbasics/index.html#getting-started-using-r",
    "title": "The basics of R programming",
    "section": "",
    "text": "R was developed by statisticians and data analysts as an interactive environment for data analysis.\nSome of the advantages of R are that:\n\nit is free and open source;\nit has the capability to save scripts;\nthere are numerous resources for learning;\nit is easy for developers to share software implementation.\n\nExpressions are evaluated in the R console when you type the expression into the console and hit Return.\nA great advantage of R over point and click analysis software is that you can save your work as scripts.\n“Base R” is what you get after you first install R. Additional components are available via packages.\n\n\n\n\nIn RStudio, you can upload additional functions and datasets in addition to the base R functions and datasets that come with R automatically. A common way to do this is by installing packages, which often contain extra functions and datasets. For this course, there are a few packages you will need to install. You only need to install each individual package once, but after you install a package, there are other steps you have to do whenever you want to use something from that package.\nTo install a package, you use the code install.packages(\"package_name\", dependencies = TRUE).\nTo load a package, you use the code library(package_name).\nIf you also want to use a dataset from a package you have loaded, then you use the code data(dataset_name). To see the dataset, you can take the additional step of View(dataset_name)."
  },
  {
    "objectID": "posts/rbasics/index.html#installing-packahes",
    "href": "posts/rbasics/index.html#installing-packahes",
    "title": "The basics of R programming",
    "section": "",
    "text": "We recommend installing packages through RStudio, rather than through R, and the code provided works in both R and RStudio. Once a package has been installed, it is technically added onto R (even if you use RStudio to install it), which is why packages must be re-installed when R is updated. However, since we use R through RStudio, any packages that are installed can be used in both R and RStudio, regardless of which one was used to install the packages.\n\n\n\n\nThe base version of R is quite minimal, but you can supplement its functions by installing additional packages.\nWe will be using tidyverse and dslabs packages for this course.\nInstall packages from R console: install.packages(\"pkg_name\")\nInstall packages from RStudio interface: Tools &gt; Install Packages (allows autocomplete)\nOnce installed, we can use library(pkg_name) to load a package each time we want to use it\n\n\n\n\n\nIf you try to load a package with library(blahblah) and get a message like Error in library(blahblah) : there is no package called ‘blahblah’, it means you need to install that package first with install.packages().\nOn the DataCamp interface we use for some problems in the course, you cannot install additional packages. The problems have been set up with the packages you need to solve them.\nYou can add the option dependencies = TRUE, which tells R to install the other things that are necessary for the package or packages to run smoothly. Otherwise, you may need to install additional packages to unlock the full functionality of a package.\nThroughout the course materials and textbook, package names are in bold.\n\n\n\n\n\ninstall.packages(\"dslabs\") # to install a single package\ninstall.packages(c(\"tidyverse\", \"dslabs\")) # to install two packages at the same time\ninstalled.packages() # to see the list of all installed packages"
  },
  {
    "objectID": "posts/rbasics/index.html#running-commands-while-editing-scripts",
    "href": "posts/rbasics/index.html#running-commands-while-editing-scripts",
    "title": "The basics of R programming",
    "section": "",
    "text": "RStudio has many useful features as an R editor, including the ability to test code easily as we write scripts and several auto complete features.\nKeyboard shortcuts:\n\nSave a script: Ctrl+S on Windows and Command+S on Mac\nRun an entire script: Ctrl+Shift+Enter on Windows Command+Shift+Return on Mac, or click “Source” on the editor pane\nRun a single line of script: Ctrl+Enter on Windows and Command+Return on Mac while the cursor is pointing to that line, or select the chunk and click “run”\nOpen a new script: Ctrl+Shift+N on Windows and Command+Shift+N on Mac\n\n\n\n\n\n\n# Here is an example how to running commends while editing scripts\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(murders)\n\nmurders %&gt;% \n  ggplot(aes(population, total, label=abb, color=region)) +\n  geom_label()"
  },
  {
    "objectID": "posts/rbasics/index.html#r-basics",
    "href": "posts/rbasics/index.html#r-basics",
    "title": "The basics of R programming",
    "section": "",
    "text": "To define a variable, we may use the assignment symbol, &lt;-.\n\n(1) type the variable name into the console and hit Return;\n\n\nuse the print() function by typing print(variable_name) and hitting Return.\n\n\nObjects are things that are stored in named containers in R. They can be variables, functions, etc.\nThe ls() function shows the names of the objects saved in your work space.\n\n\n\n\n\n# assigning values to variables\na &lt;- 1\nb &lt;- 1\nc &lt;- -1\n\n# solving the quadratic equation\n(-b + sqrt(b^2 - 4*a*c))/(2*a)\n(-b - sqrt(b^2 - 4*a*c))/(2*a)"
  },
  {
    "objectID": "posts/rbasics/index.html#function",
    "href": "posts/rbasics/index.html#function",
    "title": "The basics of R programming",
    "section": "",
    "text": "In general, to evaluate a function we need to use parentheses. If we type a function without parenthesis, R shows us the code for the function. Most functions also require an argument, that is, something to be written inside the parenthesis.\nTo access help files, we may use the help function, help(function_name), or write the question mark followed by the function name, ?function_name.\nThe help file shows you the arguments the function is expecting, some of which are required and some are optional. If an argument is optional, a default value is assigned with the equal sign. The args() function also shows the arguments a function needs.\nTo specify arguments, we use the equals sign. If no argument name is used, R assumes you’re entering arguments in the order shown in the help file.\nCreating and saving a script makes code much easier to execute.\nTo make your code more readable, use intuitive variable names and include comments (using the “#” symbol) to remind yourself why you wrote a particular line of code."
  },
  {
    "objectID": "posts/rbasics/index.html#data-types",
    "href": "posts/rbasics/index.html#data-types",
    "title": "The basics of R programming",
    "section": "",
    "text": "Note\n\n\n\nThe code data(\"dataset_name\") and data(dataset_name) do the same thing. The code will work regardless of whether the quotes are present. It is a bit faster to leave out the quotes (as we do in the Code at the bottom of this page), so that is usually what we recommend, but it is your choice.\n\n\n\n\n\nThe function class() helps us determine the type of an object.\nData frames can be thought of as tables with rows representing observations and columns representing different variables.\nTo access data from columns of a data frame, we use the dollar sign symbol, $, which is called the accessor.\nA vector is an object consisting of several entries and can be a numeric vector, a character vector, or a logical vector.\nWe use quotes to distinguish between variable names and character strings.\nFactors are useful for storing categorical data, and are more memory efficient than storing characters.\n\n\n\n\n\n\n\nKnowledge Extension\n\n\n\n\n\n\n\n\nflowchart LR\n  A{Data Type}---&gt; B[numeric]\n  A{Data Type}---&gt;C[integer]\n  A{Data Type} ---&gt;D[complex]\n  A{Data Type}---&gt;E[character]\n  A{Data Type}---&gt;F[logical]\n\n\n\n\n\n\n\n\nExplanation:Numeric: all real numbers with or without decimal values. e.g. 1, 2, 8, 1.1.Integer(整数): specifies real values without decimal points. we use the suffixL to specify integer data.Complex: specify purely imaginary values in R. We use the suffix i to specify the imaginary part. e.g. 3 + 2i.Character:specify character or string values in a variable. '' for character variables; \"\" for string variables.Logical: is known as boolean data type. It can only have two values: TRUE and FALSE\n\n\n\n\n# loading the dslabs package and the murders dataset\nlibrary(dslabs)\ndata(murders)\n\n# determining that the murders dataset is of the \"data frame\" class\nclass(murders)\n# finding out more about the structure of the object\nstr(murders)\n# showing the first 6 lines of the dataset\nhead(murders)\n\n# using the accessor operator to obtain the population column\nmurders$population\n# displaying the variable names in the murders dataset\nnames(murders)\n# determining how many entries are in a vector\npop &lt;- murders$population\nlength(pop)\n# vectors can be of class numeric and character\nclass(pop)\nclass(murders$state)\n\n# logical vectors are either TRUE or FALSE\nz &lt;- 3 == 2\nz\nclass(z)\n\n# factors are another type of class\nclass(murders$region)\n# obtaining the levels of a factor\nlevels(murders$region)"
  },
  {
    "objectID": "posts/rbasics/index.html#vectors",
    "href": "posts/rbasics/index.html#vectors",
    "title": "The basics of R programming",
    "section": "Vectors",
    "text": "Vectors\n\nKey points\n\nThe function c(), which stands for concatenate, is useful for creating vectors.\nAnother useful function for creating vectors is the seq() function, which generates sequences.\nSubsetting lets us access specific parts of a vector by using square brackets to access elements of a vector.\n\n\n\nCode\n\n# We may create vectors of class numeric or character with the concatenate function\ncodes &lt;- c(380, 124, 818)\ncountry &lt;- c(\"italy\", \"canada\", \"egypt\")\n\n# We can also name the elements of a numeric vector\n# Note that the two lines of code below have the same result\ncodes &lt;- c(italy = 380, canada = 124, egypt = 818)\ncodes &lt;- c(\"italy\" = 380, \"canada\" = 124, \"egypt\" = 818)\n\n# We can also name the elements of a numeric vector using the names() function\ncodes &lt;- c(380, 124, 818)\ncountry &lt;- c(\"italy\",\"canada\",\"egypt\")\nnames(codes) &lt;- country\n\n# Using square brackets is useful for subsetting to access specific elements of a vector\ncodes[2]\ncodes[c(1,3)]\ncodes[1:2]\n\n# If the entries of a vector are named, they may be accessed by referring to their name\ncodes[\"canada\"]\ncodes[c(\"egypt\",\"italy\")]"
  },
  {
    "objectID": "posts/rbasics/index.html#vector-coercion",
    "href": "posts/rbasics/index.html#vector-coercion",
    "title": "The basics of R programming",
    "section": "Vector Coercion",
    "text": "Vector Coercion\n\nKey Point\n\nIn general, coercion is an attempt by R to be flexible with data types by guessing what was meant when an entry does not match the expected. For example, when defining x as\n\n\n    x &lt;- c(1, \"canada\", 3)\n\nR coerced the data into characters. It guessed that because you put a character string in the vector, you meant the 1 and 3 to actually be character strings, “1” and “3”.\n\nThe function as.character() turns numbers into characters.\nThe function as.numeric() turns characters into numbers.\nIn R, missing data is assigned the value NA.\n\n\n\nQuestion\n\nclass(3L) is integer ?\n3L-3 equals 0 ?"
  },
  {
    "objectID": "posts/rbasics/index.html#sorting",
    "href": "posts/rbasics/index.html#sorting",
    "title": "The basics of R programming",
    "section": "Sorting",
    "text": "Sorting\n\n\n\n\n\n\n\n\n\nOriginal\nSort(按从小到大排列）\nOrder(Sort对应数字在原来数字排列中的顺序）\nRank(Original原来数字在Sort顺序中的排名）\n\n\n31\n4\n2\n3\n\n\n4\n15\n3\n1\n\n\n15\n31\n1\n2\n\n\n92\n65\n5\n5\n\n\n65\n92\n4\n4\n\n\n\n\nKey Points\n\nThe function sort() sorts a vector in increasing order.\nThe function order() produces the indices needed to obtain the sorted vector, e.g. a result of 2 3 1 5 4 means the sorted vector will be produced by listing the 2nd, 3rd, 1st, 5th, and then 4th item of the original vector.\nThe function rank() gives us the ranks of the items in the original vector.\nThe function max() returns the largest value, while which.max() returns the index of the largest value. The functions min() and which.min() work similarly for minimum values.\n\n\n\nCode\n\nlibrary(dslabs)\ndata(murders)\nsort(murders$total)\n\nx &lt;- c(31, 4, 15, 92, 65)\nx\nsort(x)    # puts elements in order\n\nindex &lt;- order(x)    # returns index that will put x in order\nx[index]    # rearranging by this index puts elements in order\norder(x)\n\nmurders$state[1:10]\nmurders$abb[1:10]\n\nindex &lt;- order(murders$total)\nmurders$abb[index]    # order abbreviations by total murders\n\nmax(murders$total)    # highest number of total murders\ni_max &lt;- which.max(murders$total)    # index with highest number of murders\nmurders$state[i_max]    # state name with highest number of total murders\n\nx &lt;- c(31, 4, 15, 92, 65)\nx\nrank(x)    # returns ranks (smallest to largest)"
  },
  {
    "objectID": "posts/rbasics/index.html#vector-arithmetic",
    "href": "posts/rbasics/index.html#vector-arithmetic",
    "title": "The basics of R programming",
    "section": "Vector Arithmetic",
    "text": "Vector Arithmetic\n\nKey Point\n\nIn R, arithmetic operation on vectors occur element-wise\n\n\n\nCode\n\n# The name of the state with the maximum population is found by doing the following\nmurders$state[which.max(murders$population)]\n\n# how to obtain the murder rate\nmurder_rate &lt;- murders$total / murders$population * 100000\n\n# ordering the states by murder rate, in decreasing order\nmurders$state[order(murder_rate, decreasing=TRUE)]"
  },
  {
    "objectID": "posts/rbasics/index.html#indexing",
    "href": "posts/rbasics/index.html#indexing",
    "title": "The basics of R programming",
    "section": "Indexing",
    "text": "Indexing\n\nKey Point\n\nWe can use logicals to index vectors.\nUsing the function sum()on a logical vector returns the number of entries that are true.\nThe logical operator “&” makes two logicals true only when they are both true.\n\n\n\nCode\n\n# defining murder rate as before\nmurder_rate &lt;- murders$total / murders$population * 100000\n# creating a logical vector that specifies if the murder rate in that state is less than or equal to 0.71\nindex &lt;- murder_rate &lt;= 0.71\n# determining which states have murder rates less than or equal to 0.71\nmurders$state[index]\n# calculating how many states have a murder rate less than or equal to 0.71\nsum(index)\n\n# creating the two logical vectors representing our conditions\nwest &lt;- murders$region == \"West\"\nsafe &lt;- murder_rate &lt;= 1\n# defining an index and identifying states with both conditions true\nindex &lt;- safe & west\nmurders$state[index]"
  },
  {
    "objectID": "posts/rbasics/index.html#indexing-functions",
    "href": "posts/rbasics/index.html#indexing-functions",
    "title": "The basics of R programming",
    "section": "Indexing Functions",
    "text": "Indexing Functions\n\nKey Points\n\nThe function which() gives us the entries of a logical vector that are true.\nThe function match() looks for entries in a vector and returns the index needed to access them.\nWe use the function %in% if we want to know whether or not each element of a first vector is in a second vector.\n\n\n\nCode\n\nx &lt;- c(FALSE, TRUE, FALSE, TRUE, TRUE, FALSE)\nwhich(x)    # returns indices that are TRUE\n\n# to determine the murder rate in Massachusetts we may do the following\nindex &lt;- which(murders$state == \"Massachusetts\")\nindex\nmurder_rate[index]\n\n# to obtain the indices and subsequent murder rates of New York, Florida, Texas, we do:\nindex &lt;- match(c(\"New York\", \"Florida\", \"Texas\"), murders$state)\nindex\nmurders$state[index]\nmurder_rate[index]\n\nx &lt;- c(\"a\", \"b\", \"c\", \"d\", \"e\")\ny &lt;- c(\"a\", \"d\", \"f\")\ny %in% x\n\n# to see if Boston, Dakota, and Washington are states\nc(\"Boston\", \"Dakota\", \"Washington\") %in% murders$state"
  },
  {
    "objectID": "posts/rbasics/index.html#basic-data-wrangling",
    "href": "posts/rbasics/index.html#basic-data-wrangling",
    "title": "The basics of R programming",
    "section": "Basic Data Wrangling",
    "text": "Basic Data Wrangling\n\nKey Points\n\nTo change a data table by adding a new column, or changing an existing one, we use the mutate() function.\nTo filter the data by subsetting rows, we use the function filter().\nTo subset the data by selecting specific columns, we use the select() function.\nWe can perform a series of operations by sending the results of one function to another function using the pipe operator, %&gt;%."
  },
  {
    "objectID": "posts/rbasics/index.html#creating-data-frames",
    "href": "posts/rbasics/index.html#creating-data-frames",
    "title": "The basics of R programming",
    "section": "Creating Data Frames",
    "text": "Creating Data Frames\n\nNote\nThe default settings in R have changed as of version 4.0, and it is no longer necessary to include the code stringsAsFactors = FALSE in order to keep strings as characters. Putting the entries in quotes, as in the example, is adequate to keep strings as characters. The stringsAsFactors = FALSE code is useful in certain other situations, but you do not need to include it when you create data frames in this manner.\n\n\nKey Points\n\nWe can use the data.frame() function to create data frames.\nFormerly, the data.frame() function turned characters into factors by default. To avoid this, we could utilize the stringsAsFactors argument and set it equal to false. As of R 4.0, it is no longer necessary to include the stringsAsFactors argument, because R no longer turns characters into factors by default.\n\n\n\nCode\n\n# creating a data frame with stringAsFactors = FALSE\ngrades &lt;- data.frame(names = c(\"John\", \"Juan\", \"Jean\", \"Yao\"), \n                     exam_1 = c(95, 80, 90, 85), \n                     exam_2 = c(90, 85, 85, 90),\n                     stringsAsFactors = FALSE)"
  },
  {
    "objectID": "posts/rbasics/index.html#basic-plots",
    "href": "posts/rbasics/index.html#basic-plots",
    "title": "The basics of R programming",
    "section": "Basic Plots",
    "text": "Basic Plots\n\nKey Points\n\nWe can create a simple scatterplot using the function plot().\nHistograms are graphical summaries that give you a general overview of the types of values you have. In R, they can be produced using the hist() function.\nBoxplots provide a more compact summary of a distribution than a histogram and are more useful for comparing distributions. They can be produced using the boxplot() function.\n\n\n\nCode\n\nlibrary(dplyr)\nlibrary(dslabs)\ndata(\"murders\")\n\n\n# a simple scatterplot of total murders versus population\nx &lt;- murders$population /10^6\ny &lt;- murders$total\nplot(x, y)\n\n\n\n\n\n\n\n\n\n# a histogram of murder rates\nmurders &lt;- mutate(murders, rate = total / population * 100000)\nhist(murders$rate)\n\n\n\n\n\n\n\n# boxplots of murder rates by region\nboxplot(rate~region, data = murders)"
  },
  {
    "objectID": "posts/rbasics/index.html#the-summarize-function",
    "href": "posts/rbasics/index.html#the-summarize-function",
    "title": "The basics of R programming",
    "section": "The summarize function",
    "text": "The summarize function\n\nKey Points\n\nSummarizing data is an important part of data analysis.\nSome summary ststistics are the mean, median, and standard deviation.\nThe summarize() function from dplyr provides an easy way to compute summary statics.\n\n\n\nCode\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(dslabs)\ndata(murders)\nmurders &lt;- mutate(murders, rate = total / population * 10^5)\n\n\n# minimum, median, and maximum murder rate for the states in the West region\ns &lt;- murders %&gt;% \n  filter(region == \"West\") %&gt;%\n  summarize(minimum = min(rate), \n            median = median(rate), \n            maximum = max(rate))\ns\n\n   minimum   median  maximum\n1 0.514592 1.292453 3.629527\n\n# accessing the components with the accessor $\ns$median\n\n[1] 1.292453\n\ns$maximum\n\n[1] 3.629527\n\n# average rate unadjusted by population size\nmean(murders$rate)\n\n[1] 2.779125\n\n# average rate adjusted by population size\nus_murder_rate &lt;- murders %&gt;% \n  summarize(rate = sum(total) / sum(population) * 10^5)\nus_murder_rate\n\n      rate\n1 3.034555"
  },
  {
    "objectID": "posts/rbasics/index.html#summarizing-with-more-than-one-value",
    "href": "posts/rbasics/index.html#summarizing-with-more-than-one-value",
    "title": "The basics of R programming",
    "section": "Summarizing with more than one value",
    "text": "Summarizing with more than one value\n\nKey Points\n\nThe quantile() function can be used to return the min, median, and max in a single line of code.\n\n\n\nCode\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(dslabs)\ndata(murders)\nmurders &lt;- mutate(murders, rate = total / population * 10^5)\n\n\n# minimum, median, and maximum murder rate for the states in the West region using quantile\n# note that this returns a vector\nmurders %&gt;% \n  filter(region == \"West\") %&gt;%\n  summarize(range = quantile(rate, c(0, 0.5, 1)))\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n     range\n1 0.514592\n2 1.292453\n3 3.629527\n\n# returning minimum, median, and maximum as a data frame\nmy_quantile &lt;- function(x){\n  r &lt;-  quantile(x, c(0, 0.5, 1))\n  data.frame(minimum = r[1], median = r[2], maximum = r[3]) \n}\nmurders %&gt;% \n  filter(region == \"West\") %&gt;%\n  summarize(my_quantile(rate))\n\n   minimum   median  maximum\n1 0.514592 1.292453 3.629527"
  },
  {
    "objectID": "posts/rbasics/index.html#pull-to-access-to-columns",
    "href": "posts/rbasics/index.html#pull-to-access-to-columns",
    "title": "The basics of R programming",
    "section": "Pull to access to columns",
    "text": "Pull to access to columns\n\nKey Points\n\nThe pull() function can be used to access values stored in data when using pipes: when a data object is piped that object and its columns can be accessed using the pull() function.\n\n\n\nCode\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(dslabs)\ndata(murders)\nmurders &lt;- mutate(murders, rate = total / population * 10^5)\n\n\n# average rate adjusted by population size\nus_murder_rate &lt;- murders %&gt;% \n  summarize(rate = sum(total) / sum(population) * 10^5)\nus_murder_rate\n\n      rate\n1 3.034555\n\n# us_murder_rate is stored as a data frame\nclass(us_murder_rate)\n\n[1] \"data.frame\"\n\n# the pull function can return it as a numeric value\nus_murder_rate %&gt;% pull(rate)\n\n[1] 3.034555\n\n# using pull to save the number directly\nus_murder_rate &lt;- murders %&gt;% \n  summarize(rate = sum(total) / sum(population) * 10^5) %&gt;%\n  pull(rate)\nus_murder_rate\n\n[1] 3.034555\n\n# us_murder_rate is now stored as a number\nclass(us_murder_rate)\n\n[1] \"numeric\""
  },
  {
    "objectID": "posts/rbasics/index.html#the-dot-placeholder",
    "href": "posts/rbasics/index.html#the-dot-placeholder",
    "title": "The basics of R programming",
    "section": "The dot placeholder",
    "text": "The dot placeholder\n\nKey Points\n\nThe dot (.) can be thought of as a placeholder for the data being passed through the pipe.\n\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(dslabs)\ndata(murders)\nmurders &lt;- mutate(murders, rate = total / population * 10^5)\n\n\n# average rate adjusted by population size\nus_murder_rate &lt;- murders %&gt;% \n  summarize(rate = sum(total) / sum(population) * 10^5)\nus_murder_rate\n\n      rate\n1 3.034555\n\n# using the dot to access the rate\nus_murder_rate &lt;- murders %&gt;% \n  summarize(rate = sum(total) / sum(population) * 10^5) %&gt;%\n  .$rate\nus_murder_rate\n\n[1] 3.034555\n\nclass(us_murder_rate)\n\n[1] \"numeric\""
  },
  {
    "objectID": "posts/rbasics/index.html#group-then-summarize",
    "href": "posts/rbasics/index.html#group-then-summarize",
    "title": "The basics of R programming",
    "section": "Group then summarize",
    "text": "Group then summarize\n\nKey Points\n\nSplitting data into groups and then computing summaries for each group is a common operation in data exploration.\nWe can use the dplyr group_by() function to create a special grouped data frame to facilitate such summaries.\n\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(dslabs)\ndata(murders)\nmurders &lt;- mutate(murders, rate = total / population * 10^5)\n\n\n# group by region\nmurders %&gt;% group_by(region)\n\n# A tibble: 51 × 6\n# Groups:   region [4]\n   state                abb   region    population total  rate\n   &lt;chr&gt;                &lt;chr&gt; &lt;fct&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Alabama              AL    South        4779736   135  2.82\n 2 Alaska               AK    West          710231    19  2.68\n 3 Arizona              AZ    West         6392017   232  3.63\n 4 Arkansas             AR    South        2915918    93  3.19\n 5 California           CA    West        37253956  1257  3.37\n 6 Colorado             CO    West         5029196    65  1.29\n 7 Connecticut          CT    Northeast    3574097    97  2.71\n 8 Delaware             DE    South         897934    38  4.23\n 9 District of Columbia DC    South         601723    99 16.5 \n10 Florida              FL    South       19687653   669  3.40\n# ℹ 41 more rows\n\n# summarize after grouping\nmurders %&gt;% \n  group_by(region) %&gt;%\n  summarize(median = median(rate))\n\n# A tibble: 4 × 2\n  region        median\n  &lt;fct&gt;          &lt;dbl&gt;\n1 Northeast       1.80\n2 South           3.40\n3 North Central   1.97\n4 West            1.29"
  },
  {
    "objectID": "posts/rbasics/index.html#sorting-data-tables",
    "href": "posts/rbasics/index.html#sorting-data-tables",
    "title": "The basics of R programming",
    "section": "Sorting data tables",
    "text": "Sorting data tables\n\nKey Points\n\nTo order an entire table, we can use the dplyr function arrange().\nWe can also use nested sorting to order by additional columns.\nThe function head() returns on the first few lines of a table.\nThe function top_n() returns the top n rows of a table.\n\n\n\nCode\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(dslabs)\ndata(murders)\nmurders &lt;- mutate(murders, rate = total / population * 10^5)\n\n\n# order the states by population size\nmurders %&gt;% arrange(population) %&gt;% head()\n\n                 state abb        region population total       rate\n1              Wyoming  WY          West     563626     5  0.8871131\n2 District of Columbia  DC         South     601723    99 16.4527532\n3              Vermont  VT     Northeast     625741     2  0.3196211\n4         North Dakota  ND North Central     672591     4  0.5947151\n5               Alaska  AK          West     710231    19  2.6751860\n6         South Dakota  SD North Central     814180     8  0.9825837\n\n# order the states by murder rate - the default is ascending order\nmurders %&gt;% arrange(rate) %&gt;% head()\n\n          state abb        region population total      rate\n1       Vermont  VT     Northeast     625741     2 0.3196211\n2 New Hampshire  NH     Northeast    1316470     5 0.3798036\n3        Hawaii  HI          West    1360301     7 0.5145920\n4  North Dakota  ND North Central     672591     4 0.5947151\n5          Iowa  IA North Central    3046355    21 0.6893484\n6         Idaho  ID          West    1567582    12 0.7655102\n\n# order the states by murder rate in descending order\nmurders %&gt;% arrange(desc(rate)) %&gt;% head()\n\n                 state abb        region population total      rate\n1 District of Columbia  DC         South     601723    99 16.452753\n2            Louisiana  LA         South    4533372   351  7.742581\n3             Missouri  MO North Central    5988927   321  5.359892\n4             Maryland  MD         South    5773552   293  5.074866\n5       South Carolina  SC         South    4625364   207  4.475323\n6             Delaware  DE         South     897934    38  4.231937\n\n# order the states by region and then by murder rate within region\nmurders %&gt;% arrange(region, rate) %&gt;% head()\n\n          state abb    region population total      rate\n1       Vermont  VT Northeast     625741     2 0.3196211\n2 New Hampshire  NH Northeast    1316470     5 0.3798036\n3         Maine  ME Northeast    1328361    11 0.8280881\n4  Rhode Island  RI Northeast    1052567    16 1.5200933\n5 Massachusetts  MA Northeast    6547629   118 1.8021791\n6      New York  NY Northeast   19378102   517 2.6679599\n\n# return the top 10 states by murder rate\nmurders %&gt;% top_n(10, rate)\n\n                  state abb        region population total      rate\n1               Arizona  AZ          West    6392017   232  3.629527\n2              Delaware  DE         South     897934    38  4.231937\n3  District of Columbia  DC         South     601723    99 16.452753\n4               Georgia  GA         South    9920000   376  3.790323\n5             Louisiana  LA         South    4533372   351  7.742581\n6              Maryland  MD         South    5773552   293  5.074866\n7              Michigan  MI North Central    9883640   413  4.178622\n8           Mississippi  MS         South    2967297   120  4.044085\n9              Missouri  MO North Central    5988927   321  5.359892\n10       South Carolina  SC         South    4625364   207  4.475323\n\n# return the top 10 states ranked by murder rate, sorted by murder rate\nmurders %&gt;% arrange(desc(rate)) %&gt;% top_n(10)\n\nSelecting by rate\n\n\n                  state abb        region population total      rate\n1  District of Columbia  DC         South     601723    99 16.452753\n2             Louisiana  LA         South    4533372   351  7.742581\n3              Missouri  MO North Central    5988927   321  5.359892\n4              Maryland  MD         South    5773552   293  5.074866\n5        South Carolina  SC         South    4625364   207  4.475323\n6              Delaware  DE         South     897934    38  4.231937\n7              Michigan  MI North Central    9883640   413  4.178622\n8           Mississippi  MS         South    2967297   120  4.044085\n9               Georgia  GA         South    9920000   376  3.790323\n10              Arizona  AZ          West    6392017   232  3.629527"
  },
  {
    "objectID": "posts/rbasics/index.html#introduction-to-data.table",
    "href": "posts/rbasics/index.html#introduction-to-data.table",
    "title": "The basics of R programming",
    "section": "Introduction to data.table",
    "text": "Introduction to data.table\n\nKey Points\n\nIn this course, we often use tidyverse packages to illustrate because these packages tend to have code that is very readable for beginners.\nThere are other approaches to wrangling and analyzing data in R that are faster and better at handling large objects, such as the data.table package.\nSelecting in data.table uses notation similar to that used with matrices.\nTo add a column in data.table, you can use the := function.\nBecause the data.table package is designed to avoid wasting memory, when you make a copy of a table, it does not create a new object. The := function changes by reference. If you want to make an actual copy, you need to use the copy() function.\nSide note: the R language has a new, built-in pipe operator as of version 4.1: |&gt;. This works similarly to the pipe %&gt;% you are already familiar with. You can read more about the |&gt; pipe here External link.\n\n\n\nCode\n\n# install the data.table package before you use it!\ninstall.packages(\"data.table\")\n\n# load data.table package\nlibrary(data.table)\n\n# load other packages and datasets\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(dslabs)\ndata(murders)\n\n# convert the data frame into a data.table object\nmurders &lt;- setDT(murders)\n\n# selecting in dplyr\nselect(murders, state, region)\n\n# selecting in data.table - 2 methods\nmurders[, c(\"state\", \"region\")] |&gt; head()\nmurders[, .(state, region)] |&gt; head()\n\n# adding or changing a column in dplyr\nmurders &lt;- mutate(murders, rate = total / population * 10^5)\n\n# adding or changing a column in data.table\nmurders[, rate := total / population * 100000]\nhead(murders)\nmurders[, \":=\"(rate = total / population * 100000, rank = rank(population))]\n\n# y is referring to x and := changes by reference\nx &lt;- data.table(a = 1)\ny &lt;- x\n\nx[,a := 2]\ny\n\ny[,a := 1]\nx\n\n# use copy to make an actual copy\nx &lt;- data.table(a = 1)\ny &lt;- copy(x)\nx[,a := 2]\ny"
  },
  {
    "objectID": "posts/rbasics/index.html#subsetting-with-data.table",
    "href": "posts/rbasics/index.html#subsetting-with-data.table",
    "title": "The basics of R programming",
    "section": "Subsetting with data.table",
    "text": "Subsetting with data.table\n\nKey Points\nSubsetting in data.table uses notation similar to that used with matrices.\n\n\nCode\n\n# load packages and prepare the data\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(dslabs)\ndata(murders)\nlibrary(data.table)\nmurders &lt;- setDT(murders)\nmurders &lt;- mutate(murders, rate = total / population * 10^5)\nmurders[, rate := total / population * 100000]\n\n# subsetting in dplyr\nfilter(murders, rate &lt;= 0.7)\n\n# subsetting in data.table\nmurders[rate &lt;= 0.7]\n\n# combining filter and select in data.table\nmurders[rate &lt;= 0.7, .(state, rate)]\n\n# combining filter and select in dplyr\nmurders %&gt;% filter(rate &lt;= 0.7) %&gt;% select(state, rate)"
  },
  {
    "objectID": "posts/rbasics/index.html#summarizing-with-data.table",
    "href": "posts/rbasics/index.html#summarizing-with-data.table",
    "title": "The basics of R programming",
    "section": "Summarizing with data.table",
    "text": "Summarizing with data.table\n\nKey Points\n\nIn data.table we can call functions inside .()and they will be applied to rows.\nThe group_by followed by summarize in dplyr is performed in one line in data.table using the by argument.\n\n\n\nCode\n\n# load packages and prepare the data - heights dataset\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(dslabs)\ndata(heights)\nheights &lt;- setDT(heights)\n\n# summarizing in dplyr\ns &lt;- heights %&gt;% \n  summarize(average = mean(height), standard_deviation = sd(height))\n  \n# summarizing in data.table\ns &lt;- heights[, .(average = mean(height), standard_deviation = sd(height))]\n\n# subsetting and then summarizing in dplyr\ns &lt;- heights %&gt;% \n  filter(sex == \"Female\") %&gt;%\n  summarize(average = mean(height), standard_deviation = sd(height))\n  \n# subsetting and then summarizing in data.table\ns &lt;- heights[sex == \"Female\", .(average = mean(height), standard_deviation = sd(height))]\n\n# previously defined function\nmedian_min_max &lt;- function(x){\n  qs &lt;- quantile(x, c(0.5, 0, 1))\n  data.frame(median = qs[1], minimum = qs[2], maximum = qs[3])\n}\n\n# multiple summaries in data.table\nheights[, .(median_min_max(height))]\n\n# grouping then summarizing in data.table\nheights[, .(average = mean(height), standard_deviation = sd(height)), by = sex]"
  },
  {
    "objectID": "posts/rbasics/index.html#sorting-data-frames",
    "href": "posts/rbasics/index.html#sorting-data-frames",
    "title": "The basics of R programming",
    "section": "Sorting data frames",
    "text": "Sorting data frames\n\nKey Points\n\nTo order rows in a data frame using data.table, we can use the same approach we used for filtering.\nThe default sort is an ascending order, but we can also sort tables in descending order.\nWe can also perform nested sorting by including multiple variables in the desired sort order.\n\n\n\nCode\n\n# load packages and datasets and prepare the data\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(data.table)\nlibrary(dslabs)\ndata(murders)\nmurders &lt;- setDT(murders)\nmurders[, rate := total / population * 100000]\n\n# order by population\nmurders[order(population)] |&gt; head()\n\n# order by population in descending order\nmurders[order(population, decreasing = TRUE)] \n\n# order by region and then murder rate\nmurders[order(region, rate)]"
  },
  {
    "objectID": "posts/rbasics/index.html#programming-basics-1",
    "href": "posts/rbasics/index.html#programming-basics-1",
    "title": "The basics of R programming",
    "section": "Programming Basics",
    "text": "Programming Basics\nIntroduction to Programming in R"
  },
  {
    "objectID": "posts/rbasics/index.html#basic-condationals",
    "href": "posts/rbasics/index.html#basic-condationals",
    "title": "The basics of R programming",
    "section": "Basic Condationals",
    "text": "Basic Condationals\n\nKey Points\n\nThe most common conditional expression in programming is an if-else statement, which has the form “if [condition], perform [expression], else perform [alternative expression]”.\nThe ifelse() function works similarly to an if-else statement, but it is particularly useful since it works on vectors by examining each element of the vector and returning a corresponding answer accordingly.\nThe any() function takes a vector of logicals and returns true if any of the entries are true.\nThe all() function takes a vector of logicals and returns true if all of the entries are true.\n\n\n\nCode\n\n# an example showing the general structure of an if-else statement\na &lt;- 0\nif(a!=0){\n  print(1/a)\n} else{\n  print(\"No reciprocal for 0.\")\n}\n\n# an example that tells us which states, if any, have a murder rate less than 0.5\nlibrary(dslabs)\ndata(murders)\nmurder_rate &lt;- murders$total / murders$population*100000\nind &lt;- which.min(murder_rate)\nif(murder_rate[ind] &lt; 0.5){\n  print(murders$state[ind]) \n} else{\n  print(\"No state has murder rate that low\")\n}\n\n# changing the condition to &lt; 0.25 changes the result\nif(murder_rate[ind] &lt; 0.25){\n  print(murders$state[ind]) \n} else{\n  print(\"No state has a murder rate that low.\")\n}\n\n# the ifelse() function works similarly to an if-else conditional\na &lt;- 0\nifelse(a &gt; 0, 1/a, NA)\n\n# the ifelse() function is particularly useful on vectors\na &lt;- c(0,1,2,-4,5)\nresult &lt;- ifelse(a &gt; 0, 1/a, NA)\n\n# the ifelse() function is also helpful for replacing missing values\ndata(na_example)\nno_nas &lt;- ifelse(is.na(na_example), 0, na_example) \nsum(is.na(no_nas))\n\n# the any() and all() functions evaluate logical vectors\nz &lt;- c(TRUE, TRUE, FALSE)\nany(z)\nall(z)"
  },
  {
    "objectID": "posts/rbasics/index.html#functions",
    "href": "posts/rbasics/index.html#functions",
    "title": "The basics of R programming",
    "section": "Functions",
    "text": "Functions\n\nKey Points\n\nThe R function called function() tells R you are about to define a new function.\nFunctions are objects, so must be assigned a variable name with the arrow operator.\nThe general way to define functions is:\n\n\ndecide the function name, which will be an object,\n\n\ntype function() with your function’s arguments in parentheses, - (3) write all the operations inside brackets.\n\n\nVariables defined inside a function are not saved in the workspace.\n\n\n\nCode\n\n# example of defining a function to compute the average of a vector x\navg &lt;- function(x){\n  s &lt;- sum(x)\n  n &lt;- length(x)\n  s/n\n}\n\n# we see that the above function and the pre-built R mean() function are identical\nx &lt;- 1:100\nidentical(mean(x), avg(x))\n\n# variables inside a function are not defined in the workspace\ns &lt;- 3\navg(1:10)\ns\n\n# the general form of a function\nmy_function &lt;- function(VARIABLE_NAME){\n  perform operations on VARIABLE_NAME and calculate VALUE\n  VALUE\n}\n\n# functions can have multiple arguments as well as default values\navg &lt;- function(x, arithmetic = TRUE){\n  n &lt;- length(x)\n  ifelse(arithmetic, sum(x)/n, prod(x)^(1/n))\n}"
  },
  {
    "objectID": "posts/rbasics/index.html#for-loops",
    "href": "posts/rbasics/index.html#for-loops",
    "title": "The basics of R programming",
    "section": "For Loops",
    "text": "For Loops\n\nKey Points\n\nFor-loops perform the same task over and over while changing the variable. They let us define the range that our variable takes, and then changes the value with each loop and evaluates the expression every time inside the loop.\nThe general form of a for-loop is: “For i in [some range], do operations”. This i changes across the range of values and the operations assume i is a value you’re interested in computing on.\nAt the end of the loop, the value of i is the last value of the range.\n\n\n\nCode\n\n# creating a function that computes the sum of integers 1 through n\ncompute_s_n &lt;- function(n){\n  x &lt;- 1:n\n  sum(x)\n}\n\n# a very simple for-loop\nfor(i in 1:5){\n  print(i)\n}\n\n# a for-loop for our summation\nm &lt;- 25\ns_n &lt;- vector(length = m) # create an empty vector\nfor(n in 1:m){\n  s_n[n] &lt;- compute_s_n(n)\n}\n\n# creating a plot for our summation function\nn &lt;- 1:m\nplot(n, s_n)\n\n# a table of values comparing our function to the summation formula\nhead(data.frame(s_n = s_n, formula = n*(n+1)/2))\n\n# overlaying our function with the summation formula\nplot(n, s_n)\nlines(n, n*(n+1)/2)"
  },
  {
    "objectID": "posts/thebasics/index.html",
    "href": "posts/thebasics/index.html",
    "title": "The basics of R and Rstudio",
    "section": "",
    "text": "What you will learn in this post\n\nThe R program\nWhy learn R programming\nInstallation of R, Rstudio and R packages\nIntroduction to Rmarkdown\nLoading packages in R\n\n\n\nR is a language and package-based software environment that provides a wide variety of functions for statistical computing and graphics. It is available free at the Comprehensive R Archive Network (CRAN) on the Internet. It is also open source and available for all major operating systems. Because R is a programming language it can seem a bit daunting; you have to type in commands to get it to work; however, it does have a Graphical User Interface (GUI) to make things easier and it is not so different from typing formula into Excel. R cope with a huge variety of analyses and someone have written a routine to perform nearly any type of calculation. R comes with a powerful set of routines built in at the start but there are some useful extra packages available on the CRAN website. These include routines for more specialized analyses covering many aspects of scientific research as well as other fields (e.g. economics). There are many advantages in using R:\n\nIt is free, always a consideration.\nIt is open source; this means that many bugs are ironed out. It is extremely powerful and will handle very complex analyses as easily as simple ones.\nIt handle a wide variety of analyses. This is one of the most important features: you only need to know how to use R and you can do more or less any type of analysis; there is no need to learn several different (and expensive) programs.\nIt uses simple text commands. At first this seems hard but it is actually quite easy. The upshot is that you can build up a library of commands and copy/paste them when you need them.\nDocumentation. There is a wealth of help for R. The CRAN site itself hosts a lot of material but there are also other websites that provide examples and documentation. Simply adding CRAN to a web search command will bring up plenty of options.\n\n\n\nR provides a wide variety of statistical (linear and nonlinear modelling, classical statistical tests, time-series analysis, classification, clustering, …) and graphical techniques, and is highly extensible. The R language serves as a vehicle of choice for research in statistical methodology, and an Open Source route to participation in that activity. One of R’s strengths is the ease with which well-designed publication-quality plots can be produced, including mathematical symbols and formulae where needed. R is also available as Free Software under the terms of the Free Software Foundation’s GNU General Public License in source code form. It compiles and runs on a wide variety of UNIX platforms and similar systems (including FreeBSD and Linux), Windows and MacOS.\n\n\n\n\n\n\nTip\n\n\n\nMany users think of R as a statistics system and forget that it as an environment within which statistical techniques are implemented.\n\n\n\n\n\nA new major version of R comes out once a year, and there are 2-3 minor releases each year. It’s a good idea to update regularly. Upgrading can be a bit of a hassle, especially for major versions, which require you to re-install all your packages, but putting it off only makes it worse. Getting R is easy via the Internet. The R Project website is a vast enterprise and has local mirror sites in many countries.\n\nThe first step is to go to https://cran.r-project.org/. The project’s website is the main official information source and can be reached from ?@fig-rproject.\n\n\n\n\n\n\nGetting R from the R Project website. Click the download link and select the nearest mirror site\n\n\n\n\n\nClick Download R for Mac/Windows. Once you have clicked the download link ((cran?)), you have the chance to select a mirror site. These mirrors sites are hosted in servers across the world and using a local one will generally result in a speedier download.\nClick the link appropriate for your system (Linux, Mac, Windows). Once you have selected a mirror site, you can click the link that relates to your operating system ((Rdownload?)). If you use a Mac then you will go to a page where you can select the best option for you (there are versions for various flavours of OSX). If you use Windows then you will go to a Windows-specific page. If you are a Linux user then read the documentation; you generally install R through the terminal rather than via the web page.\n\n\n\n\n\n\nGetting R from the R Project website. You can select the version that is specific to your operating system\n\n\n\n\n\nNow the final step is to click the link and download the installer file, which will download in the usual manner according to the setup of your computer.\n\nOnce you have downloaded the install file, you need to run it to get R onto your computer. If you use a Mac you need to double-click the disk image file to mount the virtual disk. Then double-click the package file to install. If you use Windows, then you need to find the EXE file and run it. The installer will copy the relevant files and you will soon be ready to run R.\n\n\n\nNow R is ready to work for you. Launch R using the regular methods specific to your operating system. If you added a desktop icon or a quick launch button then you can use these or run R from the Applications or Windows Start button.\nR is mainly used as an interactive program — you give R a command and it responds to that command. The result may influence the next command that you give R. Between the time you start R and it gives you the first prompt, any number of things might happen (depending on your installation). But the thing that always happens is that some number of packages are attached to the search list. You can see what those packages are in your case with the command:\n\nsearch()\n\n\n\n\n\n\n\nTip\n\n\n\nYou don’t type the “&gt;” — that is the R prompt, but you do hit the return key at the end of the line\n\n\nThe first item on the search list is the “global environment”. This is your work space where the objects that you create during the R session will be.\nYou quit R with the command:\n\nq()\n\nR will ask you if you want to save or delete the global environment when you quit. If you do save the global environment, then you can start another R session with those objects in the global environment at the start of the new session. You are saving the objects in the global environment, you are not saving the session. In particular, you are not saving the search list.\n\n\n\nRStudio is an integrated development environment (IDE) for R programming (rstudio?). RStudio is a set of integrated tools that allows for a more user-friendly experience for using R. Although you will likely use RStudio as your main console and editor, you must first install R, as RStudio uses R behind the scenes. Similar to R, RStudio is freely-available, cross-platform, and open-source.\nDownload and install it from http://www.rstudio.com/download. RStudio is updated a couple of times a year. When a new version is available, RStudio will let you know. It’s a good idea to upgrade regularly so you can take advantage of the latest and greatest features.\n\nGo to https://www.rstudio.com/products/rstudio/download/\nClick Download under RStudio Desktop.\nClick the link appropriate for your system (Linux, Mac, Windows)\nFollow the instructions of the Installer.\n\n\n\n\nWhenever we want to work with R, we’ll open RStudio. RStudio interfaces directly with R, and is an Integrated Development Environment (IDE). This means that RStudio comes with built-in features that make using R a little easier. When you start RStudio, you’ll see four key panels in the interface shown in Figure 1. We’ll refer to these four “panes” as the editor, the Console, the Environment, and the Files panes. The large square on the left is the Console pane, the square in the top right is the Environment pane, and the square in the bottom right is the Files pane. As you work with R more, you’ll find yourself using the tabs within each of the panes.\nWhen we create a new file, such as an R script, an R Markdown file, or a Shiny app, RStudio will open a fourth pane, known as the source or editor pane. The source pane should show up as a square in the top left. We can open up an .R script in the source pane by going to File, selecting New File, and then selecting R Script:\n\n\n\n\n\n\n\n\nFigure 1: The interface of Rstudio IDE with four key panels\n\n\n\n\n\nWe are going to have our first experience with R through RMarkdown, so let’s do the following.\n\n\n\n\nLike Excel, the power of R comes not from doing small operations individually (like 8*22.3), but rather R’s power comes from being able to operate on whole suites of numbers and datasets (R Core Team, 2023). And also like Excel, some of the biggest power in R is that there are built-in functions that you can use in your analyses (and, as we’ll see, R users can easily create and share functions, and it is this open source developer and contributor community that makes R so awesome).\nR has a mind-blowing collection of built-in functions that are used with the same syntax: function name with parentheses around what the function needs to do what it is supposed to do. We’ve seen a few functions already: we’ve seen plot() and summary().\nFunctions always have the same structure: a name, parentheses, and arguments that you can specify. function_name(arguments). When we talk about function names, we use the convention function_name() (the name with empty parentheses), but in practice, we usually supply arguments to the function function_name(arguments) so that it works on some data. Let’s see a few more function examples.\nLike in Excel, there is a function called “sum” to calculate a total. In R, it is spelled lowercase: sum(). (As I type in the Console, R will provide suggestions). Let’s use the sum() function to calculate the sum of all the distances traveled in the cars dataset. We specify a single column of a dataset using the $ operator:\nsum(cars$dist)\nAnother function is simply called c(); which combines values together.\nSo let’s create a new R code chunk. And we’ll write:\n\nc(1, 7:9)\n\n[1] 1 7 8 9\n\n\n\n\n\n\n\n\nNote\n\n\n\nsome functions don’t require arguments: try typing date() into the Console.\n\n\n\nBe sure to type the parentheses (date()); otherwise R will return the code behind the date() function rather than the output that you want/expect.\n\nSo you can see that this combines these values all into the same place, which is called a vector here. We could also do this with a non-numeric examples, which are called “strings”:\n\nc(\"San Francisco\", \"Cal Academy\") \n\n[1] \"San Francisco\" \"Cal Academy\"  \n\n\nWe need to put quotes around non-numeric values so that R does not interpret them as an object. It would definitely get grumpy and give us an error that it did not have an object by these names. And you see that R also prints in quotes. We can also put functions inside of other functions.This is called nested functions. When we add another function inside a function, R will evaluate them from the inside-out.\n\nc(sum(cars$dist), \"San Francisco\", \"Cal Academy\") \n\n[1] \"2149\"          \"San Francisco\" \"Cal Academy\"  \n\n\nSo R first evaluated the sum(cars$dist), and then evaluates the c() statement. This example demonstrates another key idea in R: the idea of classes. The output R provides is called a vector, and everything within that vector has to be the same type of thing: we can’t have both numbers and words inside. So here R is able to first calculate sum(cars$dist) as a number, but then c() will turn that number into a text, called a “string” in R: you see that it is in quotes. It is no longer a numeric, it is a string.\nThis is a big difference between R and Excel, since Excel allows you to have a mix of text and numeric in the same column or row. R’s way can feel restrictive, but it is also more predictable. In Excel, you might have a single number in your whole sheet that Excel is silently interpreting as text so it is causing errors in the analyses. In R, the whole column will be the same type. This can still cause trouble, but that is where the good practices that we are learning together can help minimize that kind of trouble.\n\n\n\nSo far we’ve been using a couple functions that are included with R out-of-the-box such as plot() and c(). We say that these functions are from “Base R”. But, one of the amazing things about R is its users’ community that create new functions and packages. An R package is a collection of functions, data, and documentation that extends the capabilities of base R. Using packages is key to the successful use of R. The majority of the packages that you will learn in this manual are part of the so-called tidyverse, which is an ecosystem of packages that share a common philosophy of data and R programming, and are designed to work together naturally.\nThe tidyverse is a coherent system of packages for data manipulation, exploration and visualization that share a common design philosophy. The Tidyverse (Wickham and Wickham, 2017) packages form a core set of functions that will allow us to perform most any type of data cleaning or analysis we will need to do. We will use the following packages from the tidyverse\n\nggplot2—for data visualisation.\ndplyr—for data manipulation.\ntidyr—for data tidying.\nreadr—for data import.\npurrr—for functional programming.\ntibble—for tibbles, a modern re-imagining of data frames.\n\n\n\nFor us to use tidyverse and any other package that is not included in Base R, we must install them first. The easiest way to install packages is to use the install.packages() command. For example, let’s go ahead and install the tidyverse package on your machine:\ninstall.packages(\"tidyverse\")\nOn your own computer, type that line of code in the console, and then press enter to run it. R will download the packages from CRAN and install it to your computer. If you have problems installing, make sure that you are connected to the internet, and that https://cloud.r-project.org/ isn’t blocked by your firewall or proxy.\n\n\n\nNow we’ve installed the package, but we need to tell R that we are going to use some functions within the tidyverse package. With exception to base R package, add on package that are installed must be called with either library or require functions to make their tools accessible in R session. Let’s us load the tidyverse package we just installed\nrequire(tidyverse)\nYou notice that when we load tidyverse, it popup a notification message showing the loaded packages and the conflicts they bring in. These conflicts happen when packages have functions with the same names as other functions. This is OK when you prefer the function in a package like tidyverse rather than some other function. Basically the last package loaded in will mask over other functions if they have common names.\n\n\n\nThere are many other excellent packages that are not part of the tidyverse, because they solve problems in a different domain, or are designed with a different set of underlying principles. This doesn’t make them better or worse, just different. In other words, the complement to the tidyverse is not the messyverse, but many other universes of interrelated packages (r4ds?). As you tackle more data science projects with R, you’ll learn new packages and new ways of thinking about data. In this course we’ll use several data packages from outside the tidyverse:\nadditional.packages = c(\"metR\", \"cowplot\", \"ggspatial\", \"patchwork\", \"ggrepel\",\n                        \"oce\", \"tmap\", \"leaflet\", \"bookdown\", \"blogdown\", \n                        \"rmarkdown\", \"tinytex\", \"tidymodels\", \"terra\", \n                        \"tidyterra\")\n\ninstall.packages(additional.packages)\n\n\n\nShow version number of R and installed OS\nsessionInfo()\nshow R version\netRversion()\nshow current working directory\ngetwd()\nchange working directory to a user defined location\nsetwd(\"C:/...\")\nlist all packages in the R package library\nlibrary()\nopen download mirrors list for package installation\navailable.packages()\ndownload R packages from CRAN repository to a local folder\n`download.packages(\"package_name\", \"download_directory\")\nupdate all packages\nupdate.packages(checkBuilt=TRUE)\nshow version numbers of R packages, dependencies of packages, recommended packages, license info and builded R version number\ninstalled.packages()\nparses and returns the description file of a package\npackageDescription(\"package_name\")\nremove packages\n`remove.packages(\"package_name\", \"installed_directory\")\nshow license info\nlicense()\ndisplay citation\ncitation()\nload packagesinto the session\nrequire(package_name)\nrunning time of R\nproc.time()\ncurrent memory usage\nmemory.size()\ntotal available memory\nmemory.limit()\nterminate an R session\nq()\ndisplay warning messages\nwarnings()\n\n\n\nBoth online and offline help documentations and manuals for R are available. You can start help while using R with help.start() function from the console. For more help, you can use online documentations from the official R website by using these links: + Help page: https://www.r-project.org/help.html + Manuals page: https://cran.r-project.org/manuals.html + Frequently Asked Questions (FAQs) Page: https://cran.r-project.org/faqs.html\nEvery function available to you should have a help page, and you access it by typing a question mark preceding the function name in the Console. Let’s have a deeper look at the arguments for plot(), using the help pages.\n    ?plot\nThis opens up the correct page in the Help Tab in the bottom-right of the RStudio IDE. You can also click on the tab and type in the function name in the search bar.\nAll help pages will have the same format, here is how I look at it:\nThe help page tells the name of the package in the top left, and broken down into sections:\n\nHelp pages\n- Description: An extended description of what the function does. - Usage: The arguments of the function and their default values. - Arguments: An explanation of the data each argument is expecting. - Details: Any important details to be aware of. - Value: The data the function returns. - See Also: Any related functions you might find useful. - Examples: Some examples for how to use the function.\n\nWhen I look at a help page, I start with the Description to see if I am in the right place for what I need to do. Reading the description for plot lets me know that yup, this is the function I want.\nI next look at the usage and arguments, which give me a more concrete view into what the function does. plot requires arguments for x and y. But we passed only one argument to plot(): we passed the cars dataset (plot(cars)). R is able to understand that it should use the two columns in that dataset as x and y, and it does so based on order: the first column “speed” becomes x and the second column “dist” becomes y. The ... means that there are many other arguments we can pass to plot(), which we should expect: I think we can all agree that it would be nice to have the option of making this figure a little more beautiful and compelling. Glancing at some of the arguments, we can understand here to be about the style of the plots.\nNext, I usually scroll down to the bottom to the examples. This is where I can actually see how the function is used, and I can also paste those examples into the Console to see their output. Let’s try it:\n\n\n\n\n\n\n\n\nFigure 2: A cosine function of x\n\n\n\n\n\n\n\n\n\nR is a programming language, and just like any language, it has different dialects. When you read about R online, you’ll frequently see people mentioning the words “base” and “tidyverse.” These are the two most common dialects of R. Base R is just that, R in its purest form. The tidyverse is a collection of add-on packages for working with different types of data. The two are fully compatible, and you can mix and match as much as you like. Both ggplot2 (Wickham, 2016) and magrittr (Bache and Wickham, 2014) are part of the tidyverse (Wickham and Wickham, 2017).\nIn recent years, the tidyverse has been heavily promoted as being “modern” R which “makes data science faster, easier and more fun.” You should believe the hype. The tidyverse is marvellous. But if you only learn tidyverse R, you will miss out on much of what R has to offer. Base R is just as marvellous, and can definitely make data science as fast, easy and fun as the tidyverse. Besides, nobody uses just base R anyway - there are a ton of non-tidyverse packages that extend and enrich R in exciting new ways. Perhaps “extended R” would be better names for the non-tidyverse dialect.\nAnyone who tells you to just learn one of these dialects is wrong. Both are great, they work extremely well together, and they are similar enough that you shouldn’t limit yourself to just mastering one of them. This book will show you both base R and tidyverse solutions to problems, so that you can decide for yourself which is faster, easier, and more fun.\nA defining property of the tidyverse is that there are separate functions for everything, which is perfect for code that relies on pipes. In contrast, base R uses fewer functions, but with more parameters, to perform the same tasks. If you use tidyverse solutions there is a good chance that there exists a function which performs exactly the task you’re going to do with its default settings. This is great (once again, especially if you want to use pipes), but it means that there are many more functions to master for tidyverse users, whereas you can make do with much fewer in base R. You will spend more time looking up function arguments when working with base R (which fortunately is fairly straightforward using the ? documentation), but on the other hand, looking up arguments for a function that you know the name of is easier than finding a function that does something very specific that you don’t know the name of. There are advantages and disadvantages to both approaches."
  },
  {
    "objectID": "posts/thebasics/index.html#the-r-program",
    "href": "posts/thebasics/index.html#the-r-program",
    "title": "The basics of R and Rstudio",
    "section": "",
    "text": "R is a language and package-based software environment that provides a wide variety of functions for statistical computing and graphics. It is available free at the Comprehensive R Archive Network (CRAN) on the Internet. It is also open source and available for all major operating systems. Because R is a programming language it can seem a bit daunting; you have to type in commands to get it to work; however, it does have a Graphical User Interface (GUI) to make things easier and it is not so different from typing formula into Excel. R cope with a huge variety of analyses and someone have written a routine to perform nearly any type of calculation. R comes with a powerful set of routines built in at the start but there are some useful extra packages available on the CRAN website. These include routines for more specialized analyses covering many aspects of scientific research as well as other fields (e.g. economics). There are many advantages in using R:\n\nIt is free, always a consideration.\nIt is open source; this means that many bugs are ironed out. It is extremely powerful and will handle very complex analyses as easily as simple ones.\nIt handle a wide variety of analyses. This is one of the most important features: you only need to know how to use R and you can do more or less any type of analysis; there is no need to learn several different (and expensive) programs.\nIt uses simple text commands. At first this seems hard but it is actually quite easy. The upshot is that you can build up a library of commands and copy/paste them when you need them.\nDocumentation. There is a wealth of help for R. The CRAN site itself hosts a lot of material but there are also other websites that provide examples and documentation. Simply adding CRAN to a web search command will bring up plenty of options.\n\n\n\nR provides a wide variety of statistical (linear and nonlinear modelling, classical statistical tests, time-series analysis, classification, clustering, …) and graphical techniques, and is highly extensible. The R language serves as a vehicle of choice for research in statistical methodology, and an Open Source route to participation in that activity. One of R’s strengths is the ease with which well-designed publication-quality plots can be produced, including mathematical symbols and formulae where needed. R is also available as Free Software under the terms of the Free Software Foundation’s GNU General Public License in source code form. It compiles and runs on a wide variety of UNIX platforms and similar systems (including FreeBSD and Linux), Windows and MacOS.\n\n\n\n\n\n\nTip\n\n\n\nMany users think of R as a statistics system and forget that it as an environment within which statistical techniques are implemented.\n\n\n\n\n\nA new major version of R comes out once a year, and there are 2-3 minor releases each year. It’s a good idea to update regularly. Upgrading can be a bit of a hassle, especially for major versions, which require you to re-install all your packages, but putting it off only makes it worse. Getting R is easy via the Internet. The R Project website is a vast enterprise and has local mirror sites in many countries.\n\nThe first step is to go to https://cran.r-project.org/. The project’s website is the main official information source and can be reached from ?@fig-rproject.\n\n\n\n\n\n\nGetting R from the R Project website. Click the download link and select the nearest mirror site\n\n\n\n\n\nClick Download R for Mac/Windows. Once you have clicked the download link ((cran?)), you have the chance to select a mirror site. These mirrors sites are hosted in servers across the world and using a local one will generally result in a speedier download.\nClick the link appropriate for your system (Linux, Mac, Windows). Once you have selected a mirror site, you can click the link that relates to your operating system ((Rdownload?)). If you use a Mac then you will go to a page where you can select the best option for you (there are versions for various flavours of OSX). If you use Windows then you will go to a Windows-specific page. If you are a Linux user then read the documentation; you generally install R through the terminal rather than via the web page.\n\n\n\n\n\n\nGetting R from the R Project website. You can select the version that is specific to your operating system\n\n\n\n\n\nNow the final step is to click the link and download the installer file, which will download in the usual manner according to the setup of your computer.\n\nOnce you have downloaded the install file, you need to run it to get R onto your computer. If you use a Mac you need to double-click the disk image file to mount the virtual disk. Then double-click the package file to install. If you use Windows, then you need to find the EXE file and run it. The installer will copy the relevant files and you will soon be ready to run R.\n\n\n\nNow R is ready to work for you. Launch R using the regular methods specific to your operating system. If you added a desktop icon or a quick launch button then you can use these or run R from the Applications or Windows Start button.\nR is mainly used as an interactive program — you give R a command and it responds to that command. The result may influence the next command that you give R. Between the time you start R and it gives you the first prompt, any number of things might happen (depending on your installation). But the thing that always happens is that some number of packages are attached to the search list. You can see what those packages are in your case with the command:\n\nsearch()\n\n\n\n\n\n\n\nTip\n\n\n\nYou don’t type the “&gt;” — that is the R prompt, but you do hit the return key at the end of the line\n\n\nThe first item on the search list is the “global environment”. This is your work space where the objects that you create during the R session will be.\nYou quit R with the command:\n\nq()\n\nR will ask you if you want to save or delete the global environment when you quit. If you do save the global environment, then you can start another R session with those objects in the global environment at the start of the new session. You are saving the objects in the global environment, you are not saving the session. In particular, you are not saving the search list.\n\n\n\nRStudio is an integrated development environment (IDE) for R programming (rstudio?). RStudio is a set of integrated tools that allows for a more user-friendly experience for using R. Although you will likely use RStudio as your main console and editor, you must first install R, as RStudio uses R behind the scenes. Similar to R, RStudio is freely-available, cross-platform, and open-source.\nDownload and install it from http://www.rstudio.com/download. RStudio is updated a couple of times a year. When a new version is available, RStudio will let you know. It’s a good idea to upgrade regularly so you can take advantage of the latest and greatest features.\n\nGo to https://www.rstudio.com/products/rstudio/download/\nClick Download under RStudio Desktop.\nClick the link appropriate for your system (Linux, Mac, Windows)\nFollow the instructions of the Installer.\n\n\n\n\nWhenever we want to work with R, we’ll open RStudio. RStudio interfaces directly with R, and is an Integrated Development Environment (IDE). This means that RStudio comes with built-in features that make using R a little easier. When you start RStudio, you’ll see four key panels in the interface shown in Figure 1. We’ll refer to these four “panes” as the editor, the Console, the Environment, and the Files panes. The large square on the left is the Console pane, the square in the top right is the Environment pane, and the square in the bottom right is the Files pane. As you work with R more, you’ll find yourself using the tabs within each of the panes.\nWhen we create a new file, such as an R script, an R Markdown file, or a Shiny app, RStudio will open a fourth pane, known as the source or editor pane. The source pane should show up as a square in the top left. We can open up an .R script in the source pane by going to File, selecting New File, and then selecting R Script:\n\n\n\n\n\n\n\n\nFigure 1: The interface of Rstudio IDE with four key panels\n\n\n\n\n\nWe are going to have our first experience with R through RMarkdown, so let’s do the following."
  },
  {
    "objectID": "posts/thebasics/index.html#r-functions",
    "href": "posts/thebasics/index.html#r-functions",
    "title": "The basics of R and Rstudio",
    "section": "",
    "text": "Like Excel, the power of R comes not from doing small operations individually (like 8*22.3), but rather R’s power comes from being able to operate on whole suites of numbers and datasets (R Core Team, 2023). And also like Excel, some of the biggest power in R is that there are built-in functions that you can use in your analyses (and, as we’ll see, R users can easily create and share functions, and it is this open source developer and contributor community that makes R so awesome).\nR has a mind-blowing collection of built-in functions that are used with the same syntax: function name with parentheses around what the function needs to do what it is supposed to do. We’ve seen a few functions already: we’ve seen plot() and summary().\nFunctions always have the same structure: a name, parentheses, and arguments that you can specify. function_name(arguments). When we talk about function names, we use the convention function_name() (the name with empty parentheses), but in practice, we usually supply arguments to the function function_name(arguments) so that it works on some data. Let’s see a few more function examples.\nLike in Excel, there is a function called “sum” to calculate a total. In R, it is spelled lowercase: sum(). (As I type in the Console, R will provide suggestions). Let’s use the sum() function to calculate the sum of all the distances traveled in the cars dataset. We specify a single column of a dataset using the $ operator:\nsum(cars$dist)\nAnother function is simply called c(); which combines values together.\nSo let’s create a new R code chunk. And we’ll write:\n\nc(1, 7:9)\n\n[1] 1 7 8 9\n\n\n\n\n\n\n\n\nNote\n\n\n\nsome functions don’t require arguments: try typing date() into the Console.\n\n\n\nBe sure to type the parentheses (date()); otherwise R will return the code behind the date() function rather than the output that you want/expect.\n\nSo you can see that this combines these values all into the same place, which is called a vector here. We could also do this with a non-numeric examples, which are called “strings”:\n\nc(\"San Francisco\", \"Cal Academy\") \n\n[1] \"San Francisco\" \"Cal Academy\"  \n\n\nWe need to put quotes around non-numeric values so that R does not interpret them as an object. It would definitely get grumpy and give us an error that it did not have an object by these names. And you see that R also prints in quotes. We can also put functions inside of other functions.This is called nested functions. When we add another function inside a function, R will evaluate them from the inside-out.\n\nc(sum(cars$dist), \"San Francisco\", \"Cal Academy\") \n\n[1] \"2149\"          \"San Francisco\" \"Cal Academy\"  \n\n\nSo R first evaluated the sum(cars$dist), and then evaluates the c() statement. This example demonstrates another key idea in R: the idea of classes. The output R provides is called a vector, and everything within that vector has to be the same type of thing: we can’t have both numbers and words inside. So here R is able to first calculate sum(cars$dist) as a number, but then c() will turn that number into a text, called a “string” in R: you see that it is in quotes. It is no longer a numeric, it is a string.\nThis is a big difference between R and Excel, since Excel allows you to have a mix of text and numeric in the same column or row. R’s way can feel restrictive, but it is also more predictable. In Excel, you might have a single number in your whole sheet that Excel is silently interpreting as text so it is causing errors in the analyses. In R, the whole column will be the same type. This can still cause trouble, but that is where the good practices that we are learning together can help minimize that kind of trouble."
  },
  {
    "objectID": "posts/thebasics/index.html#packages",
    "href": "posts/thebasics/index.html#packages",
    "title": "The basics of R and Rstudio",
    "section": "",
    "text": "So far we’ve been using a couple functions that are included with R out-of-the-box such as plot() and c(). We say that these functions are from “Base R”. But, one of the amazing things about R is its users’ community that create new functions and packages. An R package is a collection of functions, data, and documentation that extends the capabilities of base R. Using packages is key to the successful use of R. The majority of the packages that you will learn in this manual are part of the so-called tidyverse, which is an ecosystem of packages that share a common philosophy of data and R programming, and are designed to work together naturally.\nThe tidyverse is a coherent system of packages for data manipulation, exploration and visualization that share a common design philosophy. The Tidyverse (Wickham and Wickham, 2017) packages form a core set of functions that will allow us to perform most any type of data cleaning or analysis we will need to do. We will use the following packages from the tidyverse\n\nggplot2—for data visualisation.\ndplyr—for data manipulation.\ntidyr—for data tidying.\nreadr—for data import.\npurrr—for functional programming.\ntibble—for tibbles, a modern re-imagining of data frames.\n\n\n\nFor us to use tidyverse and any other package that is not included in Base R, we must install them first. The easiest way to install packages is to use the install.packages() command. For example, let’s go ahead and install the tidyverse package on your machine:\ninstall.packages(\"tidyverse\")\nOn your own computer, type that line of code in the console, and then press enter to run it. R will download the packages from CRAN and install it to your computer. If you have problems installing, make sure that you are connected to the internet, and that https://cloud.r-project.org/ isn’t blocked by your firewall or proxy.\n\n\n\nNow we’ve installed the package, but we need to tell R that we are going to use some functions within the tidyverse package. With exception to base R package, add on package that are installed must be called with either library or require functions to make their tools accessible in R session. Let’s us load the tidyverse package we just installed\nrequire(tidyverse)\nYou notice that when we load tidyverse, it popup a notification message showing the loaded packages and the conflicts they bring in. These conflicts happen when packages have functions with the same names as other functions. This is OK when you prefer the function in a package like tidyverse rather than some other function. Basically the last package loaded in will mask over other functions if they have common names.\n\n\n\nThere are many other excellent packages that are not part of the tidyverse, because they solve problems in a different domain, or are designed with a different set of underlying principles. This doesn’t make them better or worse, just different. In other words, the complement to the tidyverse is not the messyverse, but many other universes of interrelated packages (r4ds?). As you tackle more data science projects with R, you’ll learn new packages and new ways of thinking about data. In this course we’ll use several data packages from outside the tidyverse:\nadditional.packages = c(\"metR\", \"cowplot\", \"ggspatial\", \"patchwork\", \"ggrepel\",\n                        \"oce\", \"tmap\", \"leaflet\", \"bookdown\", \"blogdown\", \n                        \"rmarkdown\", \"tinytex\", \"tidymodels\", \"terra\", \n                        \"tidyterra\")\n\ninstall.packages(additional.packages)\n\n\n\nShow version number of R and installed OS\nsessionInfo()\nshow R version\netRversion()\nshow current working directory\ngetwd()\nchange working directory to a user defined location\nsetwd(\"C:/...\")\nlist all packages in the R package library\nlibrary()\nopen download mirrors list for package installation\navailable.packages()\ndownload R packages from CRAN repository to a local folder\n`download.packages(\"package_name\", \"download_directory\")\nupdate all packages\nupdate.packages(checkBuilt=TRUE)\nshow version numbers of R packages, dependencies of packages, recommended packages, license info and builded R version number\ninstalled.packages()\nparses and returns the description file of a package\npackageDescription(\"package_name\")\nremove packages\n`remove.packages(\"package_name\", \"installed_directory\")\nshow license info\nlicense()\ndisplay citation\ncitation()\nload packagesinto the session\nrequire(package_name)\nrunning time of R\nproc.time()\ncurrent memory usage\nmemory.size()\ntotal available memory\nmemory.limit()\nterminate an R session\nq()\ndisplay warning messages\nwarnings()\n\n\n\nBoth online and offline help documentations and manuals for R are available. You can start help while using R with help.start() function from the console. For more help, you can use online documentations from the official R website by using these links: + Help page: https://www.r-project.org/help.html + Manuals page: https://cran.r-project.org/manuals.html + Frequently Asked Questions (FAQs) Page: https://cran.r-project.org/faqs.html\nEvery function available to you should have a help page, and you access it by typing a question mark preceding the function name in the Console. Let’s have a deeper look at the arguments for plot(), using the help pages.\n    ?plot\nThis opens up the correct page in the Help Tab in the bottom-right of the RStudio IDE. You can also click on the tab and type in the function name in the search bar.\nAll help pages will have the same format, here is how I look at it:\nThe help page tells the name of the package in the top left, and broken down into sections:\n\nHelp pages\n- Description: An extended description of what the function does. - Usage: The arguments of the function and their default values. - Arguments: An explanation of the data each argument is expecting. - Details: Any important details to be aware of. - Value: The data the function returns. - See Also: Any related functions you might find useful. - Examples: Some examples for how to use the function.\n\nWhen I look at a help page, I start with the Description to see if I am in the right place for what I need to do. Reading the description for plot lets me know that yup, this is the function I want.\nI next look at the usage and arguments, which give me a more concrete view into what the function does. plot requires arguments for x and y. But we passed only one argument to plot(): we passed the cars dataset (plot(cars)). R is able to understand that it should use the two columns in that dataset as x and y, and it does so based on order: the first column “speed” becomes x and the second column “dist” becomes y. The ... means that there are many other arguments we can pass to plot(), which we should expect: I think we can all agree that it would be nice to have the option of making this figure a little more beautiful and compelling. Glancing at some of the arguments, we can understand here to be about the style of the plots.\nNext, I usually scroll down to the bottom to the examples. This is where I can actually see how the function is used, and I can also paste those examples into the Console to see their output. Let’s try it:\n\n\n\n\n\n\n\n\nFigure 2: A cosine function of x"
  },
  {
    "objectID": "posts/thebasics/index.html#flavours-of-r-base-and-tidyverse",
    "href": "posts/thebasics/index.html#flavours-of-r-base-and-tidyverse",
    "title": "The basics of R and Rstudio",
    "section": "",
    "text": "R is a programming language, and just like any language, it has different dialects. When you read about R online, you’ll frequently see people mentioning the words “base” and “tidyverse.” These are the two most common dialects of R. Base R is just that, R in its purest form. The tidyverse is a collection of add-on packages for working with different types of data. The two are fully compatible, and you can mix and match as much as you like. Both ggplot2 (Wickham, 2016) and magrittr (Bache and Wickham, 2014) are part of the tidyverse (Wickham and Wickham, 2017).\nIn recent years, the tidyverse has been heavily promoted as being “modern” R which “makes data science faster, easier and more fun.” You should believe the hype. The tidyverse is marvellous. But if you only learn tidyverse R, you will miss out on much of what R has to offer. Base R is just as marvellous, and can definitely make data science as fast, easy and fun as the tidyverse. Besides, nobody uses just base R anyway - there are a ton of non-tidyverse packages that extend and enrich R in exciting new ways. Perhaps “extended R” would be better names for the non-tidyverse dialect.\nAnyone who tells you to just learn one of these dialects is wrong. Both are great, they work extremely well together, and they are similar enough that you shouldn’t limit yourself to just mastering one of them. This book will show you both base R and tidyverse solutions to problems, so that you can decide for yourself which is faster, easier, and more fun.\nA defining property of the tidyverse is that there are separate functions for everything, which is perfect for code that relies on pipes. In contrast, base R uses fewer functions, but with more parameters, to perform the same tasks. If you use tidyverse solutions there is a good chance that there exists a function which performs exactly the task you’re going to do with its default settings. This is great (once again, especially if you want to use pipes), but it means that there are many more functions to master for tidyverse users, whereas you can make do with much fewer in base R. You will spend more time looking up function arguments when working with base R (which fortunately is fairly straightforward using the ? documentation), but on the other hand, looking up arguments for a function that you know the name of is easier than finding a function that does something very specific that you don’t know the name of. There are advantages and disadvantages to both approaches."
  },
  {
    "objectID": "posts/timeline/index.html",
    "href": "posts/timeline/index.html",
    "title": "Creating a Timeline graphic using R and ggplot2",
    "section": "",
    "text": "In this post we’re going to be using R and ggplot2 to create a project timeline with milestones and milestone statuses.\nThe finished product will look like as illustrated in Figure 1\n\n\n\n\n\n\nFigure 1: A timeline of the Milestone for learning Modern\n\n\n\n\nrequire(tidyverse)\nrequire(lubridate)\nrequire(scales)\nrequire(ggrepel)\n\n\nset.seed(182)\n\nasuta = tibble(\n  milestone = c(\"kick-off\\nmeeting\", \"Community\\nOutreach and\\nEngagement\", \n                \"Inception\\nand Annexes\", \"Intervention\\nStrategy 2.0\", \"Fisheries\\npatterns\", \n                \"Progress\\nReport 1.0\", \"MEET\\nReport\", \"Community\\nProfile\", \n                \"Resource \\nandClimate\\n\",\"GBV\\ntraining\", \"Intervention\\nStrategy 3.0\", \n                \"Financing\\nAccess\", \"Climate change\\nAdaptation\", \"Progress\\nReport 2.0\"),\n  date = c(100424, 300424, 100524, 200524, 300524, 080624, 300624, 150724,030824, 310724,180824, 200824, 310824, 100924),\n   direction = rep(c(1,-1), times = 7),\n  status = c(\"Implementing\", \"Implementing\", \"Reporting\", \"Strategy\", \"Implementing\",\"Reporting\", \n             \"Reporting\", \"Reporting\",\"Implementing\", \"Reporting\", \"Strategy\", \"Reporting\", \n             \"Reporting\", \"Reporting\")\n) |&gt; \n  mutate(\n   date = dmy(date),\n    position =abs(rnorm(n = 14, mean = .15, sd = .08))  * direction,\n    text_position =  if_else(position &gt;= 0, position + 0.08, position - 0.08)\n    ) |&gt; \n  mutate(\n    milestone = paste0(\"Milestone-\", 1:14,\"\\n\", milestone)\n  ) |&gt; \n  arrange(date)\n\ndate_range = asuta |&gt; pull(date) |&gt; range()\n\nasuta.week = tibble(\n  daily = seq(date_range[1], date_range[2], by = \"5 day\")\n) |&gt; \n  mutate(\n    day = day(daily),\n    month = month(daily, label = T),\n    year = year(daily)\n    )\n\n\nggplot()+\n  geom_hline(yintercept = 0)+\n  geom_segment(data = asuta, aes(x = date, y = 0, yend = position, color = status))+\n  geom_point(data = asuta, aes(x = date, y = position, color = status),size = 3) +\n  geom_text(data = asuta, aes(x = date, y = text_position, label = milestone, color = status), size = 3)+\n  scale_x_date(expand = expansion(mult = 0.08))+\n  theme_minimal()+\n  theme(\n    axis.title = element_blank(), \n    axis.text = element_blank(), \n    axis.ticks = element_blank(), \n    panel.background = element_blank(),\n    panel.grid = element_blank(), \n    legend.position = \"top\",\n    legend.title = element_blank(),\n    legend.background = element_blank()\n    ) +\n  geom_text(data = asuta.week, aes(x = daily, y = -0.02, label = day), size = 2.5, color = \"gray50\")+\n  geom_text(data = asuta.week |&gt; distinct(month, .keep_all = TRUE), aes(x = daily, y = -0.05, label = month), fontface = \"bold\", color = \"grey20\", size = 3)+\n   geom_text(data = asuta.week |&gt; distinct(month, .keep_all = TRUE) |&gt; slice(1) , aes(x = daily, y = -0.09, label = year), color = \"black\", size = 3.5)+\n  ggsci::scale_color_d3(limits = c(\"Implementing\", \"Strategy\", \"Reporting\"))\n\n\n\n\n\n\n\n# ggsave(\"milestone_timeline.pdf\", width = 8, height = 4, dpi = 300)\n\n\nasuta = asuta |&gt; \n  mutate(date = date+days(19))\n\ndate_range = asuta |&gt; pull(date) |&gt; range()\n\n\nasuta.week = tibble(\n  daily = seq(date_range[1], date_range[2], by = \"5 day\")\n) |&gt; \n  mutate(\n    day = day(daily),\n    month = month(daily, label = T),\n    year = year(daily)\n    )\n\n\nggplot()+\n  geom_hline(yintercept = 0)+\n  geom_segment(data = asuta, aes(x = date, y = 0, yend = position, color = status))+\n  geom_point(data = asuta, aes(x = date, y = position, color = status),size = 3) +\n  geom_text(data = asuta, aes(x = date, y = text_position, label = milestone, color = status), size = 3)+\n  scale_x_date(expand = expansion(mult = 0.08))+\n  theme_minimal()+\n  theme(\n    axis.title = element_blank(), \n    axis.text = element_blank(), \n    axis.ticks = element_blank(), \n    panel.background = element_blank(),\n    panel.grid = element_blank(), \n    legend.position = \"top\",\n    legend.title = element_blank(),\n    legend.background = element_blank()\n    ) +\n  geom_text(data = asuta.week, aes(x = daily, y = -0.02, label = day), size = 2.5, color = \"gray50\")+\n  geom_text(data = asuta.week |&gt; distinct(month, .keep_all = TRUE), aes(x = daily, y = -0.05, label = month), fontface = \"bold\", color = \"grey20\", size = 3)+\n   geom_text(data = asuta.week |&gt; distinct(month, .keep_all = TRUE) |&gt; slice(1) , aes(x = daily, y = -0.09, label = year), color = \"black\", size = 3.5)+\n  ggsci::scale_color_lancet(limits = c(\"Implementing\", \"Strategy\", \"Reporting\"))\n\n\n\n\n\n\n\n# today()+days(10)\n\n\n\n\nCitationBibTeX citation:@online{semba2023,\n  author = {Semba, Masumbuko},\n  title = {Creating a {Timeline} Graphic Using {R} and Ggplot2},\n  date = {2023-11-24},\n  url = {https://lugoga.github.io/kitaa/posts/timeline/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSemba, M., 2023. Creating a Timeline graphic using R and ggplot2 [WWW\nDocument]. URL https://lugoga.github.io/kitaa/posts/timeline/"
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Presentations",
    "section": "",
    "text": "Our Agenda\n\nAn introduction\nGet familiar with R and Rstudio\nData types\nData structures\nReading and writing data in Rstudio\nTidying Data with tidyverse\nPlotting\n\nPlotting and Visualization_A\nPlotting and Visualization_B\nPlotting and Visualization_C\n\nData Manipulation\nDescriptive Statistics\nInferential Statistics\nModelling and simulation\nSpatial Handling and Analysis\nFurther topics\n\nGIt and Github\nReproducibility with Quarto\nWebsites and blog\nUsing python from Rstudio\nGenerating HTML, PDF and Word Reports"
  }
]